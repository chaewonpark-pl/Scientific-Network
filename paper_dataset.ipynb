{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일을 UTF-8 형식으로 변환하여 저장했습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. MacRoman 인코딩으로 파일 읽기\n",
    "collabo_df = pd.read_csv('Paper_Dataset.csv', encoding='MacRoman', low_memory=False)\n",
    "\n",
    "# 2. UTF-8로 다시 저장\n",
    "collabo_df.to_csv('Paper_Dataset_UTF8.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"파일을 UTF-8 형식으로 변환하여 저장했습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elsevier + Semantic 합쳐주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to normalize titles\n",
    "def normalize_title(title):\n",
    "    # Convert to lowercase\n",
    "    title = title.lower()\n",
    "    # Remove special characters and multiple spaces\n",
    "    title = re.sub(r'\\s+', ' ', re.sub(r'[^a-zA-Z0-9\\s]', '', title))\n",
    "    return title.strip()\n",
    "\n",
    "# Try different encodings in case of decoding errors\n",
    "try:\n",
    "    # Load the datasets using 'utf-8' encoding first\n",
    "    elsevier_df = pd.read_csv('Collabo_Paper_Elsevier.csv', encoding='utf-8')\n",
    "    sem_updated_df = pd.read_csv('Collabo_Paper_Sem_Updated.csv', encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    # If 'utf-8' fails, try 'latin1' encoding\n",
    "    elsevier_df = pd.read_csv('Collabo_Paper_Elsevier.csv', encoding='utf-8')\n",
    "    sem_updated_df = pd.read_csv('Collabo_Paper_Sem_Updated.csv', encoding='utf-8')\n",
    "\n",
    "# Normalize titles in both dataframes\n",
    "elsevier_df['Normalized_Title'] = elsevier_df['Title'].apply(normalize_title)\n",
    "sem_updated_df['Normalized_Title'] = sem_updated_df['Title'].apply(normalize_title)\n",
    "\n",
    "# Drop duplicate titles in sem_updated_df, keeping only the first occurrence\n",
    "sem_updated_df = sem_updated_df.drop_duplicates(subset=['Normalized_Title'], keep='first')\n",
    "\n",
    "# Merge the datasets on the normalized title\n",
    "merged_df = pd.merge(elsevier_df, sem_updated_df[['Normalized_Title', 'Abstract', 'Authors', 'Citation Count', 'Influential Citation Count']],\n",
    "                     on='Normalized_Title', how='left')\n",
    "\n",
    "# Drop the 'Normalized_Title' column, as it's no longer needed\n",
    "merged_df = merged_df.drop(columns=['Normalized_Title'])\n",
    "\n",
    "# Save the resulting dataframe to a new CSV file\n",
    "merged_df.to_csv('Collabo_Paper_Dataset.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Merged dataset saved as 'Collabo_Paper_Dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset <- Industry Number (6T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_11016\\1370044914.py:4: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  collabo_df = pd.read_csv('Paper_Dataset_Test.csv', encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collabo_Paper_Dataset 열 이름:\n",
      "Index(['과제고유번호', '과제수행년도', '성과발생년도', '성과발생부처명', '성과사업ID', '성과사업명', '성과고유번호',\n",
      "       '논문_SCI구분_최종', 'Title', 'Journal', '논문번호', '논문_권', '논문_호', 'DOI',\n",
      "       '논문_기여율_확정', 'Affiliation', 'Authors', 'Citation Count',\n",
      "       'Influential Citation Count', 'Abstract'],\n",
      "      dtype='object')\n",
      "\n",
      "Processed_data 열 이름:\n",
      "Index(['사업ID', '부처명', '총연구비', '정부연구비', '총연구기간_시작', '총연구기간_종료', '연구기간',\n",
      "       '참여인원_공동위탁', '과학기술표준분류', '육T관련기술', '연구개발단계', '경제사회목적', '과제고유번호',\n",
      "       '과제수행년도', '협업_매트릭스', '협업_type_new', '총_참여조직_수', '대기업_수', '중견_수', '중소_수',\n",
      "       '대학_수', '연구기관_수', '기타_수', 'blau_index', '논문_개수', 'SCI_개수', '비SCI개수',\n",
      "       '특허_개수', '출원_개수', '등록_개수', '국내_특허_개수', '국내_출원_개수', '국내_등록_개수',\n",
      "       '해외_특허_개수', '해외_출원_개수', '해외_등록_개수', '성과_개수', '연도별_밀도', 'HHI', 'rv_HHI',\n",
      "       'GDC', 'num_external_tie', 'avg_num_external_tie'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 불러오기\n",
    "collabo_df = pd.read_csv('Paper_Dataset_Test.csv', encoding='utf-8')\n",
    "processed_df = pd.read_csv('processed_data_v7_231120.csv', encoding='utf-8')\n",
    "\n",
    "# 각 데이터프레임의 열 이름 출력\n",
    "print(\"Collabo_Paper_Dataset 열 이름:\")\n",
    "print(collabo_df.columns)\n",
    "\n",
    "print(\"\\nProcessed_data 열 이름:\")\n",
    "print(processed_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper_Dataset.csv 인코딩: UTF-8-SIG\n",
      "processed_data_v7_231120.csv 인코딩: UTF-8-SIG\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# 파일의 인코딩을 감지하기\n",
    "with open('Paper_Dataset.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(\"Paper_Dataset.csv 인코딩:\", result['encoding'])\n",
    "\n",
    "with open('processed_data_v7_231120.csv', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(\"processed_data_v7_231120.csv 인코딩:\", result['encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_11016\\3414150844.py:4: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  collabo_df = pd.read_csv('Paper_Dataset.csv', encoding='utf-8-sig')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 불러오기 (BOM을 제거하기 위해 'utf-8-sig' 사용)\n",
    "collabo_df = pd.read_csv('Paper_Dataset.csv', encoding='utf-8-sig')\n",
    "processed_df = pd.read_csv('processed_data_v7_231120.csv', encoding='utf-8-sig')\n",
    "\n",
    "# '과제고유번호'를 기준으로 '육T관련기술' 병합\n",
    "merged_df = pd.merge(collabo_df, processed_df[['과제고유번호', '육T관련기술']],\n",
    "                     on='과제고유번호', how='left')\n",
    "\n",
    "# CSV 파일로 저장 (UTF-8로)\n",
    "merged_df.to_csv('Paper_Dataset_2.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"병합 및 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract 한줄로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_11016\\4196072912.py:4: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  collabo_df = pd.read_csv('Paper_Dataset.csv', encoding='utf-8-sig')  # BOM 제거를 위한 인코딩\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract 열의 줄바꿈 제거 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. CSV 파일 불러오기\n",
    "collabo_df = pd.read_csv('Paper_Dataset.csv', encoding='utf-8-sig')  # BOM 제거를 위한 인코딩\n",
    "\n",
    "# 2. 'Abstract' 열의 엔터를 공백으로 대체하여 한 줄로 만들기\n",
    "collabo_df['Abstract'] = collabo_df['Abstract'].str.replace('\\n', ' ', regex=True)\n",
    "\n",
    "# 3. CSV 파일로 저장\n",
    "collabo_df.to_csv('Paper_Dataset_Processed.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Abstract 열의 줄바꿈 제거 및 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affiliation 깨진 글씨 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "\n",
    "# Step 2: Check for rows where 'Affiliation' contains Korean characters or '?'\n",
    "# A regex pattern to check for Korean characters\n",
    "pattern = r'[\\uac00-\\ud7af?]'  # Korean characters range and '?'\n",
    "filtered_rows = df[df['Affiliation'].str.contains(pattern, na=False)]\n",
    "\n",
    "# Step 3: Output results and count\n",
    "print(filtered_rows)\n",
    "print(\"Number of rows containing Korean characters or '?':\", len(filtered_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacement_dict = {\n",
      "    \"Universit채tsklinikum Freiburg\": \"\",\n",
      "    \"T챕l챕com Paris\": \"\",\n",
      "    \"Universit채t Wien\": \"\",\n",
      "    \"Universit횪 degli Studi della Campania Luigi Vanvitelli\": \"\",\n",
      "    \"Max Planck In짯sti짯tute for Mar짯ine Mi짯cro짯bi짯o짯logy\": \"\",\n",
      "    \"Universit챕 de Franche-Comt챕\": \"\",\n",
      "    \"Universit챕 du Qu챕bec 횪 Chicoutimi\": \"\",\n",
      "    \"Bundesanstalt f체r Geowissenschaften und Rohstoffe\": \"\",\n",
      "    \"Technische Universit채t Bergakademie Freiberg\": \"\",\n",
      "    \"Institut de Recherche en Informatique et Syst챔mes Al챕atoires\": \"\",\n",
      "    \"H첩gskolen i Bergen\": \"\",\n",
      "    \"Goethe-Universit채t Frankfurt am Main\": \"\",\n",
      "    \"Universidad Aut처noma de Sinaloa\": \"\",\n",
      "    \"횋cole Normale Sup챕rieure\": \"\",\n",
      "    \"Tecnol처gico de Monterrey\": \"\",\n",
      "    \"The Catholic University of Korea Eunpyeong St. Mary?셲 Hospital\": \"\",\n",
      "    \"Katolicki Uniwersytet Lubelski Jana Paw흢a II\": \"\",\n",
      "    \"Y캇ld캇z Teknik 횥niversitesi\": \"\",\n",
      "    \"Instiutet f철r rymdfysik\": \"\",\n",
      "    \"Institut f체r Chemische Technologie von Materialien\": \"\",\n",
      "    \"Universit횪 di Pisa\": \"\",\n",
      "    \"Technische Universit채t Braunschweig\": \"\",\n",
      "    \"National Research University ?쏮oscow Power Engineering Institute?? Voronezhskiy Gosudarstvenniy Universitet\": \"\",\n",
      "    \"Leibniz-Institut f체r Kristallz체chtung\": \"\",\n",
      "    \"University of Hawai軻i at M훮noa\": \"\",\n",
      "    \"Universit챕 de Versailles Saint-Quentin-en-Yvelines\": \"\",\n",
      "    \"Universit챕 de Lausanne (UNIL)\": \"\",\n",
      "    \"Universit챕 d'Artois\": \"\",\n",
      "    \"H척pital Universitaire San Jo찾o\": \"\",\n",
      "    \"Institut de Chimie et Proc챕d챕s pour l'Energie, l'Environnement et la Sant챕 (ICPEES)\": \"\",\n",
      "    \"Lule책 University of Technology\": \"\",\n",
      "    \"University of M체nster\": \"\",\n",
      "    \"Max-Planck-Institut f체r Biochemie\": \"\",\n",
      "    \"Universit챕 du Littoral C척te d?쁎pale\": \"\",\n",
      "    \"Institut de Neurosciences Cognitives et Int챕gratives d?섲quitaine\": \"\",\n",
      "    \"Direction Sant챕 Confort\": \"\",\n",
      "    \"Leibniz-Institut f체r Naturstoff-Forschung und Infektionsbiologie e. V. ??Hans-Kn철ll-Institut\": \"\",\n",
      "    \"Friedrich-Schiller-Universit채t Jena\": \"\",\n",
      "    \"Universidade de Bras챠lia\": \"\",\n",
      "    \"Dipartimento di Fisica e Astronomia dell'Universit횪\": \"\",\n",
      "    \"Instytut Metali Niezelaznych Oddzia흢 w Poznaniu Centralne Laboratorium Akumulator처w i Ogniw\": \"\",\n",
      "    \"Uniwersytet Gda흦ski\": \"\",\n",
      "    \"Rheinland-Pf채lzische Technische Universit채t Kaiserslautern-Landau\": \"\",\n",
      "    \"National Research University ?쏮oscow Power Engineering Institute??\": \"\",\n",
      "    \"Universit챕 Savoie Mont Blanc\": \"\",\n",
      "    \"횋cole Polytechnique F챕d챕rale de Lausanne\": \"\",\n",
      "    \"Universit챕 Libre de Bruxelles\": \"\",\n",
      "    \"Firat 횥niversitesi\": \"\",\n",
      "    \"Institut Ru휃er Bo큄kovi훶\": \"\",\n",
      "    \"Universit챕 de Poitiers\": \"\",\n",
      "    \"Technischen Universit채t Ilmenau\": \"\",\n",
      "    \"Universit챕 de Tunis El Manar\": \"\",\n",
      "    \"Universitat Polit챔cnica de Val챔ncia\": \"\",\n",
      "    \"Bauhaus-Universit채t Weimar\": \"\",\n",
      "    \"Universit챕 de Bordeaux\": \"\",\n",
      "    \"C찼tedras CONACYT\": \"\",\n",
      "    \"Universit횪 degli Studi di Sassari\": \"\",\n",
      "    \"Helmholtz Zentrum f체r Umweltforschung\": \"\",\n",
      "    \"Universit채t Regensburg\": \"\",\n",
      "    \"Universidad T챕cnica de Babahoyo\": \"\",\n",
      "    \"CEA LETI혻\": \"\",\n",
      "    \"?cole Polytechnique F?d?rale de Lausanne\": \"\",\n",
      "    \"Universit횪 degli Studi di Siena\": \"\",\n",
      "    \"Universit횪 del Salento\": \"\",\n",
      "    \"National Institute of Horticultural 竊?Herbal Science\": \"\",\n",
      "    \"Universit횪 Telematica Internazionale UNINETTUNO\": \"\",\n",
      "    \"Universidad P첬blica de Navarra\": \"\",\n",
      "    \"Sveu훾ili큄te u Zagrebu, Prirodoslovno - Matemati훾ki Fakultet\": \"\",\n",
      "    \"Universidade de S찾o Paulo\": \"\",\n",
      "    \"Fraunhofer-Institut f체r Angewandte Polymerforschung\": \"\",\n",
      "    \"Universit횪 degli Studi di Messina\": \"\",\n",
      "    \"Universidad Polit챕cnica de Madrid\": \"\",\n",
      "    \"Universit횪 degli Studi di Palermo\": \"\",\n",
      "    \"Eski힊ehir Osmangazi 횥niversitesi\": \"\",\n",
      "    \"Gottfried Wilhelm Leibniz Universit채t Hannover\": \"\",\n",
      "    \"Universit횪 degli Studi di Catania\": \"\",\n",
      "    \"Institut Sup챕rieur d'Electronique de Paris (ISEP)\": \"\",\n",
      "    \"Instituto Federal de Educa챌찾o, Ci챗ncia e Tecnologia do Cear찼, Fortaleza\": \"\",\n",
      "    \"Martin-Luther-Universit채t Halle-Wittenberg\": \"\",\n",
      "    \"EMBL?셲 European Bioinformatics Institute\": \"\",\n",
      "    \"Karlsruher Institut f체r Technologie, Campus Nord\": \"\",\n",
      "    \"Universit횪 degli Studi di Genova\": \"\",\n",
      "    \"Universit채tsmedizin Greifswald\": \"\",\n",
      "    \"IRSN Institut de Radioprotection et de Suret챕 Nucl챕aire\": \"\",\n",
      "    \"Alma Mater Studiorum Universit횪 di Bologna\": \"\",\n",
      "    \"Universit챕 Paris Cit챕\": \"\",\n",
      "    \"Universit챕 de Tours\": \"\",\n",
      "    \"Universit챕 Abou Bekr Belkaid Tlemcen\": \"\",\n",
      "    \"Laboratoire de l'Acc챕l챕rateur Lin챕aire\": \"\",\n",
      "    \"Centre Inria Sophia Antipolis - M챕diterran챕e\": \"\",\n",
      "    \"Medizinische Universit채t Graz\": \"\",\n",
      "    \"Bilkent 횥niversitesi\": \"\",\n",
      "    \"Technology Arts Sciences TH K철ln\": \"\",\n",
      "    \"Transport첩konomisk institutt\": \"\",\n",
      "    \"Centro de Investigaciones Energ챕ticas, Medioambientales y Tecnol처gicas\": \"\",\n",
      "    \"Universit횪 di Trento\": \"\",\n",
      "    \"Universit챕 Saint-Joseph de Beyrouth\": \"\",\n",
      "    \"Universit챕 Laval\": \"\",\n",
      "    \"Universit챕 de Technologie de Troyes\": \"\",\n",
      "    \"Creation and Love Women?셲 Hospital\": \"\",\n",
      "    \"Laborat처rio de Instrumentacao e F챠sica Experimental de Part챠culas\": \"\",\n",
      "    \"Universit챕 de Picardie Jules Verne\": \"\",\n",
      "    \"Universit챕 Claude Bernard Lyon 1\": \"\",\n",
      "    \"Instituto Superior T챕cnico\": \"\",\n",
      "    \"Organizaci처n Sanitaria Integrada Goierri - Alto Urola\": \"\",\n",
      "    \"Universit채t des Saarlandes\": \"\",\n",
      "    \"Politechnika Wroc흢awska\": \"\",\n",
      "    \"Universit챕 de Mons\": \"\",\n",
      "    \"Nantes Universit챕\": \"\",\n",
      "    \"Kyung-In Women?셲 University\": \"\",\n",
      "    \"Instituto de Investigaci처n Sanitaria de la Fundaci처n Jim챕nez D챠az\": \"\",\n",
      "    \"Universit횪 degli Studi di Cagliari\": \"\",\n",
      "    \"Universit횪 Cattolica del Sacro Cuore, Campus di Brescia\": \"\",\n",
      "    \"Universit챕 Paris-Saclay\": \"\",\n",
      "    \"Centro de Investigaci처n Biom챕dica en Red de C찼ncer\": \"\",\n",
      "    \"Universit채tsklinikum Heidelberg\": \"\",\n",
      "    \"Institut Universitaire de Radiophysique Appliqu챕e\": \"\",\n",
      "    \"Instituto de Bioingenier챠a de Catalu챰a\": \"\",\n",
      "    \"Universit횪 della Calabria\": \"\",\n",
      "    \"Universit횪 degli Studi di Napoli Federico II\": \"\",\n",
      "    \"Link철pings Universitet\": \"\",\n",
      "    \"Institut Mines T챕l챕com\": \"\",\n",
      "    \"Laboratorio Nacional de Fusi처n\": \"\",\n",
      "    \"Universit횪 degli Studi di Padova\": \"\",\n",
      "    \"Organisation Europ챕enne pour la Recherche Nucl챕aire\": \"\",\n",
      "    \"Universit챕 de Lyon\": \"\",\n",
      "    \"Universitat de Val챔ncia\": \"\",\n",
      "    \"Bundesamt f체r Seeschiffahrt und Hydrographie\": \"\",\n",
      "    \"Institut F챕d챕ratif de Recherche 49\": \"\",\n",
      "    \"Instituto Nacional de Neurolog챠a y Neurocirug챠a\": \"\",\n",
      "    \"Institut de la Micro챕lectronique, Electromagn챕tisme et Photonique - Laboratoire d'Hyperfr챕quences et de Caract챕risation\": \"\",\n",
      "    \"Rutgers University?밡ew Brunswick\": \"\",\n",
      "    \"Zentrum f체r Regenerative Therapien Dresden\": \"\",\n",
      "    \"Max-Planck-Institut f체r Kolloid- und Grenzfl채chenforschung\": \"\",\n",
      "    \"Helmholtz?밵entrum Geesthacht\": \"\",\n",
      "    \"횜rebro Universitet\": \"\",\n",
      "    \"Laboratoire des Sciences des Proc챕d챕s et des Mat챕riaux\": \"\",\n",
      "    \"Instituci처 Catalana de Recerca i Estudis Avan챌ats\": \"\",\n",
      "    \"Universit챕 de Tunis El Manar, Facult챕 des Sciences de Tunis\": \"\",\n",
      "    \"Universit채t Ulm\": \"\",\n",
      "    \"Universit채t Rostock\": \"\",\n",
      "    \"Pusan ?뗢딳ational University Children's Hospital\": \"\",\n",
      "    \"Leibniz-Institut f체r Polymerforschung Dresden e.V.\": \"\",\n",
      "    \"Organisation Mondiale de la Sant챕\": \"\",\n",
      "    \"Sodankyl채 Geophysical Observatory\": \"\",\n",
      "    \"Laboratoire des Mat챕riaux et du G챕nie Physique (LMGP)\": \"\",\n",
      "    \"Technische Universit채t Wien\": \"\",\n",
      "    \"Technische Universit채t M체nchen\": \"\",\n",
      "    \"횋cole des Mines de Saint-횋tienne\": \"\",\n",
      "    \"Johannes Gutenberg-Universit채t Mainz\": \"\",\n",
      "    \"Centro Brasileiro de Pesquisas F챠sicas\": \"\",\n",
      "    \"Universit챕 Clermont Auvergne\": \"\",\n",
      "    \"Bergische Universit채t Wuppertal\": \"\",\n",
      "    \"Universidad de Alcal찼\": \"\",\n",
      "    \"Universidad de Le처n\": \"\",\n",
      "    \"Universit채t Z체rich\": \"\",\n",
      "    \"Fundaci처n Agencia Aragonesa para la Investigaci처n y el Desarrollo (ARAID)\": \"\",\n",
      "    \"Facult챕 des Sciences Semlalia\": \"\",\n",
      "    \"횋cole Nationale Sup챕rieure d'Ing챕nieurs de Tunis\": \"\",\n",
      "    \"Queen?셲 University\": \"\",\n",
      "    \"Ludwig-Maximilians-Universit채t M체nchen\": \"\",\n",
      "    \"H척pital Saint-Antoine\": \"\",\n",
      "    \"G챕olocalisation (AME-GEOLOC)\": \"\",\n",
      "    \"Universidade Federal do Piau챠\": \"\",\n",
      "    \"D梳죍 h沼뛠 Nguyen Tat Thanh\": \"\",\n",
      "    \"Klinikum der Universit채t Regensburg und Medizinische Fakult채t\": \"\",\n",
      "    \"Universit횪 degli Studi dell'Aquila\": \"\",\n",
      "    \"Rheinisch-Westf채lische Technische Hochschule Aachen\": \"\",\n",
      "    \"Karlsruher Institut f체r Technologie\": \"\",\n",
      "    \"Universit횪 degli Studi di Trieste\": \"\",\n",
      "    \"Centro de Investigacion y de Estudios Avanzados del Instituto Polit챕cnico Nacional\": \"\",\n",
      "    \"Universit챕 de Strasbourg\": \"\",\n",
      "    \"University of Rzesz처w\": \"\",\n",
      "    \"Hochschule F횄쩌r Technik Stuttgart\": \"\",\n",
      "    \"Univerzita Palack?ho v Olomouci\": \"\",\n",
      "    \"Universit챕 Toulouse III - Paul Sabatier\": \"\",\n",
      "    \"Redeemer?쁲 University\": \"\",\n",
      "    \"Tr튼沼쓓g 휂梳죍 h沼뛠 S튼 ph梳죑 H횪 N沼셢\": \"\",\n",
      "    \"Universit횪 degli Studi di Enna \"Kore\"\": \"\",\n",
      "    \"Universit챕 de Perpignan Via Domitia\": \"\",\n",
      "    \"Interfaces Traitements Organisation et DYnamique des Syst챔mes ??ITODYS\": \"\",\n",
      "    \"Bo휓azi챌i 횥niversitesi\": \"\",\n",
      "    \"Universidad Aut처noma de Chiapas\": \"\",\n",
      "    \"Centro de Investigaci?n y de Estudios Avanzados del Instituto Polit?cnico Nacional\": \"\",\n",
      "    \"Brandenburgische Technische Universit채t Cottbus\": \"\",\n",
      "    \"Tartu 횥likooli Genoomika Instituut\": \"\",\n",
      "    \"Alfred-Wegener-Institut Helmholtz-Zentrum f체r Polar- und Meeresforschung\": \"\",\n",
      "    \"Christian-Albrechts-Universit채t zu Kiel\": \"\",\n",
      "    \"Universidad Nacional de C처rdoba\": \"\",\n",
      "    \"Sveu훾ili큄te u Zagrebu, Geodetski fakultet\": \"\",\n",
      "    \"Interactions h척tes-agents pathog챔nes - (IHAP)\": \"\",\n",
      "    \"Universidad Mayor de San Andr챕s\": \"\",\n",
      "    \"Universit횪 degli Studi di Perugia\": \"\",\n",
      "    \"Freie Universit채t Berlin\": \"\",\n",
      "    \"D챕partement d'Informatique de l'ENS\": \"\",\n",
      "    \"Gangnam St. Mary?셲 One Eye Clinic\": \"\",\n",
      "    \"Universit채tsklinikum Hamburg-Eppendorf\": \"\",\n",
      "    \"Muse챕 Canadien de la Nature\": \"\",\n",
      "    \"Norges Milj첩- og Biovitenskapelige Universitet\": \"\",\n",
      "    \"Global Core Research Center for Ship and Offshore Plants (GCRE?륲OP)\": \"\",\n",
      "    \"Centre de Recherche et des Technologies de L'Energie (CRTEn) BorjCedria B.P N째952050-Hammam Lif.\": \"\",\n",
      "    \"Technische Universit채t Berlin\": \"\",\n",
      "    \"Consejo Superior de Investigaciones Cient챠ficas\": \"\",\n",
      "    \"Universit챕 de Toulouse\": \"\",\n",
      "    \"Wroc흢aw University of Environmental and Life Sciences\": \"\",\n",
      "    \"G철teborgs Universitet\": \"\",\n",
      "    \"Universit챕 de Lorraine\": \"\",\n",
      "    \"CHU de Qu챕bec-Universit챕 Laval\": \"\",\n",
      "    \"T횥B캅TAK Ulusal Metroloji Enstit체s체\": \"\",\n",
      "    \"Univerzita Jana Evangelisty Purkyne v 횣st챠 nad Labem\": \"\",\n",
      "    \"CSIC-UNIOVI-Principado de Asturias - Centro de Investigaci처n en Nanomateriales y Nanotecnolog챠a (CINN)\": \"\",\n",
      "    \"Technische Universit채t Chemnitz\": \"\",\n",
      "    \"Bundesanstalt f체r Materialforschung und -Pr체fung\": \"\",\n",
      "    \"Universit횪 degli Studi dell'Insubria\": \"\",\n",
      "    \"Philipps-Universit채t Marburg\": \"\",\n",
      "    \"Technick찼 Univerzita v Ko큄iciach\": \"\",\n",
      "    \"Universidad de Ja챕n\": \"\",\n",
      "    \"Universit채t Greifswald\": \"\",\n",
      "    \"CSIC - Instituto de 횙ptica Daza de Vald챕s (IO)\": \"\",\n",
      "    \"Universidad de Sevilla, Escuela T챕cnica Superior de Ingenier챠a\": \"\",\n",
      "    \"Universit챕 Gustave Eiffel\": \"\",\n",
      "    \"Universit챕 Catholique de Louvain\": \"\",\n",
      "    \"M챕t챕o France\": \"\",\n",
      "    \"Uniwersytet Jagiello흦ski\": \"\",\n",
      "    \"Laboratoire d'Oc챕anographie et du Climat : Exp챕rimentations et Approches Num챕riques\": \"\",\n",
      "    \"H철gskolan i Bor책s\": \"\",\n",
      "    \"Deutsches Zentrum f체r Neurodegenerative Erkrankungen\": \"\",\n",
      "    \"Berliner Institut f체r Gesundheitsforschung\": \"\",\n",
      "    \"University of 횋vora\": \"\",\n",
      "    \"Universit횪 degli Studi di Milano\": \"\",\n",
      "    \"Universidad T챕cnica Federico Santa Mar챠a\": \"\",\n",
      "    \"Service Hydrographique et Oc챕anographique de la Marine\": \"\",\n",
      "    \"Technische Universit채t Darmstadt\": \"\",\n",
      "    \"Centre de Recherche en Astronomie Astrophysique et G챕ophysique\": \"\",\n",
      "    \"Universit챕 de Bretagne Occidentale\": \"\",\n",
      "    \"횋cole polytechnique\": \"\",\n",
      "    \"Universit채t Trier\": \"\",\n",
      "    \"CICECO ??Instituto de Materiais de Aveiro\": \"\",\n",
      "    \"Aix Marseille Universit챕\": \"\",\n",
      "    \"Forschungszentrum J체lich GmbH\": \"\",\n",
      "    \"Universidade Federal do Esp챠rito Santo\": \"\",\n",
      "    \"Laboratoire Aim챕 Cotton\": \"\",\n",
      "    \"CRISMAT - Laboratoire de Crystallographie et Sciences des Mat챕riaux\": \"\",\n",
      "    \"Instituto Polit챕cnico Nacional\": \"\",\n",
      "    \"Cheil General Hospital and Women?셲 Healthcare Center\": \"\",\n",
      "    \"Universit채t Siegen\": \"\",\n",
      "    \"K첩benhavns Universitet\": \"\",\n",
      "    \"횋cole Normale Sup챕rieure Paris-Saclay\": \"\",\n",
      "    \"Eberhard Karls Universit채t T체bingen\": \"\",\n",
      "    \"Universit챕 C척te d'Azur\": \"\",\n",
      "    \"Institut \"Jo탑ef Stefan\"\": \"\",\n",
      "    \"Universit채t Stuttgart\": \"\",\n",
      "    \"Z체hlke Group\": \"\",\n",
      "    \"National Technical University of Ukraine ?쏧gor Sikorsky Kyiv Polytechnic Institute??Temperature dependence of the excitonic energy band gap in In(Ga)As nanostructures\": \"\",\n",
      "    \"Leibniz-Institut f체r Festk철rper- und Werkstoffforschung Dresden\": \"\",\n",
      "    \"Tr튼沼쓓g 휂梳죍 h沼뛠 C척ng nghi沼뇈 th횪nh ph沼?H沼?Ch챠 Minh\": \"\",\n",
      "    \"Erciyes 횥niversitesi\": \"\",\n",
      "    \"Centro de Investiga??o em Sistemas Confi?veis e de Tempo Real\": \"\",\n",
      "    \"Hanyang Women?셲 University\": \"\",\n",
      "    \"Tartu 횥likool\": \"\",\n",
      "    \"Universit횪 degli studi di Bari Aldo Moro\": \"\",\n",
      "    \"Universit횪 Ca' Foscari Venezia\": \"\",\n",
      "    \"횋lectroniques, Syst챔mes de Communication et Microsyst챔mes (ESYCOM)\": \"\",\n",
      "    \"Yak캇n Do휓u 횥niversitesi\": \"\",\n",
      "    \"Sveu훾ili큄te u Zagrebu, Fakultet Strojarstva i Brodogradnje\": \"\",\n",
      "    \"Fraunhofer-Institut f체r Nachrichtentechnik Heinrich-Hertz-Institut\": \"\",\n",
      "    \"Gebze Teknik 횥niversitesi\": \"\",\n",
      "    \"University of Jyv채skyl채\": \"\",\n",
      "    \"Benem챕rita Universidad Aut처noma de Puebla\": \"\",\n",
      "    \"Universit챕 Grenoble Alpes\": \"\",\n",
      "    \"Medizinische Universit채t Wien\": \"\",\n",
      "    \"Leibniz-Institut f?r Festk?rper- und Werkstoffforschung Dresden\": \"\",\n",
      "    \"Sapienza Universit횪 di Roma\": \"\",\n",
      "    \"휂梳죍 h沼뛠 M沼?Th횪nh ph沼?H沼?Ch챠 Minh\": \"\",\n",
      "    \"Universit채t Basel\": \"\",\n",
      "    \"Max-Planck-Institut f체r Eisenforschung GmbH\": \"\",\n",
      "    \"Laboratoire Plasma et Conversion d?섷nergie\": \"\",\n",
      "    \"Soomyung Women?셲 University\": \"\",\n",
      "    \"University of Nebraska?밚incoln\": \"\",\n",
      "    \"Universit채t Bonn\": \"\",\n",
      "    \"Technische Universit채t Dresden\": \"\",\n",
      "    \"Institut Lumi챔re Mati챔re\": \"\",\n",
      "    \"Universit챕 Ahmed Draia - Adrar\": \"\",\n",
      "    \"Universidad Aut처noma de Madrid\": \"\",\n",
      "    \"J체lich Aachen Research Alliance (JARA)-Fundamentals of Future Information Technology\": \"\",\n",
      "    \"Sorbonne Universit챕\": \"\",\n",
      "    \"Universit챕 de Montpellier\": \"\",\n",
      "    \"Institut de Recerca contra la Leuc챔mia Josep Carreras (IJC)\": \"\",\n",
      "    \"A?셎harqiyah University\": \"\",\n",
      "    \"Universit챕 de Gen챔ve\": \"\",\n",
      "    \"Universidade Tecnol처gica Federal do Paran찼\": \"\",\n",
      "    \"Gesellschaft f체r Anlagen- und Reaktorsicherheit mbH\": \"\",\n",
      "    \"Universit챕 Badji Mokhtar - Annaba\": \"\",\n",
      "    \"Abdullah G체l 횥niversitesi\": \"\",\n",
      "    \"Sj챈llands Universitetshospital\": \"\",\n",
      "    \"Institut des Mat챕riaux Poreux de Paris IMAP\": \"\",\n",
      "    \"Universitat Polit챕cnica de Catalunya\": \"\",\n",
      "    \"Universit채t Augsburg\": \"\",\n",
      "    \"Deutsches Zentrum f체r Luft- und Raumfahrt (DLR)\": \"\",\n",
      "    \"Universit횪 degli Studi di Milano-Bicocca\": \"\",\n",
      "    \"Universit챕 de Jijel\": \"\",\n",
      "    \"Institut Parisien de Chimie Mol챕culaire\": \"\",\n",
      "    \"Universit챕 McGill\": \"\",\n",
      "    \"횋cole de Technologie Sup챕rieure\": \"\",\n",
      "    \"Universit채tsklinikum des Saarlandes Medizinische Fakult채t der Universit채t des Saarlandes\": \"\",\n",
      "    \"Universidad Aut처noma del Estado de Morelos\": \"\",\n",
      "    \"Sardar Bahadur Khan Women?셲 University (SBKWU)\": \"\",\n",
      "    \"Dumlupinar 횥niversitesi\": \"\",\n",
      "    \"Charit챕 ??Universit채tsmedizin Berlin\": \"\",\n",
      "    \"Georg-August-Universit채t G철ttingen\": \"\",\n",
      "    \"Universit채tsmedizin G철ttingen\": \"\",\n",
      "    \"Zentrum f체r Technologietransfer und Telekommunikation (ZTT)\": \"\",\n",
      "    \"Universidade Federal de Uberl창ndia\": \"\",\n",
      "    \"Ministerio de Salud P첬blica\": \"\",\n",
      "    \"Universidad Aut처noma del Estado de M챕xico\": \"\",\n",
      "    \"ARMTEC Tecnologia em Rob처tica\": \"\",\n",
      "    \"Universit채t Innsbruck\": \"\",\n",
      "    \"It채-Suomen yliopisto\": \"\",\n",
      "    \"Helmholtz-Zentrum Berlin f체r Materialien und Energie (HZB)\": \"\",\n",
      "    \"Medizinisches Laserzentrum L체beck\": \"\",\n",
      "    \"E철tv철s Lor찼nd Tudom찼nyegyetem\": \"\",\n",
      "    \"Necmettin Erbakan 횥niversitesi\": \"\",\n",
      "    \"Universit챕 d'Orl챕ans\": \"\",\n",
      "    \"Universit챕 de Tunis El Manar, Ecole Nationale d'Ing챕nieurs de Tunis\": \"\",\n",
      "    \"Laboratoire de Physique et d'횋tude des Mat챕riaux\": \"\",\n",
      "    \"Universit채t Hamburg\": \"\",\n",
      "    \"Institut d'Investigaci처 Biomedica de Bellvitge\": \"\",\n",
      "    \"Universit횪 degli Studi di Torino\": \"\",\n",
      "    \"Institut des Mol챕cules et Mat챕riaux du Mans\": \"\",\n",
      "    \"Universit횪 Cattolica del Sacro Cuore, Campus di Roma\": \"\",\n",
      "    \"Friedrich-Alexander-Universit채t Erlangen-N체rnberg\": \"\",\n",
      "    \"Instituto de Telecomunica챌천es\": \"\",\n",
      "    \"Ume책 Universitet\": \"\",\n",
      "    \"Fundaci처 Institut Universitari per a la recerca a l'Atenci처 Prim횪ria de Salut Jordi Gol i Gurina (IDIAPJGol)\": \"\",\n",
      "    \"Dongduck Women?셲 University\": \"\",\n",
      "    \"Narodowe Centrum Bada흦 J훳drowych, Otwock\": \"\",\n",
      "    \"Fraunhofer-Institut F체r Organische Elektronik, Elektronenstrahl- Und Plasmatechnik\": \"\",\n",
      "    \"Tr튼沼쓓g 휂梳죍 h沼뛠 Giao th척ng v梳춏 t梳즜\": \"\",\n",
      "    \"Universit채t Freiburg\": \"\",\n",
      "    \"Universit채t Bremen\": \"\",\n",
      "    \"Universit챕 du Qu챕bec 횪 Montr챕al\": \"\",\n",
      "    \"Universit채t Leipzig\": \"\",\n",
      "    \"Universit챕 d'Evry Val d'Essonne\": \"\",\n",
      "    \"D챕partement de Mod챕lisation des Syst챔mes et Structures\": \"\",\n",
      "    \"O?셄eal Comprehensive Cancer Center\": \"\",\n",
      "    \"Kyungin Women?셲 University\": \"\",\n",
      "    \"Sookmyung Women?셲 University\": \"\",\n",
      "    \"Klinikum der Universit채t M체nchen\": \"\",\n",
      "    \"횇ngstr철mlaboratoriet\": \"\",\n",
      "    \"CISE-Centro de Investiga챌찾o em Sistemas Electromecatr처nicos\": \"\",\n",
      "    \"Universitat Aut챵noma de Barcelona\": \"\",\n",
      "    \"Laboratoire National des Champs Magn챕tiques Intenses (LNCMI)\": \"\",\n",
      "    \"Universit채t Duisburg-Essen\": \"\",\n",
      "    \"Ecole Nationale Sup챕rieure de Chimie de Rennes\": \"\",\n",
      "    \"Universit채t zu K철ln\": \"\",\n",
      "    \"Uniwersytet 힃l훳ski w Katowicach\": \"\",\n",
      "    \"CSIC-INTA - Centro de Astrobiolog챠a (CAB)\": \"\",\n",
      "    \"Helmholtz-Institute Erlangen-N체renberg for Renewable Energy (HI ERN)\": \"\",\n",
      "    \"La Rochelle Universit챕\": \"\",\n",
      "    \"ETH Z체rich\": \"\",\n",
      "    \"Julius-Maximilians-Universit채t W체rzburg\": \"\",\n",
      "    \"Centrale M챕diterran챕e\": \"\",\n",
      "    \"National Technical University of Ukraine ?쏧gor Sikorsky Kyiv Polytechnic Institute??\": \"\",\n",
      "    \"Centre de Recherches en Canc챕rologie de Toulouse\": \"\",\n",
      "    \"Centre 횋nergie Mat챕riaux T챕l챕communications\": \"\",\n",
      "    \"Uniwersytet Miko흢aja Kopernika w Toruniu\": \"\",\n",
      "    \"University of Wroc흢aw\": \"\",\n",
      "    \"T챕l챕com SudParis\": \"\",\n",
      "    \"Universit채tsklinikum M체nster\": \"\",\n",
      "    \"Centre de recherche du CHU de Qu챕bec-Universit챕 Laval\": \"\",\n",
      "    \"Univerzita Komensk챕ho v Bratislave\": \"\",\n",
      "    \"Humboldt-Universit채t zu Berlin\": \"\",\n",
      "    \"Medizinische Universit채t Wien, Zentrum f체r Medizinische Physik und Biomedizinische Technik\": \"\",\n",
      "    \"Universit챕 des Sciences et de la Technologie Houari Boumediene\": \"\",\n",
      "    \"Medizinische Fakult채t, RWTH Aachen University\": \"\",\n",
      "    \"Universit횪 degli Studi di Foggia\": \"\",\n",
      "    \"B체hler AG\": \"\",\n",
      "    \"Justus-Liebig-Universit채t Gie횩en\": \"\",\n",
      "    \"Szegedi Tudom찼nyegyetem (SZTE)\": \"\",\n",
      "    \"Universit횪 degli Studi di Brescia\": \"\",\n",
      "    \"Universit채t Potsdam\": \"\",\n",
      "    \"IN2P3 - Institut National de Physique Nucl챕aire et de Physique Des Particules\": \"\",\n",
      "    \"Centre de D챕veloppement des Technologies Avanc챕es\": \"\",\n",
      "    \"Universit채t Heidelberg\": \"\",\n",
      "    \"Erzincan Binali Y캇ld캇r캇m 횥niversitesi\": \"\",\n",
      "    \"Pavol Jozef 힋af찼rik University in Ko큄ice\": \"\",\n",
      "    \"Pusan ?뗢딳ational University Dental Hospital\": \"\",\n",
      "    \"Universit횪 degli Studi di Roma \"Tor Vergata\"\": \"\",\n",
      "    \"ECPM 횋cole Europ챕enne de Chimie, Polym챔res et Mat챕riaux de Strasbourg\": \"\",\n",
      "    \"Universit챕 de Neuch창tel\": \"\",\n",
      "    \"Universidad Nacional Aut처noma de M챕xico\": \"\",\n",
      "    \"Heinrich-Heine-Universit채t D체sseldorf\": \"\",\n",
      "    \"Universit채t Bayreuth\": \"\",\n",
      "    \"University of Jyv?skyl?\": \"\",\n",
      "    \"Universit챕 de Lille\": \"\",\n",
      "    \"Institut de Recherches Cliniques de Montr챕al\": \"\",\n",
      "    \"Universit횪 degli Studi di Salerno\": \"\",\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_11016\\3863818269.py:5: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Paper_Dataset.csv', encoding='utf-8-sig')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Read the CSV file with utf-8-sig encoding\n",
    "df = pd.read_csv('Paper_Dataset.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Step 2: Initialize a list to store problematic organizations\n",
    "problematic_orgs = []\n",
    "\n",
    "# Step 3: Define a function to extract organizations containing Korean characters or \"?\"\n",
    "def extract_problematic_affiliations(affiliation_str):\n",
    "    # Check if the affiliation_str is a valid string\n",
    "    if isinstance(affiliation_str, str):\n",
    "        affiliations = affiliation_str.split('; ')\n",
    "        for org in affiliations:\n",
    "            if re.search(r'[\\uac00-\\ud7af]|[\\?]', org):  # Check for Korean characters or \"?\"\n",
    "                problematic_orgs.append(org)\n",
    "\n",
    "# Step 4: Apply the function to the 'Affiliation' column\n",
    "df['Affiliation'].apply(extract_problematic_affiliations)\n",
    "\n",
    "# Step 5: Create the replacement dictionary\n",
    "replacement_dict = {org: \"\" for org in set(problematic_orgs)}\n",
    "\n",
    "# Step 6: Print the replacement dictionary in the desired format\n",
    "print(\"replacement_dict = {\")\n",
    "for key in replacement_dict:\n",
    "    print(f'    \"{key}\": \"\",')\n",
    "print(\"}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affiliation Dictionary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affiliation at row 13249 (with blanks included): Institucio Catalana de Recerca i Estudis Avancats, ICREA; ETH Zurich; University of Autonoma de Barcelona\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig',low_memory=False)\n",
    "\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채tsklinikum Freiburg\", \"University ofsklinikum Freiburg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"T챕l챕com Paris\", \"Telecom Paris\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕t Wien\", \"University of Wien\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi della Campania Luigi Vanvitelli\", \"Universita degli Studi della Campania Luigi Vanvitelli\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Max Planck In짯sti짯tute for Mar짯ine Mi짯cro짯bi짯o짯logy\", \"Max Planck Institute for Marine Microbiology\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Franche-Comt챕\", \"Universite de Franche-Comte\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 du Qu챕bec 횪 Chicoutimi\", \"Universite du Quebec a Chicoutimi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Bundesanstalt f체r Geowissenschaften und Rohstoffe\", \"Bundesanstalt fur Geowissenschaften und Rohstoffe (BGR)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technische Universit채t Bergakademie Freiberg\", \"Technische University of Bergakademie Freiberg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut de Recherche en Informatique et Syst챔mes Al챕atoires\", \"Institut de Recherche en Informatique et Systemes Aleatoires (IRISA)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"H첩gskolen i Bergen\", \"Høgskolen i Bergen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Goethe-Universit채t Frankfurt am Main\", \"Goethe-University of Frankfurt am Main\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Aut처noma de Sinaloa\", \"Universidad Autonoma de Sinaloa\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횋cole Normale Sup챕rieure\", \"Ecole Normale Superieure\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Tecnol처gico de Monterrey\", \"Tecnologico de Monterrey\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"The Catholic University of Korea Eunpyeong St. Mary?셲 Hospital\", \"The Catholic University of Korea Eunpyeong St. Mary's Hospital\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Katolicki Uniwersytet Lubelski Jana Paw흢a II\", \"Katolicki Uniwersytet Lubelski Jana Pawla II\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Y캇ld캇z Teknik 횥niversitesi\", \"Yıldız Teknik Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instiutet f철r rymdfysik\", \"Institutet for rymdfysik\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut f체r Chemische Technologie von Materialien\", \"Institut fur Chemische Technologie von Materialien\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 di Pisa\", \"University of Pisa\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technische Universit채t Braunschweig\", \"Technical University of Braunschweig\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"National Research University ?쏮oscow Power Engineering Institute?? Voronezhskiy Gosudarstvenniy Universitet\", \"National Research University Moscow Power Engineering Institute Voronezh State University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Leibniz-Institut f체r Kristallz체chtung\", \"Leibniz-Institut fur Kristallzuchtung\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Hawai軻i at M훮noa\", \"University of Hawai‘i at Manoa\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Versailles Saint-Quentin-en-Yvelines\", \"Universite de Versailles Saint-Quentin-en-Yvelines\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Lausanne (UNIL)\", \"Universite de Lausanne (UNIL)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 d'Artois\", \"Universite d'Artois\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"H척pital Universitaire San Jo찾o\", \"Hopital Universitaire San Joao\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut de Chimie et Proc챕d챕s pour l'Energie, l'Environnement et la Sant챕 (ICPEES)\", \"Institut de Chimie et Procedes pour l'Energie, l'Environnement et la Sante (ICPEES)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Lule책 University of Technology\", \"Lulea University of Technology\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of M체nster\", \"University of Munster\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Max-Planck-Institut f체r Biochemie\", \"Max-Planck-Institut fur Biochemie\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 du Littoral C척te d?쁎pale\", \"University of the Littoral Opal Coast\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut de Neurosciences Cognitives et Int챕gratives d?섲quitaine\", \"Institut de Neurosciences Cognitives et Integratives d'Aquitaine\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Direction Sant챕 Confort\", \"Direction Sante Confort\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Leibniz-Institut f체r Naturstoff-Forschung und Infektionsbiologie e. V. ??Hans-Kn철ll-Institut\", \"Leibniz-Institut fur Naturstoff-Forschung und Infektionsbiologie e. V. Hans-Knoll-Institut\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Friedrich-Schiller-Universit채t Jena\", \"Friedrich-Schiller-University of Jena\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidade de Bras챠lia\", \"Universidade de Brasilia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Dipartimento di Fisica e Astronomia dell'Universit횪\", \"Dipartimento di Fisica e Astronomia dell'Universita\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instytut Metali Niezelaznych Oddzia흢 w Poznaniu Centralne Laboratorium Akumulator처w i Ogniw\", \"Instytut Metali Niezelaznych Oddzial w Poznaniu Centralne Laboratorium Akumulatorow i Ogniw\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Uniwersytet Gda흦ski\", \"University of Gdansk\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Rheinland-Pf채lzische Technische Universit채t Kaiserslautern-Landau\", \"Rheinland-Pfalzische Technische University of Kaiserslautern-Landau\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"National Research University ?쏮oscow Power Engineering Institute\", \"National Research University Moscow Power Engineering Institute\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Savoie Mont Blanc\", \"Universite Savoie Mont Blanc\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횋cole Polytechnique F챕d챕rale de Lausanne\", \"Ecole Polytechnique Federale de Lausanne\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Libre de Bruxelles\", \"Universite Libre de Bruxelles\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Firat 횥niversitesi\", \"Fırat Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut Ru휃er Bo큄kovi훶\", \"Institut Ruder BoSkovic\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Poitiers\", \"Universite de Poitiers\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technischen Universit채t Ilmenau\", \"Technische University of Ilmenau\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Tunis El Manar\", \"Universite de Tunis El Manar\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Polit챔cnica de Val챔ncia\", \"University of Politecnica de Valencia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Bauhaus-Universit채t Weimar\", \"Bauhaus-University of Weimar\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Bordeaux\", \"Universite de Bordeaux\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"C찼tedras CONACYT\", \"Catedras CONACYT\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Sassari\", \"Universita degli Studi di Sassari\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Helmholtz Zentrum f체r Umweltforschung\", \"Helmholtz Zentrum fur Umweltforschung\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Regensburg\", \"University of Regensburg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad T챕cnica de Babahoyo\", \"Technical University of Babahoyo\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CEA LETI혻\", \"CEA LETI\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"?cole Polytechnique F?d?rale de Lausanne\", \"Ecole Polytechnique Federale de Lausanne\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Siena\", \"Universita degli Studi di Siena\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 del Salento\", \"Universita del Salento\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"National Institute of Horticultural 竊?Herbal Science\", \"National Institute of Horticultural & Herbal Science\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 Telematica Internazionale UNINETTUNO\", \"Universita Telematica Internazionale UNINETTUNO\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad P첬blica de Navarra\", \"Public University of Navarre\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sveu훾ili큄te u Zagrebu, Prirodoslovno - Matemati훾ki Fakultet\", \"Faculty of Science, University of Zagreb\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidade de S찾o Paulo\", \"Universidade de São Paulo\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Fraunhofer-Institut f체r Angewandte Polymerforschung\", \"Fraunhofer-Institut fur Angewandte Polymerforschung\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Messina\", \"Universita degli Studi di Messina\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Polit챕cnica de Madrid\", \"Universidad Politecnica de Madrid\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Palermo\", \"Universita degli Studi di Palermo\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Eski힊ehir Osmangazi 횥niversitesi\", \"Eskisehir Osmangazi University, Meselik Campus\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Gottfried Wilhelm Leibniz Universit채t Hannover\", \"Gottfried Wilhelm Leibniz University of Hannover\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Catania\", \"Universita degli Studi di Catania\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut Sup챕rieur d'Electronique de Paris (ISEP)\", \"Institut Superieur d'Electronique de Paris (ISEP)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituto Federal de Educa챌찾o, Ci챗ncia e Tecnologia do Cear찼, Fortaleza\", \"Instituto Federal de Educacao, Ciencia e Tecnologia do Ceara, Fortaleza\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Martin-Luther-Universit채t Halle-Wittenberg\", \"Martin-Luther-University of Halle-Wittenberg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"EMBL?셲 European Bioinformatics Institute\", \"EMBL's European Bioinformatics Institute\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Karlsruher Institut f체r Technologie, Campus Nord\", \"Karlsruhe Institute of Technology Campus North\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Genova\", \"University of Genoa\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채tsmedizin Greifswald\", \"Greifswald Medical School\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"IRSN Institut de Radioprotection et de Surete Nucleaire\", \"IRSN Institut de Radioprotection et de Surete Nucleaire\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Alma Mater Studiorum Universit횪 di Bologna\", \"Alma Mater Studiorum Universita di Bologna\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Paris Cit챕\", \"Universite Paris Cite\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Tours\", \"Universite de Tours\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Abou Bekr Belkaid Tlemcen\", \"Universite Abou Bekr Belkaid Tlemcen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratoire de l'Accelerateur Lineaire\", \"Laboratoire de l'Accelerateur Lineaire\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centre Inria Sophia Antipolis - Mediterranee\", \"Centre Inria Sophia Antipolis - Mediterranee\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Medizinische Universit채t Graz\", \"Medizinische University of Graz\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Bilkent 횥niversitesi\", \"Bilkent Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technology Arts Sciences TH Koln\", \"Technology Arts Sciences TH Koln\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Transportøkonomisk institutt\", \"Institute of Transport Economics\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centro de Investigaciones Energeticas, Medioambientales y Tecnologicas\", \"Centre for Energy, Environmental and Technological Research (CIEMAT)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 di Trento\", \"Universita di Trento\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Saint-Joseph de Beyrouth\", \"Universite Saint-Joseph de Beyrouth\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Laval\", \"Universite Laval\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Technologie de Troyes\", \"Universite de Technologie de Troyes\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Creation and Love Women’s Hospital\", \"Creation and Love Women’s Hospital\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratorio de Instrumentação e Fisica Experimental de Particulas\", \"Laboratory of Instrumentation and Experimental Particles Physics\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Picardie Jules Verne\", \"Universite de Picardie Jules Verne\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Claude Bernard Lyon 1\", \"Universite Claude Bernard Lyon 1\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituto Superior Tecnico\", \"Instituto Superior Tecnico\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Organizacion Sanitaria Integrada Goierri - Alto Urola\", \"Organizacion Sanitaria Integrada Goierri - Alto Urola\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of des Saarlandes\", \"University of des Saarlandes\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Politechnika Wroclawska\", \"Politechnika Wroclawska\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Mons\", \"Universite de Mons\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Nantes Universit챕\", \"Nantes Universite\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Kyung-In Women?셲 University\", \"Kyung-In Women’s University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituto de Investigaci처n Sanitaria de la Fundaci처n Jim챕nez D챠az\", \"Instituto de Investigacion Sanitaria de la Fundacion Jimenez Diaz\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Cagliari\", \"Universita degli Studi di Cagliari\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 Cattolica del Sacro Cuore, Campus di Brescia\", \"Universita Cattolica del Sacro Cuore, Campus di Brescia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Paris-Saclay\", \"Universite Paris-Saclay\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centro de Investigaci처n Biom챕dica en Red de C찼ncer\", \"Centro de Investigacion Biomedica en Red de Cancer\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채tsklinikum Heidelberg\", \"University Hospital Heidelberg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut Universitaire de Radiophysique Appliqu챕e\", \"Institut Universitaire de Radiophysique Appliquee\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituto de Bioingenier챠a de Catalu챰a\", \"Institute for Bioengineering of Catalonia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 della Calabria\", \"Universita della Calabria\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Napoli Federico II\", \"Universita degli Studi di Napoli Federico II\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Link철pings Universitet\", \"Linkopings Universitet\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut Mines T챕l챕com\", \"Institut Mines Telecom\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratorio Nacional de Fusi처n\", \"Laboratorio Nacional de Fusion\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Padova\", \"Universita degli Studi di Padova\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Organisation Europ챕enne pour la Recherche Nucl챕aire\", \"Organisation Europeenne pour la Recherche Nucleaire\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Lyon\", \"Universite de Lyon\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of de Val챔ncia\", \"University of de Valencia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Bundesamt f체r Seeschiffahrt und Hydrographie\", \"Bundesamt fur Seeschifffahrt und Hydrographie\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut F챕d챕ratif de Recherche 49\", \"Institut Federatif de Recherche 49\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituto Nacional de Neurolog챠a y Neurocirug챠a\", \"National Institute of Neurology and Neurosurgery\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut de la Micro챕lectronique, Electromagn챕tisme et Photonique - Laboratoire d'Hyperfr챕quences et de Caract챕risation\", \"Institut de la Microelectronique, Electromagnetisme et Photonique - Laboratoire d'Hyperfrequences et de Caracterisation\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Rutgers University?밡ew Brunswick\", \"Rutgers University-New Brunswick\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Zentrum f체r Regenerative Therapien Dresden\", \"Zentrum fur Regenerative Therapien Dresden\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Max-Planck-Institut f체r Kolloid- und Grenzfl채chenforschung\", \" Max Planck Institute of Colloids and Interfaces\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Helmholtz?밵entrum Geesthacht\", \"Helmholtz-Zentrum Geesthacht\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횜rebro Universitet\", \"Orebro Universitet\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratoire des Sciences des Proc챕d챕s et des Mat챕riaux\", \"Laboratoire des Sciences des Procedes et des Materiaux\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centro Brasileiro de Pesquisas F챠sicas\", \"Centro Brasileiro de Pesquisas Fisicas\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Clermont Auvergne\", \"Universite Clermont Auvergne\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Bergische Universit채t Wuppertal\", \"Bergische University of Wuppertal\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad de Alcal찼\", \"Universidad de Alcala\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad de Le처n\", \"Universidad de Leon\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Z체rich\", \"University of Zurich\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Fundaci처n Agencia Aragonesa para la Investigaci처n y el Desarrollo (ARAID)\", \"Fundacion Agencia Aragonesa para la Investigacion y el Desarrollo (ARAID)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Facult챕 des Sciences Semlalia\", \"Faculte des Sciences Semlalia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횋cole Nationale Sup챕rieure d'Ing챕nieurs de Tunis\", \"Ecole Nationale Superieure d'Ingenieurs de Tunis\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Queen?셲 University\", \"Queen’s University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Ludwig-Maximilians-Universit채t M체nchen\", \"Ludwig-Maximilians-University of Munchen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"H척pital Saint-Antoine\", \"Hospital Saint-Antoine Ap-Hp\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"G챕olocalisation (AME-GEOLOC)\", \"Geolocalisation (AME-GEOLOC)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidade Federal do Piau챠\", \"Universidade Federal do Piaui\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"D梳죍 h沼뛠 Nguyen Tat Thanh\", \"Nguyen Tat Thanh University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Klinikum der Universit채t Regensburg und Medizinische Fakult채t\", \"Klinikum der University of Regensburg und Medizinische Fakultat\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi dell'Aquila\", \"Universita degli Studi dell'Aquila\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Rheinisch-Westf채lische Technische Hochschule Aachen\", \"Rheinisch-Westfalische Technische Hochschule Aachen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Karlsruher Institut f체r Technologie\", \"Karlsruher Institut fur Technologie\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Trieste\", \"Universita degli Studi di Trieste\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centro de Investigacion y de Estudios Avanzados del Instituto Polit챕cnico Nacional\", \"Centro de Investigacion y de Estudios Avanzados del Instituto Politecnico Nacional\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Strasbourg\", \"Universite de Strasbourg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Rzesz처w\", \"University of Rzeszow\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Hochschule F횄쩌r Technik Stuttgart\", \"Hochschule fur Technik Stuttgart\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Univerzita Palack?ho v Olomouci\", \"Univerzita Palackeho v Olomouci\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Toulouse III - Paul Sabatier\", \"Universite Toulouse III - Paul Sabatier\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Redeemer?쁲 University\", \"Redeemer's University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Tr튼沼쓓g 휂梳죍 h沼뛠 S튼 ph梳죑 H횪 N沼셢\", \"Hanoi National University of Education\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Enna\", \"University of Enna\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Perpignan Via Domitia\", \"Universite de Perpignan Via Domitia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Interfaces Traitements Organisation et DYnamique des Syst챔mes ??ITODYS\", \"Interfaces Traitements Organisation et DYnamique des Systemes (ITODYS)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Bo휓azi챌i 횥niversitesi\", \"Boğaziçi Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Aut처noma de Chiapas\", \"Universidad Autonoma de Chiapas\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centro de Investigaci?n y de Estudios Avanzados del Instituto Polit?cnico Nacional\", \"Centro de Investigacion y de Estudios Avanzados del Instituto Politecnico Nacional\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Brandenburgische Technische Universit채t Cottbus\", \"Brandenburgische Technische University of Cottbus\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Tartu 횥likooli Genoomika Instituut\", \"Tartu Ulikooli Genoomika Instituut\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Alfred-Wegener-Institut Helmholtz-Zentrum f체r Polar- und Meeresforschung\", \"Alfred-Wegener-Institut Helmholtz-Zentrum fur Polar- und Meeresforschung\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Christian-Albrechts-Universit채t zu Kiel\", \"Christian-Albrechts-University of zu Kiel\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Nacional de C처rdoba\", \"Universidad Nacional de Cordoba\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sveu훾ili큄te u Zagrebu, Geodetski fakultet\", \"SveuciliSte u Zagrebu, Geodetski fakultet\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Interactions h척tes-agents pathog챔nes - (IHAP)\", \"Interactions hôtes-agents pathogenes - (IHAP)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Mayor de San Andr챕s\", \"Universidad Mayor de San Andres\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Perugia\", \"Universita degli Studi di Perugia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Freie Universit채t Berlin\", \"Freie University of Berlin\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"D챕partement d'Informatique de l'ENS\", \"Departement d'Informatique de l'ENS\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Gangnam St. Mary?셲 One Eye Clinic\", \"Gangnam St. Mary's One Eye Clinic\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채tsklinikum Hamburg-Eppendorf\", \"University Medical Center Hamburg-Eppendorf \")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Muse챕 Canadien de la Nature\", \"Musee Canadien de la Nature\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Norges Milj첩- og Biovitenskapelige Universitet\", \"Norwegian University of Life Sciences\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Global Core Research Center for Ship and Offshore Plants (GCRE?륲OP)\", \"Global Core Research Center for Ship and Offshore Plants (GCRC-SOP)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centre de Recherche et des Technologies de L'Energie (CRTEn) BorjCedria B.P N째952050-Hammam Lif.\", \"Centre de Recherche et des Technologies de l'Energie (CRTEn) BorjCedria B.P N°952050-Hammam Lif.\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technische Universit채t Berlin\", \"Technische University of Berlin\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Consejo Superior de Investigaciones Cient챠ficas\", \"Consejo Superior de Investigaciones Cientificas\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Toulouse\", \"Universite de Toulouse\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Wroc흢aw University of Environmental and Life Sciences\", \"Wroclaw University of Environmental and Life Sciences\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"G철teborgs Universitet\", \"Goteborgs Universitet\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Lorraine\", \"Universite de Lorraine\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CHU de Qu챕bec-Universit챕 Laval\", \"CHU de Quebec-Universite Laval\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"T횥B캅TAK Ulusal Metroloji Enstit체s체\", \"TUBİTAK Ulusal Metroloji Enstitusu\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Univerzita Jana Evangelisty Purkyne v 횣st챠 nad Labem\", \"University of Jan Evangelista in Usti nad Labem (UJEP)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CSIC-UNIOVI-Principado de Asturias - Centro de Investigaci처n en Nanomateriales y Nanotecnolog챠a (CINN)\", \"CSIC-UNIOVI-Principado de Asturias - Centro de Investigacion en Nanomateriales y Nanotecnologia (CINN)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technische Universit채t Chemnitz\", \"Technische University of Chemnitz\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Bundesanstalt f체r Materialforschung und -Pr체fung\", \"Bundesanstalt fur Materialforschung und -Prufung\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi dell'Insubria\", \"Universita degli Studi dell'Insubria\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Philipps-Universit채t Marburg\", \"Philipps-University of Marburg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technick찼 Univerzita v Ko큄iciach\", \"Technicka Univerzita v KoSiciach\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad de Ja챕n\", \"Universidad de Jaen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Greifswald\", \"University of Greifswald\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CSIC - Instituto de 횙ptica Daza de Vald챕s (IO)\", \"CSIC - Instituto de Optica Daza de Valdes (IO)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad de Sevilla, Escuela T챕cnica Superior de Ingenier챠a\", \"Universidad de Sevilla, Escuela Tecnica Superior de Ingenieria\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Gustave Eiffel\", \"Universite Gustave Eiffel\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Catholique de Louvain\", \"Universite Catholique de Louvain\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"M챕t챕o France\", \"Meteo France\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Uniwersytet Jagiello흦ski\", \"Uniwersytet Jagiellonski\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratoire d'Oc챕anographie et du Climat : Exp챕rimentations et Approches Num챕riques\", \"Laboratoire d'Oceanographie et du Climat : Experimentations et Approches Numeriques\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"H철gskolan i Bor책s\", \"Hogskolan i Boras\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Trier\", \"University of Trier\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CICECO ??Instituto de Materiais de Aveiro\", \"CICECO – Instituto de Materiais de Aveiro\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Aix Marseille Universit챕\", \"Aix Marseille Universite\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Forschungszentrum J체lich GmbH\", \"Forschungszentrum Julich GmbH\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidade Federal do Esp챠rito Santo\", \"Universidade Federal do Espirito Santo\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratoire Aim챕 Cotton\", \"Laboratoire Aime Cotton\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CRISMAT - Laboratoire de Crystallographie et Sciences des Mat챕riaux\", \"CRISMAT - Laboratoire de Crystallographie et Sciences des Materiaux\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituto Polit챕cnico Nacional\", \"Instituto Politecnico Nacional\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Cheil General Hospital and Women?셲 Healthcare Center\", \"Cheil General Hospital and Women’s Healthcare Center\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Siegen\", \"University of Siegen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"K첩benhavns Universitet\", \"Kobenhavns Universitet\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횋cole Normale Sup챕rieure Paris-Saclay\", \"Ecole Normale Superieure Paris-Saclay\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Eberhard Karls Universit채t T체bingen\", \"Eberhard Karls University of Tubingen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 C척te d'Azur\", \"Universite Cote d'Azur\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut 'Jo탑ef Stefan'\", \"Institut Jozef Stefan\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Stuttgart\", \"University of Stuttgart\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Z체hlke Group\", \"Zuhlke Group\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"National Technical University of Ukraine ?쏧gor Sikorsky Kyiv Polytechnic Institute??Temperature dependence of the excitonic energy band gap in In(Ga)As nanostructures\", \"National Technical University of Ukraine – Igor Sikorsky Kyiv Polytechnic Institute\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Leibniz-Institut f체r Festk철rper- und Werkstoffforschung Dresden\", \"Leibniz-Institut fur Festkorper- und Werkstoffforschung Dresden\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Tr튼沼쓓g 휂梳죍 h沼뛠 C척ng nghi沼뇈 th횪nh ph沼?H沼?Ch챠 Minh\", \"HCMC University of Technology (HUTECH) – Sai Gon Campus\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Erciyes 횥niversitesi\", \"Erciyes Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centro de Investiga??o em Sistemas Confi?veis e de Tempo Real\", \"Centro de Investigacao em Sistemas Confiaveis e de Tempo Real\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Hanyang Women?셲 University\", \"Hanyang Women's University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Tartu 횥likool\", \"Tartu Ulikool\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli studi di Bari Aldo Moro\", \"Universita degli Studi di Bari Aldo Moro\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 Ca' Foscari Venezia\", \"Universita Ca' Foscari Venezia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횋lectroniques, Syst챔mes de Communication et Microsyst챔mes (ESYCOM)\", \"Electroniques, Systemes de Communication et Microsystemes (ESYCOM)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Yak캇n Do휓u 횥niversitesi\", \"Yakin Dogu Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sveu훾ili큄te u Zagrebu, Fakultet Strojarstva i Brodogradnje\", \"SveuciliSte u Zagrebu, Fakultet Strojarstva i Brodogradnje\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Fraunhofer-Institut f체r Nachrichtentechnik Heinrich-Hertz-Institut\", \"Fraunhofer-Institut fur Nachrichtentechnik Heinrich-Hertz-Institut\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Gebze Teknik 횥niversitesi\", \"Gebze Teknik Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Jyv채skyl채\", \"University of Jyvaskyla\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Benem챕rita Universidad Aut처noma de Puebla\", \"Benemerita Universidad Autonoma de Puebla\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Grenoble Alpes\", \"Universite Grenoble Alpes\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Medizinische Universit채t Wien\", \"Medizinische University of Wien\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Leibniz-Institut f?r Festk?rper- und Werkstoffforschung Dresden\", \"Leibniz-Institut fur Festkorper- und Werkstoffforschung Dresden\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sapienza Universit횪 di Roma\", \"Sapienza Universita di Roma\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"휂梳죍 h沼뛠 M沼?Th횪nh ph沼?H沼?Ch챠 Minh\", \"HCMC Open University - Campus 1\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Basel\", \"University of Basel\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Max-Planck-Institut f체r Eisenforschung GmbH\", \"Max-Planck-Institut fur Eisenforschung GmbH\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratoire Plasma et Conversion d?섷nergie\", \"Laboratoire Plasma et Conversion d'Energie\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Soomyung Women?셲 University\", \"Soomyung Women's University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Nebraska?밚incoln\", \"University of Nebraska–Lincoln\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Bonn\", \"University of Bonn\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Technische Universit채t Dresden\", \"Technische University of Dresden\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut Lumi챔re Mati챔re\", \"Institut Lumiere Matiere\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Ahmed Draia - Adrar\", \"Universite Ahmed Draia - Adrar\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Aut처noma de Madrid\", \"Universidad Autonoma de Madrid\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"J체lich Aachen Research Alliance (JARA)-Fundamentals of Future Information Technology\", \"Julich Aachen Research Alliance (JARA)-Fundamentals of Future Information Technology\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sorbonne Universit챕\", \"Sorbonne Universite\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Montpellier\", \"Universite de Montpellier\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut de Recerca contra la Leuc챔mia Josep Carreras (IJC)\", \"Institut de Recerca contra la Leucemia Josep Carreras (IJC)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"A?셎harqiyah University\", \"A’Sharqiyah University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Gen챔ve\", \"Universite de Geneve\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidade Tecnol처gica Federal do Paran찼\", \"Universidade Tecnologica Federal do Parana\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Gesellschaft f체r Anlagen- und Reaktorsicherheit mbH\", \"Gesellschaft fur Anlagen- und Reaktorsicherheit mbH\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 Badji Mokhtar - Annaba\", \"Universite Badji Mokhtar - Annaba\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Abdullah G체l 횥niversitesi\", \"Abdullah Gul Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sj챈llands Universitetshospital\", \"Roskilde Sygehus\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut des Mat챕riaux Poreux de Paris IMAP\", \"Institut des Materiaux Poreux de Paris IMAP\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Polit챕cnica de Catalunya\", \"University of Politecnica de Catalunya\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Augsburg\", \"University of Augsburg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Deutsches Zentrum f체r Luft- und Raumfahrt (DLR)\", \"Deutsches Zentrum fur Luft- und Raumfahrt (DLR)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Milano-Bicocca\", \"Universita degli Studi di Milano-Bicocca\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Jijel\", \"Universite de Jijel\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut Parisien de Chimie Mol챕culaire\", \"Institut Parisien de Chimie Moleculaire\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 McGill\", \"Universite McGill\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횋cole de Technologie Sup챕rieure\", \"Ecole de Technologie Superieure\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채tsklinikum des Saarlandes Medizinische Fakult채t der Universit채t des Saarlandes\", \" Saarland University Hospital\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Aut처noma del Estado de Morelos\", \"Universidad Autonoma del Estado de Morelos\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sardar Bahadur Khan Women?셲 University (SBKWU)\", \"Sardar Bahadur Khan Women’s University (SBKWU)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Dumlupinar 횥niversitesi\", \"Dumlupinar Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Charit챕 ??Universit채tsmedizin Berlin\", \"Charite - Universitatsmedizin Berlin\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Georg-August-Universit채t G철ttingen\", \"Georg-August-University of Gottingen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채tsmedizin G철ttingen\", \"University Medicine Gottingen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Zentrum f체r Technologietransfer und Telekommunikation (ZTT)\", \"Zentrum fur Technologietransfer und Telekommunikation (ZTT)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidade Federal de Uberl창ndia\", \"Universidade Federal de Uberlandia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Ministerio de Salud P첬blica\", \"Ministerio de Salud Publica\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Aut처noma del Estado de M챕xico\", \"Universidad Autonoma del Estado de Mexico\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"ARMTEC Tecnologia em Rob처tica\", \"ARMTEC Tecnologia em Robotica\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Innsbruck\", \"University of Innsbruck\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"It채-Suomen yliopisto\", \"Ita-Suomen yliopisto\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Helmholtz-Zentrum Berlin f체r Materialien und Energie (HZB)\", \"Helmholtz-Zentrum Berlin fur Materialien und Energie (HZB)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Medizinisches Laserzentrum L체beck\", \"Medizinisches Laserzentrum Lubeck\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"E철tv철s Lor찼nd Tudom찼nyegyetem\", \"Eotvos Lorand Unievrsity\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Necmettin Erbakan 횥niversitesi\", \"Necmettin Erbakan Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 d'Orl챕ans\", \"Universite d'Orleans\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Tunis El Manar, Ecole Nationale d'Ing챕nieurs de Tunis\", \"Universite de Tunis El Manar, Ecole Nationale d'Ingenieurs de Tunis\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratoire de Physique et d'횋tude des Mat챕riaux\", \"Laboratoire de Physique et d'Etude des Materiaux\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Hamburg\", \"University of Hamburg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut d'Investigaci처 Biomedica de Bellvitge\", \"Institut d'Investigacio Biomedica de Bellvitge\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Torino\", \"Universita degli Studi di Torino\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut des Mol챕cules et Mat챕riaux du Mans\", \"Institut des Molecules et Materiaux du Mans\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 Cattolica del Sacro Cuore, Campus di Roma\", \"Universita Cattolica del Sacro Cuore, Campus di Roma\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Friedrich-Alexander-Universit채t Erlangen-N체rnberg\", \"Friedrich-Alexander-University of Erlangen-Nurnberg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituto de Telecomunica챌천es\", \"Instituto de Telecomunicacoes\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Ume책 Universitet\", \"Umea Universitet\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Fundaci처 Institut Universitari per a la recerca a l'Atenci처 Prim횪ria de Salut Jordi Gol i Gurina (IDIAPJGol)\", \"Fundacio Institut Universitari per a la recerca a l'Atencio Primaria de Salut Jordi Gol i Gurina (IDIAPJGol)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Dongduck Women?셲 University\", \"Dongduk Women's University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Narodowe Centrum Bada흦 J훳drowych, Otwock\", \"National Centre for Nuclear Research\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Fraunhofer-Institut F체r Organische Elektronik, Elektronenstrahl- Und Plasmatechnik\", \"Fraunhofer-Institut fur Organische Elektronik, Elektronenstrahl- und Plasmatechnik\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Tr튼沼쓓g 휂梳죍 h沼뛠 Giao th척ng v梳춏 t梳즜\", \"University of Transport and Communications\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Freiburg\", \"University of Freiburg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Bremen\", \"University of Bremen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 du Qu챕bec 횪 Montr챕al\", \"Universite du Quebec a Montreal\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Leipzig\", \"University of Leipzig\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 d'Evry Val d'Essonne\", \"Universite d'Evry Val d'Essonne\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"D챕partement de Mod챕lisation des Syst챔mes et Structures\", \"Departement de Modelisation des Systemes et Structures\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"O?셄eal Comprehensive Cancer Center\", \"Oregon Comprehensive Cancer Center\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Kyungin Women?셲 University\", \"Kyungin Women's University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Sookmyung Women?셲 University\", \"Sookmyung Women's University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Klinikum der Universit채t M체nchen\", \"Klinikum der University of Munchen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"횇ngstr철mlaboratoriet\", \"Angstrom Laboratory\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CISE-Centro de Investiga챌찾o em Sistemas Electromecatr처nicos\", \"CISE-Centro de Investigacao em Sistemas Eletromecatronicos\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universitat Aut챵noma de Barcelona\", \"University of Autonoma de Barcelona\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Laboratoire National des Champs Magn챕tiques Intenses (LNCMI)\", \"Laboratoire National des Champs Magnetiques Intenses (LNCMI)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Duisburg-Essen\", \"University of Duisburg-Essen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Ecole Nationale Sup챕rieure de Chimie de Rennes\", \"Ecole Nationale Superieure de Chimie de Rennes\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t zu K철ln\", \"University of Cologne\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Uniwersytet 힃l훳ski w Katowicach\", \" University of Silesia in Katowice\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"CSIC-INTA - Centro de Astrobiolog챠a (CAB)\", \"CSIC-INTA - Centro de Astrobiologia (CAB)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Helmholtz-Institute Erlangen-N체renberg for Renewable Energy (HI ERN)\", \"Helmholtz-Institut Erlangen-Nurnberg fur Erneuerbare Energien (HI ERN)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"La Rochelle Universit챕\", \"La Rochelle Universite\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"ETH Z체rich\", \"ETH Zurich\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Julius-Maximilians-Universit채t W체rzburg\", \"Julius-Maximilians-University of Wurzburg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centrale M챕diterran챕e\", \"Centrale Mediterranee\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"National Technical University of Ukraine ?쏧gor Sikorsky Kyiv Polytechnic Institute??\", \"National Technical University of Ukraine Igor Sikorsky Kyiv Polytechnic Institute\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centre de Recherches en Canc챕rologie de Toulouse\", \"Centre de Recherches en Cancerologie de Toulouse\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centre 횋nergie Mat챕riaux T챕l챕communications\", \"Centre Energie Materiaux Telecommunications\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Uniwersytet Miko흢aja Kopernika w Toruniu\", \"Nicolaus Copernicus University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Wroc흢aw\", \"University of Wroclaw\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"T챕l챕com SudParis\", \"Telecom SudParis\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채tsklinikum M체nster\", \"Munster University Hospital\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centre de recherche du CHU de Qu챕bec-Universit챕 Laval\", \"Centre de recherche du CHU de Quebec-Universite Laval\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Univerzita Komensk챕ho v Bratislave\", \"Comenius University Bratislava\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Humboldt-Universit채t zu Berlin\", \"Humboldt-University of zu Berlin\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Medizinische Universit채t Wien, Zentrum f체r Medizinische Physik und Biomedizinische Technik\", \"Medizinische University of Wien, Zentrum fur Medizinische Physik und Biomedizinische Technik\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 des Sciences et de la Technologie Houari Boumediene\", \"Universite des Sciences et de la Technologie Houari Boumediene\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Medizinische Fakult채t, RWTH Aachen University\", \"Medizinische Fakultat, RWTH Aachen University\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Foggia\", \"Universita degli Studi di Foggia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"B체hler AG\", \"Buhler AG\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Justus-Liebig-Universit채t Gie횩en\", \"Justus-Liebig-University of Giessen\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Szegedi Tudom찼nyegyetem (SZTE)\", \"Szegedi Tudomanyegyetem (SZTE)\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Brescia\", \"Universita degli Studi di Brescia\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Potsdam\", \"University of Potsdam\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"IN2P3 - Institut National de Physique Nucl챕aire et de Physique Des Particules\", \"IN2P3 - Institut National de Physique Nucleaire et de Physique des Particules\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Centre de D챕veloppement des Technologies Avanc챕es\", \"Advanced Technology Development Centre\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Heidelberg\", \"University of Heidelberg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Erzincan Binali Y캇ld캇r캇m 횥niversitesi\", \"Erzincan Binali Yıldırım Universitesi\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Pavol Jozef 힋af찼rik University in Ko큄ice\", \"Pavol Jozef Safarik University in Kosice\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Pusan ?뗢딳ational University Dental Hospital\", \"Pusan National University Dental Hospital\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Roma\", \"Universita degli Studi di Roma\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"ECPM 횋cole Europ챕enne de Chimie, Polym챔res et Mat챕riaux de Strasbourg\", \"ECPM Ecole Europeenne de Chimie, Polymeres et Materiaux de Strasbourg\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Neuch창tel\", \"Universite de Neuchatel\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universidad Nacional Aut처noma de M챕xico\", \"Universidad Nacional Autonoma de Mexico\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Heinrich-Heine-Universit채t D체sseldorf\", \"Heinrich-Heine-University Dusseldorf\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit채t Bayreuth\", \"University of Bayreuth\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"University of Jyv?skyl?\", \"University of Jyvaskyla\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit챕 de Lille\", \"Universite de Lille\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Institut de Recherches Cliniques de Montr챕al\", \"Institut de Recherches Cliniques de Montreal\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Universit횪 degli Studi di Salerno\", \"University of Salerno\")\n",
    "df['Affiliation'] = df['Affiliation'].str.replace(\"Instituci처 Catalana de Recerca i Estudis Avan챌ats\", \"Institucio Catalana de Recerca i Estudis Avancats, ICREA\")\n",
    "\n",
    "# Step 4: Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('Paper_Dataset_Dict.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Step 3: Retrieve the 13249th affiliation (index starts from 0, so we use 13248)\n",
    "affiliation_13249 = df.iloc[13247]['Affiliation']  # iloc counts all rows including blanks\n",
    "print(f\"Affiliation at row 13249 (with blanks included): {affiliation_13249}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Paper용 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affiliation만 비어있는 개수: 2123\n",
      "Authors만 비어있는 개수: 4327\n",
      "둘다 비어있는 개수: 1744\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 2: Count missing values based on the conditions\n",
    "affiliation_only_missing_count = df['Affiliation'].isna().sum() - df['Authors'].isna().sum()\n",
    "authors_only_missing_count = df['Authors'].isna().sum() - df['Affiliation'].isna().sum()\n",
    "both_missing_count = df[(df['Affiliation'].isna()) & (df['Authors'].isna())].shape[0]\n",
    "\n",
    "# Step 3: Print the results\n",
    "print(f\"Affiliation만 비어있는 개수: {df['Affiliation'].isna().sum() - both_missing_count}\")\n",
    "print(f\"Authors만 비어있는 개수: {df['Authors'].isna().sum() - both_missing_count}\")\n",
    "print(f\"둘다 비어있는 개수: {both_missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "육T관련기술 = 1\n",
      "Affiliation만 비어있는 개수: 670\n",
      "Authors만 비어있는 개수: 1022\n",
      "둘다 비어있는 개수: 611\n",
      "------------------------------\n",
      "육T관련기술 = 2\n",
      "Affiliation만 비어있는 개수: 199\n",
      "Authors만 비어있는 개수: 638\n",
      "둘다 비어있는 개수: 291\n",
      "------------------------------\n",
      "육T관련기술 = 3\n",
      "Affiliation만 비어있는 개수: 196\n",
      "Authors만 비어있는 개수: 329\n",
      "둘다 비어있는 개수: 140\n",
      "------------------------------\n",
      "육T관련기술 = 4\n",
      "Affiliation만 비어있는 개수: 13\n",
      "Authors만 비어있는 개수: 40\n",
      "둘다 비어있는 개수: 9\n",
      "------------------------------\n",
      "육T관련기술 = 5\n",
      "Affiliation만 비어있는 개수: 643\n",
      "Authors만 비어있는 개수: 1327\n",
      "둘다 비어있는 개수: 473\n",
      "------------------------------\n",
      "육T관련기술 = 6\n",
      "Affiliation만 비어있는 개수: 19\n",
      "Authors만 비어있는 개수: 41\n",
      "둘다 비어있는 개수: 9\n",
      "------------------------------\n",
      "육T관련기술 = 7\n",
      "Affiliation만 비어있는 개수: 383\n",
      "Authors만 비어있는 개수: 929\n",
      "둘다 비어있는 개수: 209\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 2: Function to count missing values for Affiliation and Authors for each 육T관련기술 group\n",
    "def count_missing_by_group(group):\n",
    "    both_missing_count = group[(group['Affiliation'].isna()) & (group['Authors'].isna())].shape[0]\n",
    "    affiliation_only_missing_count = group['Affiliation'].isna().sum() - both_missing_count\n",
    "    authors_only_missing_count = group['Authors'].isna().sum() - both_missing_count\n",
    "    \n",
    "    return affiliation_only_missing_count, authors_only_missing_count, both_missing_count\n",
    "\n",
    "# Step 3: Loop over each 육T관련기술 value (1 to 7) and count missing values\n",
    "for value in range(1, 8):\n",
    "    group = df[df['육T관련기술'] == value]\n",
    "    \n",
    "    affiliation_only_missing, authors_only_missing, both_missing = count_missing_by_group(group)\n",
    "    \n",
    "    print(f\"육T관련기술 = {value}\")\n",
    "    print(f\"Affiliation만 비어있는 개수: {affiliation_only_missing}\")\n",
    "    print(f\"Authors만 비어있는 개수: {authors_only_missing}\")\n",
    "    print(f\"둘다 비어있는 개수: {both_missing}\")\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journal\n",
      "JOURNAL OF HIGH ENERGY PHYSICS               222\n",
      "JOURNAL OF NANOSCIENCE AND NANOTECHNOLOGY    203\n",
      "PHYSICS LETTERS B                            162\n",
      "JOURNAL OF ALLOYS AND COMPOUNDS              120\n",
      "ACS APPLIED MATERIALS & INTERFACES           105\n",
      "                                            ... \n",
      "Antioxidants                                   1\n",
      "JOURNAL OF CRANIOFACIAL SURGERY                1\n",
      "Micro & Nano Letters                           1\n",
      "GLOBAL CHALLENGES                              1\n",
      "ANNALS OF THORACIC SURGERY                     1\n",
      "Name: count, Length: 1720, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter rows where either 'Affiliation' or 'Authors' is empty\n",
    "filtered_df = df[df['Affiliation'].isnull() | df['Authors'].isnull()]\n",
    "\n",
    "# Step 2: Group by 'Journal' and count the occurrences\n",
    "journal_counts = filtered_df['Journal'].value_counts()\n",
    "\n",
    "# Step 3: Display the result\n",
    "print(journal_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SJR Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JOURNAL OF ENVIRONMENTAL RADIOACTIVITY', 'JOURNAL OF THE AMERICAN SOCIETY FOR MASS SPECTROMETRY', 'JOURNAL OF HAZARDOUS MATERIALS', 'ATW-INTERNATIONAL JOURNAL FOR NUCLEAR POWER', 'ANNALS OF NUCLEAR ENERGY', 'SCIENCE AND TECHNOLOGY OF NUCLEAR INSTALLATIONS', 'INTERNATIONAL JOURNAL OF PRESSURE VESSELS AND PIPING', 'SWISS JOURNAL OF GEOSCIENCES', 'NUCLEAR ENGINEERING AND TECHNOLOGY', 'NUCLEAR INSTRUMENTS & METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT']\n",
      "Total unique journals: 3922\n"
     ]
    }
   ],
   "source": [
    "## Unique jounral 추출\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 1: Extract unique journal names\n",
    "unique_journals = df['Journal'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Display the first 10 unique journal names and the total count of unique journals\n",
    "unique_journals_list = unique_journals.tolist()\n",
    "unique_journals_count = len(unique_journals_list)\n",
    "\n",
    "print(unique_journals_list[:10])  # Print first 10 unique journal names\n",
    "print(f\"Total unique journals: {unique_journals_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search URL: https://www.scimagojr.com/journalsearch.php?q=Journal+of+Environmental+Radioactivity\n",
      "Journal Link: https://www.scimagojr.com/journalsearch.php?q=23388&tip=sid&clean=0\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_journal_link(journal_name):\n",
    "#     # Prepare the search URL\n",
    "#     search_url = f\"https://www.scimagojr.com/journalsearch.php?q={journal_name.replace(' ', '+')}\"\n",
    "#     print(f\"Search URL: {search_url}\")  # Print search URL\n",
    "\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "#     }\n",
    "    \n",
    "#     # Fetch the search results page\n",
    "#     response = requests.get(search_url, headers=headers)\n",
    "    \n",
    "#     # Check if the request was successful\n",
    "#     if response.status_code != 200:\n",
    "#         print(\"Failed to retrieve the webpage.\")\n",
    "#         return None\n",
    "    \n",
    "#     # Parse the HTML content\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "#     # Find the first journal link\n",
    "#     first_journal = soup.select_one('a[href*=\"journalsearch.php?q=\"]')\n",
    "    \n",
    "#     # Check if the link was found\n",
    "#     if first_journal:\n",
    "#         # Construct the full URL\n",
    "#         journal_url = f\"https://www.scimagojr.com/{first_journal['href']}\"\n",
    "#         return journal_url\n",
    "#     else:\n",
    "#         print(\"Journal not found.\")\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# journal_name = \"Journal of Environmental Radioactivity\"\n",
    "# link = get_journal_link(journal_name)\n",
    "# if link:\n",
    "#     print(\"Journal Link:\", link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 (get journal link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/3922 journals.\n",
      "Processed 200/3922 journals.\n",
      "Processed 300/3922 journals.\n",
      "Processed 400/3922 journals.\n",
      "Processed 500/3922 journals.\n",
      "Processed 600/3922 journals.\n",
      "Processed 700/3922 journals.\n",
      "Processed 800/3922 journals.\n",
      "Processed 900/3922 journals.\n",
      "Processed 1000/3922 journals.\n",
      "Processed 1100/3922 journals.\n",
      "Processed 1200/3922 journals.\n",
      "Processed 1300/3922 journals.\n",
      "Processed 1400/3922 journals.\n",
      "Processed 1500/3922 journals.\n",
      "Processed 1600/3922 journals.\n",
      "Processed 1700/3922 journals.\n",
      "Processed 1800/3922 journals.\n",
      "Processed 1900/3922 journals.\n",
      "Processed 2000/3922 journals.\n",
      "Processed 2100/3922 journals.\n",
      "Processed 2200/3922 journals.\n",
      "Processed 2300/3922 journals.\n",
      "Processed 2400/3922 journals.\n",
      "Processed 2500/3922 journals.\n",
      "Processed 2600/3922 journals.\n",
      "Processed 2700/3922 journals.\n",
      "Processed 2800/3922 journals.\n",
      "Processed 2900/3922 journals.\n",
      "Processed 3000/3922 journals.\n",
      "Processed 3100/3922 journals.\n",
      "Processed 3200/3922 journals.\n",
      "Processed 3300/3922 journals.\n",
      "Processed 3400/3922 journals.\n",
      "Processed 3500/3922 journals.\n",
      "Processed 3600/3922 journals.\n",
      "Processed 3700/3922 journals.\n",
      "Processed 3800/3922 journals.\n",
      "Processed 3900/3922 journals.\n",
      "Processed 3922/3922 journals.\n",
      "Number of journals with no links: 134\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Get unique journal names from the entire dataset\n",
    "unique_journals = df['Journal'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def get_journal_link(journal_name):\n",
    "    # Prepare the search URL\n",
    "    search_url = f\"https://www.scimagojr.com/journalsearch.php?q={journal_name.replace(' ', '+')}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Fetch the search results page\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the first journal link\n",
    "    first_journal = soup.select_one('a[href*=\"journalsearch.php?q=\"]')\n",
    "    \n",
    "    # Check if the link was found\n",
    "    if first_journal:\n",
    "        # Construct the full URL\n",
    "        journal_url = f\"https://www.scimagojr.com/{first_journal['href']}\"\n",
    "        return journal_url\n",
    "    else:\n",
    "        return None  # Return None if no link was found\n",
    "\n",
    "# Create a DataFrame to store the journal links for all unique journals\n",
    "journal_links = []\n",
    "total_count = len(unique_journals)\n",
    "not_found_count = 0  # Initialize counter for not found links\n",
    "\n",
    "# Loop through each unique journal and get the journal link\n",
    "for index, journal in enumerate(unique_journals):\n",
    "    link = get_journal_link(journal)\n",
    "    journal_links.append(link)\n",
    "    \n",
    "    if link is None:\n",
    "        not_found_count += 1  # Increment the count if the link is not found\n",
    "\n",
    "    # Notify after processing every 100 journals\n",
    "    if (index + 1) % 100 == 0 or (index + 1) == total_count:\n",
    "        print(f\"Processed {index + 1}/{total_count} journals.\")\n",
    "\n",
    "# Create a DataFrame for the unique journals and their links\n",
    "links_df = pd.DataFrame({\n",
    "    'Journal': unique_journals,\n",
    "    'Journal Link': journal_links\n",
    "})\n",
    "\n",
    "# Merge the links back into the original DataFrame\n",
    "df_subset = df.merge(links_df, on='Journal', how='left')\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_subset.to_csv('Journal_Link.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print the count of journals with no links\n",
    "print(f\"Number of journals with no links: {not_found_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/235 journals.\n",
      "Processed 200/235 journals.\n",
      "Processed 235/235 journals.\n",
      "Number of journals with no links: 0\n"
     ]
    }
   ],
   "source": [
    "# 다중 검색되는 저널 -> 첫 번째로 링크 수집\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Filter journals where 'Journal_Link' is not NaN and 'Scope_All' is NaN\n",
    "filtered_df = df[(~df['Journal_Link'].isna()) & (df['Scope_All'].isna())]\n",
    "\n",
    "# Get unique journal names from the filtered dataset\n",
    "unique_journals = filtered_df['Journal'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def get_first_journal_link(journal_name):\n",
    "    # Prepare the search URL\n",
    "    search_url = f\"https://www.scimagojr.com/journalsearch.php?q={journal_name.replace(' ', '+')}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # Fetch the search results page\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the first journal link (first search result using XPath-like method)\n",
    "    first_journal = soup.select_one('div.search_results a')\n",
    "    \n",
    "    # Check if the link was found\n",
    "    if first_journal:\n",
    "        # Construct the full URL for the journal\n",
    "        journal_url = f\"https://www.scimagojr.com/{first_journal['href']}\"\n",
    "        return journal_url\n",
    "    else:\n",
    "        return None  # Return None if no link was found\n",
    "\n",
    "# Create a DataFrame to store the journal links for all unique journals\n",
    "journal_links = []\n",
    "total_count = len(unique_journals)\n",
    "not_found_count = 0  # Initialize counter for not found links\n",
    "\n",
    "# Loop through each unique journal and get the journal link\n",
    "for index, journal in enumerate(unique_journals):\n",
    "    link = get_first_journal_link(journal)\n",
    "    journal_links.append(link)\n",
    "    \n",
    "    if link is None:\n",
    "        not_found_count += 1  # Increment the count if the link is not found\n",
    "\n",
    "    # Notify after processing every 100 journals\n",
    "    if (index + 1) % 100 == 0 or (index + 1) == total_count:\n",
    "        print(f\"Processed {index + 1}/{total_count} journals.\")\n",
    "\n",
    "# Create a DataFrame for the unique journals and their links\n",
    "links_df = pd.DataFrame({\n",
    "    'Journal': unique_journals,\n",
    "    'Journal Link': journal_links\n",
    "})\n",
    "\n",
    "# Merge the links back into the filtered DataFrame\n",
    "filtered_df = filtered_df.merge(links_df, on='Journal', how='left')\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "filtered_df.to_csv('Journal_Link_Updated.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print the count of journals with no links\n",
    "print(f\"Number of journals with no links: {not_found_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset updated and saved to 'Paper_Dataset_Updated.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original dataset and the updated journal links dataset\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df_paper = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "file_path_updated = 'Journal_Link_Updated.csv'\n",
    "df_updated_links = pd.read_csv(file_path_updated, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Drop duplicates in the updated links DataFrame based on 'Journal'\n",
    "df_updated_links_unique = df_updated_links.drop_duplicates(subset='Journal')\n",
    "\n",
    "# Filter rows in Paper_Dataset.csv where 'Scope_All' is empty and 'Journal_Link' is not NaN\n",
    "mask = (df_paper['Scope_All'].isna()) & (~df_paper['Journal_Link'].isna())\n",
    "\n",
    "# Merge the datasets based on 'Journal' to update 'Journal_Link' where 'Scope_All' is empty\n",
    "df_paper.loc[mask, 'Journal_Link'] = df_paper.loc[mask, 'Journal'].map(df_updated_links_unique.set_index('Journal')['Journal Link'])\n",
    "\n",
    "# Save the modified dataset to a new CSV file\n",
    "df_paper.to_csv('Paper_Dataset_Updated.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Dataset updated and saved to 'Paper_Dataset_Updated.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 (get All / Tree category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 10개에 30초 걸림\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_journal_scope_all(journal_url):\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "#     }\n",
    "    \n",
    "#     response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "        \n",
    "#         if subject_area_header:\n",
    "#             top_level_subjects = []\n",
    "#             for ul in subject_area_header.find_all_next('ul'):\n",
    "#                 for li in ul.find_all('li', recursive=False):\n",
    "#                     a_tag = li.find('a')\n",
    "#                     if a_tag:\n",
    "#                         top_level_subjects.append(a_tag.text.strip())\n",
    "#             return \"; \".join(top_level_subjects)\n",
    "        \n",
    "#         return None\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# def get_journal_scope_tree(journal_url):\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "#     }\n",
    "    \n",
    "#     response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "        \n",
    "#         if subject_area_header:\n",
    "#             top_level_subjects = []\n",
    "#             for sibling in subject_area_header.find_all_next():\n",
    "#                 if sibling.name == 'h2':\n",
    "#                     break\n",
    "#                 if sibling.name == 'ul':\n",
    "#                     for li in sibling.find_all('li', recursive=False):\n",
    "#                         a_tag = li.find('a')\n",
    "#                         if a_tag and not li.find('ul', class_='treecategory'):\n",
    "#                             top_level_subjects.append(a_tag.text.strip())\n",
    "#             return \"; \".join(top_level_subjects)\n",
    "        \n",
    "#         return None\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # Load the dataset\n",
    "# file_path = 'Paper_Dataset.csv'\n",
    "# df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# # Trim whitespace from column names\n",
    "# df.columns = df.columns.str.strip()\n",
    "\n",
    "# # Get unique journal links from the dataset\n",
    "# unique_journals = df['Journal_Link'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# # Create a DataFrame to store the results\n",
    "# results = pd.DataFrame(unique_journals, columns=['Journal_Link'])\n",
    "\n",
    "# # Initialize lists to store the scopes\n",
    "# scope_all_list = []\n",
    "# scope_tree_list = []\n",
    "\n",
    "# # Iterate over each unique journal link\n",
    "# for journal_link in results['Journal_Link']:\n",
    "#     # Check if journal_link is valid\n",
    "#     if pd.isna(journal_link):\n",
    "#         scope_all_list.append(None)\n",
    "#         scope_tree_list.append(None)\n",
    "#         continue  # Skip to the next link\n",
    "    \n",
    "#     scope_all = get_journal_scope_all(journal_link)\n",
    "#     scope_tree = get_journal_scope_tree(journal_link)\n",
    "    \n",
    "#     scope_all_list.append(scope_all)\n",
    "#     scope_tree_list.append(scope_tree)\n",
    "\n",
    "#     # Notify after processing every 100 journals\n",
    "#     if (index + 1) % 100 == 0:\n",
    "#         print(f\"Processed {index + 1}/{len(results)} journals.\")\n",
    "\n",
    "\n",
    "# # Add the scopes to the results DataFrame\n",
    "# results['Scope_All'] = scope_all_list\n",
    "# results['Scope_Tree'] = scope_tree_list\n",
    "\n",
    "# # Merge results back into the original DataFrame based on the journal links\n",
    "# df = df.merge(results, on='Journal_Link', how='left')\n",
    "\n",
    "# # Save the updated DataFrame to a new CSV file\n",
    "# df.to_csv('Updated_Paper_Dataset.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# print(\"Scope_All and Scope_Tree have been added to the dataset, and the updated dataset has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/2893 journals.\n",
      "Processed 200/2893 journals.\n",
      "Processed 300/2893 journals.\n",
      "Processed 400/2893 journals.\n",
      "Processed 500/2893 journals.\n",
      "Processed 600/2893 journals.\n",
      "Processed 700/2893 journals.\n",
      "Processed 800/2893 journals.\n",
      "Processed 900/2893 journals.\n",
      "Processed 1000/2893 journals.\n",
      "Processed 1100/2893 journals.\n",
      "Processed 1200/2893 journals.\n",
      "Processed 1300/2893 journals.\n",
      "Processed 1400/2893 journals.\n",
      "Processed 1500/2893 journals.\n",
      "Processed 1600/2893 journals.\n",
      "Processed 1700/2893 journals.\n",
      "Processed 1800/2893 journals.\n",
      "Processed 1900/2893 journals.\n",
      "Processed 2000/2893 journals.\n",
      "Processed 2100/2893 journals.\n",
      "Processed 2200/2893 journals.\n",
      "Processed 2300/2893 journals.\n",
      "Processed 2400/2893 journals.\n",
      "Processed 2500/2893 journals.\n",
      "Processed 2600/2893 journals.\n",
      "Processed 2700/2893 journals.\n",
      "Processed 2800/2893 journals.\n",
      "Scope_All and Scope_Tree have been added to the dataset, and the updated dataset has been saved.\n"
     ]
    }
   ],
   "source": [
    "# 10개에 18초 걸림\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_journal_scopes(journal_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "        \n",
    "        if subject_area_header:\n",
    "            top_level_subjects = []\n",
    "            scope_tree_subjects = []\n",
    "            \n",
    "            # Traverse through the elements after the subject area header\n",
    "            for sibling in subject_area_header.find_all_next():\n",
    "                if sibling.name == 'h2':\n",
    "                    break  # Stop if we encounter another header\n",
    "                if sibling.name == 'ul':\n",
    "                    for li in sibling.find_all('li', recursive=False):\n",
    "                        a_tag = li.find('a')\n",
    "                        if a_tag:\n",
    "                            # Add to Scope_All\n",
    "                            top_level_subjects.append(a_tag.text.strip())\n",
    "                            # Only add to Scope_Tree if not a treecategory\n",
    "                            if not li.find('ul', class_='treecategory'):\n",
    "                                scope_tree_subjects.append(a_tag.text.strip())\n",
    "            \n",
    "            # Join the subject areas into a single string\n",
    "            scope_all = \"; \".join(top_level_subjects)\n",
    "            scope_tree = \"; \".join(scope_tree_subjects)\n",
    "            return scope_all, scope_tree\n",
    "        \n",
    "        return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Trim whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Get unique journal links from the dataset\n",
    "unique_journals = df['Journal_Link'].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame(unique_journals, columns=['Journal_Link'])\n",
    "\n",
    "# Initialize lists to store the scopes\n",
    "scope_all_list = []\n",
    "scope_tree_list = []\n",
    "\n",
    "# Iterate over each unique journal link\n",
    "for index, journal_link in enumerate(results['Journal_Link']):\n",
    "    # Check if journal_link is valid\n",
    "    if pd.isna(journal_link):\n",
    "        scope_all_list.append(None)\n",
    "        scope_tree_list.append(None)\n",
    "        continue  # Skip to the next link\n",
    "    \n",
    "    scope_all, scope_tree = get_journal_scopes(journal_link)\n",
    "    \n",
    "    scope_all_list.append(scope_all)\n",
    "    scope_tree_list.append(scope_tree)\n",
    "    \n",
    "    # Notify after processing every 100 journals\n",
    "    if (index + 1) % 100 == 0:\n",
    "        print(f\"Processed {index + 1}/{len(results)} journals.\")\n",
    "\n",
    "# Add the scopes to the results DataFrame\n",
    "results['Scope_All'] = scope_all_list\n",
    "results['Scope_Tree'] = scope_tree_list\n",
    "\n",
    "# Merge results back into the original DataFrame based on the journal links\n",
    "df = df.merge(results, on='Journal_Link', how='left')\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('Paper_Scope.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Scope_All and Scope_Tree have been added to the dataset, and the updated dataset has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/109 missing scope journals.\n",
      "Missing Scope_All and Scope_Tree have been collected and added to the dataset, and the updated dataset has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_journal_scopes(journal_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "        \n",
    "        if subject_area_header:\n",
    "            top_level_subjects = []\n",
    "            scope_tree_subjects = []\n",
    "            \n",
    "            # Traverse through the elements after the subject area header\n",
    "            for sibling in subject_area_header.find_all_next():\n",
    "                if sibling.name == 'h2':\n",
    "                    break  # Stop if we encounter another header\n",
    "                if sibling.name == 'ul':\n",
    "                    for li in sibling.find_all('li', recursive=False):\n",
    "                        a_tag = li.find('a')\n",
    "                        if a_tag:\n",
    "                            # Add to Scope_All\n",
    "                            top_level_subjects.append(a_tag.text.strip())\n",
    "                            # Only add to Scope_Tree if not a treecategory\n",
    "                            if not li.find('ul', class_='treecategory'):\n",
    "                                scope_tree_subjects.append(a_tag.text.strip())\n",
    "            \n",
    "            # Join the subject areas into a single string\n",
    "            scope_all = \"; \".join(top_level_subjects)\n",
    "            scope_tree = \"; \".join(scope_tree_subjects)\n",
    "            return scope_all, scope_tree\n",
    "        \n",
    "        return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Trim whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Get the rows where 'Scope_All' is missing (NaN)\n",
    "missing_scope_df = df[df['Scope_All'].isna() & df['Journal_Link'].notna()]\n",
    "\n",
    "# Create a DataFrame to store the results for missing data\n",
    "results = pd.DataFrame(missing_scope_df['Journal_Link'].drop_duplicates().reset_index(drop=True), columns=['Journal_Link'])\n",
    "\n",
    "# Initialize lists to store the scopes\n",
    "scope_all_list = []\n",
    "scope_tree_list = []\n",
    "\n",
    "# Iterate over each unique journal link for missing scope data\n",
    "for index, journal_link in enumerate(results['Journal_Link']):\n",
    "    # Collect the scope data for missing Scope_All\n",
    "    scope_all, scope_tree = get_journal_scopes(journal_link)\n",
    "    \n",
    "    scope_all_list.append(scope_all)\n",
    "    scope_tree_list.append(scope_tree)\n",
    "    \n",
    "    # Notify after processing every 100 journals\n",
    "    if (index + 1) % 100 == 0:\n",
    "        print(f\"Processed {index + 1}/{len(results)} missing scope journals.\")\n",
    "\n",
    "# Add the scopes to the results DataFrame\n",
    "results['Scope_All'] = scope_all_list\n",
    "results['Scope_Tree'] = scope_tree_list\n",
    "\n",
    "# Merge only the updated scopes back into the original DataFrame\n",
    "df_updated = df.merge(results, on='Journal_Link', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Update only rows where Scope_All was missing\n",
    "df_updated['Scope_All'] = df_updated['Scope_All'].combine_first(df_updated['Scope_All_new'])\n",
    "df_updated['Scope_Tree'] = df_updated['Scope_Tree'].combine_first(df_updated['Scope_Tree_new'])\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_updated.drop(columns=['Scope_All_new', 'Scope_Tree_new'], inplace=True)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df_updated.to_csv('Paper_Scope_Updated.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Missing Scope_All and Scope_Tree have been collected and added to the dataset, and the updated dataset has been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 All - Tree = Top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset that already has Scope_All and Scope_Tree\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Function to calculate Scope_Top\n",
    "def calculate_scope_top(scope_all, scope_tree):\n",
    "    if pd.isna(scope_all) or pd.isna(scope_tree):\n",
    "        return None\n",
    "    \n",
    "    # Split the subjects and remove any leading/trailing spaces\n",
    "    list1 = [subject.strip() for subject in scope_all.split(';')]\n",
    "    list2 = [subject.strip() for subject in scope_tree.split(';')]\n",
    "    \n",
    "    # Convert lists to sets for easy comparison\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Find subjects that are in scope_all but not in scope_tree\n",
    "    unique_subjects = set1 - set2\n",
    "    \n",
    "    # Join the unique subjects back into a single string\n",
    "    return \"; \".join(unique_subjects)\n",
    "\n",
    "# Apply the function to calculate Scope_Top for each row\n",
    "df['Scope_Top'] = df.apply(lambda row: calculate_scope_top(row['Scope_All'], row['Scope_Tree']), axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('Paper_Scope_Top.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Scope: \"Environmental Science; Environmental Chemistry; Health, Toxicology and Mutagenesis; Pollution; Waste Management and Disposal\"\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_journal_subject_areas(journal_url):\n",
    "#     # Step 2: Set headers to mimic a web browser\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "#     }\n",
    "    \n",
    "#     # Send a GET request to the journal URL with headers\n",
    "#     response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         # Parse the HTML content\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#         # Find the Subject Area and Category section\n",
    "#         subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "#         if subject_area_header:\n",
    "#             # Find the <ul> that follows the header\n",
    "#             subject_area_list = subject_area_header.find_next('ul')\n",
    "#             if subject_area_list:\n",
    "#                 # Extract all the <a> tags in the <ul>\n",
    "#                 subject_areas = [a.text for a in subject_area_list.find_all('a')]\n",
    "#                 # Join the subject areas into a single string\n",
    "#                 search_scope = \"; \".join(subject_areas)\n",
    "#                 return f'Search Scope: \"{search_scope}\"'\n",
    "#         return \"Subject area not found.\"\n",
    "#     else:\n",
    "#         return f\"Failed to retrieve journal detail page: {response.status_code}\"\n",
    "\n",
    "# # Example usage\n",
    "# journal_url = \"https://www.scimagojr.com/journalsearch.php?q=23388&tip=sid&clean=0\"\n",
    "# subject_areas = get_journal_subject_areas(journal_url)\n",
    "# print(subject_areas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Scope: \"Environmental Science\"\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_journal_subject_areas(journal_url):\n",
    "#     # Step 2: Set headers to mimic a web browser\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "#     }\n",
    "    \n",
    "#     # Send a GET request to the journal URL with headers\n",
    "#     response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         # Parse the HTML content\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#         # Find the Subject Area and Category section\n",
    "#         subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "#         if subject_area_header:\n",
    "#             # Find the first <ul> that follows the header\n",
    "#             subject_area_list = subject_area_header.find_next('ul')\n",
    "#             if subject_area_list:\n",
    "#                 # Extract top-level <li> items that contain <a> tags\n",
    "#                 top_level_subjects = []\n",
    "#                 for li in subject_area_list.find_all('li', recursive=False):\n",
    "#                     # Check if it has an <a> tag and capture its text\n",
    "#                     a_tag = li.find('a')\n",
    "#                     if a_tag:\n",
    "#                         top_level_subjects.append(a_tag.text.strip())\n",
    "#                 # Join the subject areas into a single string\n",
    "#                 search_scope = \"; \".join(top_level_subjects)\n",
    "#                 return f'Search Scope: \"{search_scope}\"'\n",
    "#         return \"Subject area not found.\"\n",
    "#     else:\n",
    "#         return f\"Failed to retrieve journal detail page: {response.status_code}\"\n",
    "\n",
    "# # Example usage\n",
    "# journal_url = \"https://www.scimagojr.com/journalsearch.php?q=23388&tip=sid&clean=0\"\n",
    "# subject_areas = get_journal_subject_areas(journal_url)\n",
    "# print(subject_areas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL Category (Top + Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Scope: \"Health Professions; Radiological and Ultrasound Technology; Medicine; Medicine (miscellaneous); Public Health, Environmental and Occupational Health; Radiology, Nuclear Medicine and Imaging; Physics and Astronomy; Radiation\"\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_journal_subject_areas(journal_url):\n",
    "#     # Step 2: Set headers to mimic a web browser\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "#     }\n",
    "    \n",
    "#     # Send a GET request to the journal URL with headers\n",
    "#     response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         # Parse the HTML content\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#         # Find the Subject Area and Category section\n",
    "#         subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "#         if subject_area_header:\n",
    "#             # Initialize a list to collect subject areas\n",
    "#             top_level_subjects = []\n",
    "            \n",
    "#             # Find all <ul> elements that follow the header\n",
    "#             for ul in subject_area_header.find_all_next('ul'):\n",
    "#                 # Extract the top-level <li> items that contain <a> tags\n",
    "#                 for li in ul.find_all('li', recursive=False):\n",
    "#                     a_tag = li.find('a')\n",
    "#                     if a_tag:\n",
    "#                         top_level_subjects.append(a_tag.text.strip())\n",
    "            \n",
    "#             # Join the subject areas into a single string\n",
    "#             search_scope = \"; \".join(top_level_subjects)\n",
    "#             return f'Search Scope: \"{search_scope}\"'\n",
    "#         return \"Subject area not found.\"\n",
    "#     else:\n",
    "#         return f\"Failed to retrieve journal detail page: {response.status_code}\"\n",
    "\n",
    "# # Example usage\n",
    "# journal_url = \"https://www.scimagojr.com/journalsearch.php?q=29514&tip=sid&clean=0\"\n",
    "# subject_areas = get_journal_subject_areas(journal_url)\n",
    "# print(subject_areas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree category 만 (세부)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Scope: \"Radiological and Ultrasound Technology; Medicine (miscellaneous); Public Health, Environmental and Occupational Health; Radiology, Nuclear Medicine and Imaging; Radiation\"\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# def get_journal_subject_areas(journal_url):\n",
    "#     # Step 2: Set headers to mimic a web browser\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36\"\n",
    "#     }\n",
    "    \n",
    "#     # Send a GET request to the journal URL with headers\n",
    "#     response = requests.get(journal_url, headers=headers)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         # Parse the HTML content\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#         # Find the Subject Area and Category section\n",
    "#         subject_area_header = soup.find('h2', string=\"Subject Area and Category\")\n",
    "#         if subject_area_header:\n",
    "#             # Initialize a list to collect top-level subject areas\n",
    "#             top_level_subjects = []\n",
    "            \n",
    "#             # Find all <ul> elements that follow the header until the next header\n",
    "#             for sibling in subject_area_header.find_all_next():\n",
    "#                 # Stop if we encounter another header (h2)\n",
    "#                 if sibling.name == 'h2':\n",
    "#                     break\n",
    "#                 # Look for <ul> elements only and extract top-level <li> items\n",
    "#                 if sibling.name == 'ul':\n",
    "#                     for li in sibling.find_all('li', recursive=False):\n",
    "#                         a_tag = li.find('a')\n",
    "#                         if a_tag:\n",
    "#                             # Only add if not a treecategory\n",
    "#                             if not li.find('ul', class_='treecategory'):\n",
    "#                                 top_level_subjects.append(a_tag.text.strip())\n",
    "\n",
    "#             # Join the subject areas into a single string\n",
    "#             search_scope = \"; \".join(top_level_subjects)\n",
    "#             return f'Search Scope: \"{search_scope}\"'\n",
    "#         return \"Subject area not found.\"\n",
    "#     else:\n",
    "#         return f\"Failed to retrieve journal detail page: {response.status_code}\"\n",
    "\n",
    "# # Example usage\n",
    "# journal_url = \"https://www.scimagojr.com/journalsearch.php?q=29514&tip=sid&clean=0\"\n",
    "# subject_areas = get_journal_subject_areas(journal_url)\n",
    "# print(subject_areas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL - Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Scope: \"Medicine; Health Professions; Physics and Astronomy\"\n"
     ]
    }
   ],
   "source": [
    "def find_unique_subjects(scope1, scope2):\n",
    "    # Split the search scopes into lists and strip whitespace\n",
    "    list1 = [subject.strip() for subject in scope1.split(';')]\n",
    "    list2 = [subject.strip() for subject in scope2.split(';')]\n",
    "    \n",
    "    # Convert lists to sets for easy comparison\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Find subjects that are in scope1 but not in scope2\n",
    "    unique_subjects = set1 - set2\n",
    "    \n",
    "    # Join the unique subjects into a single string\n",
    "    return f'Search Scope: \"{\"; \".join(unique_subjects)}\"' if unique_subjects else 'No unique subjects found.'\n",
    "\n",
    "# Example inputs\n",
    "scope1 = \"Health Professions; Radiological and Ultrasound Technology; Medicine; Medicine (miscellaneous); Public Health, Environmental and Occupational Health; Radiology, Nuclear Medicine and Imaging; Physics and Astronomy; Radiation\"\n",
    "scope2 = \"Radiological and Ultrasound Technology; Medicine (miscellaneous); Public Health, Environmental and Occupational Health; Radiology, Nuclear Medicine and Imaging; Radiation\"\n",
    "\n",
    "# Get unique subjects\n",
    "output = find_unique_subjects(scope1, scope2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자잘구리 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specified fields have been filled in for the 'applied sciences-basel' journal, and the updated dataset has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 2: Trim whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Step 3: Define the journal criteria and new values\n",
    "journal_name = \"applied sciences-basel\"\n",
    "journal_link = \"https://www.scimagojr.com/journalsearch.php?q=21100829268&tip=sid&clean=0\"\n",
    "scope_all = \"Chemical Engineering; Fluid Flow and Transfer Processes; Process Chemistry and Technology; Computer Science; Computer Science Applications; Engineering; Engineering (miscellaneous); Materials Science; Materials Science (miscellaneous); Physics and Astronomy; Instrumentation\"\n",
    "scope_top = \"Chemical Engineering; Computer Science; Engineering; Materials Science; Physics and Astronomy\"\n",
    "scope_tree = \"Fluid Flow and Transfer Processes; Process Chemistry and Technology; Computer Science Applications; Engineering (miscellaneous); Materials Science (miscellaneous); Instrumentation\"\n",
    "\n",
    "# Step 4: Update the DataFrame\n",
    "mask = df['Journal'].str.lower() == journal_name.lower()  # Case insensitive comparison\n",
    "df.loc[mask, 'Journal_Link'] = journal_link\n",
    "df.loc[mask, 'Scope_All'] = scope_all\n",
    "df.loc[mask, 'Scope_Top'] = scope_top\n",
    "df.loc[mask, 'Scope_Tree'] = scope_tree\n",
    "\n",
    "# Step 5: Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('Updated_Paper_Dataset.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"The specified fields have been filled in for the 'applied sciences-basel' journal, and the updated dataset has been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journal -> Journal_Link 수동 입력\n",
    "# Input data as a multiline string\n",
    "input_data = \"\"\"\n",
    "Journal: Space Weather an AGU journal\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=4000151603&tip=sid&clean=0\n",
    "\n",
    "Journal: SPACE WEATHER-THE INTERNATIONAL JOURNAL OF RESEARCH AND APPLICATIONS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=4000151603&tip=sid&clean=0\n",
    "\n",
    "Journal: ACTA CRYSTALLOGRAPHICA SECTION F-STRUCTURAL BIOLOGY AND CRYSTALLIZATION COMMUNICATIONS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100326880&tip=sid&clean=0\n",
    "\n",
    "Journal: ACTA CRYSTALLOGRAPHICA SECTION D-BIOLOGICAL CRYSTALLOGRAPHY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100778657&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF SURFACE INVESTIGATION-X-RAY SYNCHROTRON AND NEUTRON TECHNIQUES\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=63269&tip=sid&clean=0\n",
    "\n",
    "Journal: MATERIALS SCIENCE AND ENGINEERING A-STRUCTURAL MATERIALS PROPERTIES MICROSTRUCTURE AND PROCESSING\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=17811&tip=sid&clean=0\n",
    "\n",
    "Journal: VIRUSES-BASEL\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700188364&tip=sid&clean=0\n",
    "\n",
    "Journal: ANNUAL REVIEW OF CHEMICAL AND BIOMOLECULAR ENGINEERING, VOL 3\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700188418&tip=sid&clean=0\n",
    "\n",
    "Journal: CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND APPLICATIONS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=24596&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF AGRICULTURAL METEOROLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=30706&tip=sid&clean=0\n",
    "\n",
    "Journal: SYMMETRY-BASEL\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100201542&tip=sid&clean=0 \n",
    "\n",
    "Journal: KOREAN JOURNAL OF HORTICULTURAL SCIENCE & TECHNOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19900192027&tip=sid&clean=0\n",
    "\n",
    "Journal: Agronomy-Basel\n",
    "Journal_Link: http://scimagojr.com/journalsearch.php?q=15639&tip=sid&clean=0 \n",
    "\n",
    "Journal: Plants-Basel\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100788294&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO TECHNOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=11400153333&tip=sid&clean=0\n",
    "\n",
    "Journal: FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF GRID COMPUTING AND ESCIENCE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=12264&tip=sid&clean=0\n",
    "\n",
    "Journal: INTERNATIONAL JOURNAL OF REFRIGERATION-REVUE INTERNATIONALE DU FROID\n",
    "Journal_Link: http://scimagojr.com/journalsearch.php?q=16113&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF THE KOREAN SURGICAL SOCIETY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100326082&tip=sid&clean=0 \n",
    "\n",
    "Journal: JOURNAL OF ENGINEERING FOR GAS TURBINES AND POWER-TRANSACTIONS OF THE ASME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=20962&tip=sid&clean=0\n",
    "\n",
    "Journal: MICROSYSTEM TECHNOLOGIES-MICRO-AND NANOSYSTEMS-INFORMATION STORAGE AND PROCESSING SYSTEMS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=26738&tip=sid&clean=0 \n",
    "\n",
    "Journal: ARCHIVES OF OTOLARYNGOLOGY-HEAD & NECK SURGERY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100200823&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF CHEMICAL AND ENGINEERING DATA\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=24158&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF ENGINEERING MATERIALS AND TECHNOLOGY-TRANSACTIONS OF THE ASME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21155&tip=sid&clean=0\n",
    "\n",
    "Journal: HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=29470&tip=sid&clean=0\n",
    "\n",
    "Journal: CANADIAN JOURNAL OF OPHTHALMOLOGY-JOURNAL CANADIEN D OPHTALMOLOGIE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=13766&tip=sid&clean=0\n",
    "\n",
    "Journal: MATERIALS SCIENCE AND ENGINEERING B-ADVANCED FUNCTIONAL SOLID-STATE MATERIALS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=17812&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF MANUFACTURING SCIENCE AND ENGINEERING-TRANSACTIONS OF THE ASME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=20966&tip=sid&clean=0 \n",
    "\n",
    "Journal: ANNALS OF OCCUPATIONAL HYGIENE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100821119&tip=sid&clean=0\n",
    "\n",
    "Journal: The Korean Journal of Physiology & Pharmacology\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=23173&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF MOLECULAR CATALYSIS A-CHEMICAL\n",
    "Journal_Link: http://scimagojr.com/journalsearch.php?q=17619&tip=sid&clean=0\n",
    "\n",
    "Journal: EARTHQUAKES AND STRUCTURES\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700188258&tip=sid\n",
    "\n",
    "Journal: TEHNICKI VJESNIK-TECHNICAL GAZETTE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=14569&tip=sid&clean=0\n",
    "\n",
    "Journal: ADSORPTION-JOURNAL OF THE INTERNATIONAL ADSORPTION SOCIETY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=16302&tip=sid&clean=0 \n",
    "\n",
    "Journal: JOURNAL OF INFLAMMATION-LONDON\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=40695&tip=sid&clean=0\n",
    "\n",
    "Journal: Transactions of Nonferrous Metals Society of China (English Edition)\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=27854&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF COATINGS TECHNOLOGY AND RESEARCH\n",
    "Journal_Link: http://scimagojr.com/journalsearch.php?q=12725&tip=sid&clean=0\n",
    "\n",
    "Journal: Nanoscale reseach Letters\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21101151625&tip=sid&clean=0\n",
    "\n",
    "Journal: ACSPhotonics\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100368207&tip=sid&clean=0\n",
    "\n",
    "Journal: ABSTRACTS OF PAPERS OF THE AMERICAN CHEMICAL SOCIETY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=22680&tip=sid&clean=0\n",
    "\n",
    "Journal: ACM SIGCOMM COMPUTER COMMUNICATION REVIEW\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=13683&tip=sid&clean=0\n",
    "\n",
    "Journal: ACS Applied Materials and Interfaces\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700171101&tip=sid&clean=0\n",
    "\n",
    "Journal: ACTA VETERINARIA-BEOGRAD\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=17060&tip=sid&clean=0\n",
    "\n",
    "Journal: Agriculture-Basel\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100781511&tip=sid&clean=0\n",
    "\n",
    "Journal: AGRONOMY-BASEL\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100447811&tip=sid&clean=0\n",
    "\n",
    "Journal: ALGAL RESEARCH-BIOMASS BIOFUELS AND BIOPRODUCTS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100201089&tip=sid&clean=0\n",
    "\n",
    "Journal: ALTEX-ALTERNATIVES TO ANIMAL EXPERIMENTATION\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=26767&tip=sid&clean=0\n",
    "\n",
    "Journal: Alzheimers & Dementia\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=3600148102&tip=sid&clean=0\n",
    "\n",
    "Journal: Alzheimers Research & Therapy\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700174935&tip=sid&clean=0\n",
    "\n",
    "Journal: ANNALS OF ANATOMY-ANATOMISCHER ANZEIGER\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=27472&tip=sid&clean=0\n",
    "\n",
    "Journal: Antibiotics-Basel\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100469670&tip=sid&clean=0\n",
    "\n",
    "Journal: ANTONIE VAN LEEUWENHOEK INTERNATIONAL JOURNAL OF GENERAL AND MOLECULAR MICROBIOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=14944&tip=sid&clean=0\n",
    "\n",
    "Journal: APPLIED SCIENCES BASEL\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100829268&tip=sid&clean=0\n",
    "\n",
    "Journal: ARCHIVES OF OTOLARYNGOLOGY-HEAD & NECK SURGERY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100200823&tip=sid&clean=0\n",
    "\n",
    "Journal: ATW-INTERNATIONAL JOURNAL FOR NUCLEAR POWER\n",
    "Journal_Link: http://scimagojr.com/journalsearch.php?q=29351&tip=sid&clean=0\n",
    "\n",
    "Journal: Biosensors-Basel\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100293900&tip=sid&clean=0\n",
    "\n",
    "Journal: CENTRAL EUROPEAN JOURNAL OF GEOSCIENCES\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=4400151402&tip=sid&clean=0\n",
    "\n",
    "Journal: CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND APPLICATIONS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=24596&tip=sid&clean=0\n",
    "\n",
    "Journal: CMC-Computers Materials & Continua\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=24364&tip=sid&clean=0\n",
    "\n",
    "Journal: CURRENT MEDICAL IMAGING REVIEWS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=4700152432&tip=sid&clean=0\n",
    "\n",
    "Journal: Diabetes Metabolic Syndrome and Obesity-Targets and Therapy\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700174905&tip=sid&clean=0\n",
    "\n",
    "Journal: Diversity-Basel\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=6000187990&tip=sid&clean=0\n",
    "\n",
    "Journal: EKSPLOATACJA I NIEZAWODNOSC-MAINTENANCE AND RELIABILITY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700182638&tip=sid&clean=0\n",
    "\n",
    "Journal: ENGINEERING SCIENCE AND TECHNOLOGY-AN INTERNATIONAL JOURNAL-JESTECH\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100806003&tip=sid&clean=0\n",
    "\n",
    "Journal: FERMENTATION-BASEL\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100900055&tip=sid&clean=0\n",
    "\n",
    "Journal: FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=12264&tip=sid&clean=0\n",
    "\n",
    "Journal: GRAEFES ARCHIVE FOR CLINICAL AND EXPERIMENTAL OPHTHALMOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=13904&tip=sid&clean=0\n",
    "\n",
    "Journal: HEPATOBILIARY SURGERY AND NUTRITION\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21101019769&tip=sid&clean=0\n",
    "\n",
    "Journal: HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=29470&tip=sid&clean=0\n",
    "\n",
    "Journal: IEEE transactions on applied superconductivity : a publication of the IEEE Superconductivity Committee\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=26026&tip=sid&clean=0\n",
    "\n",
    "Journal: IEEE TRANSACTIONS ON INFORMATION TECHNOLOGY IN BIOMEDICINE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100256982&tip=sid&clean=0\n",
    "\n",
    "Journal: IIE TRANSACTIONS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100794597&tip=sid&clean=0\n",
    "\n",
    "Journal: INJURY-INTERNATIONAL JOURNAL OF THE CARE OF THE INJURED\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=29801&tip=sid&clean=0\n",
    "\n",
    "Journal: INORGANIC CHEMISTRY COMMUNICATIONS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=25267&tip=sid&clean=0\n",
    "\n",
    "Journal: J. Alloys Compd.\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=12325&tip=sid&clean=0\n",
    "\n",
    "Journal: J. of Ceramic Precessing Research\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=22026&tip=sid&clean=0\n",
    "\n",
    "Journal: JNCI-JOURNAL OF THE NATIONAL CANCER INSTITUTE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=12459&tip=sid&clean=0\n",
    "\n",
    "Journal: Jouranl of Ceramic Processing Research\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=22026&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF AGRICULTURAL METEOROLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=30706&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF ALZHEIMERS DISEASE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=16246&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF BONE AND JOINT SURGERY-AMERICAN VOLUME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=12198&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF CLOUD COMPUTING-ADVANCES SYSTEMS AND APPLICATIONS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100383744&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF CRANIO-MAXILLOFACIAL SURGERY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21658&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF ENVIRONMENTAL SCIENCES-CHINA\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=23393&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF FOOD SCIENCE AND TECHNOLOGY-MYSORE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=20617&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF MACROMOLECULAR SCIENCE PART A-PURE AND APPLIED CHEMISTRY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=25891&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF MACROMOLECULAR SCIENCE PART B-PHYSICS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=28522&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF MANUFACTURING SCIENCE AND ENGINEERING-TRANSACTIONS OF THE ASME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=20966&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF MATERIALS RESEARCH AND TECHNOLOGY-JMR&T\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100383742&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF MECHANISMS AND ROBOTICS-TRANSACTIONS OF THE ASME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19700186816&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF OFFSHORE MECHANICS AND ARCTIC ENGINEERING-TRANSACTIONS OF THE ASME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=20985&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF PHYSIOLOGY-LONDON\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=23478&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF PROSTHODONTICS-IMPLANT ESTHETIC AND RECONSTRUCTIVE DENTISTRY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=26177&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO TECHNOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=11400153333&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF SURFACE INVESTIGATION-X-RAY SYNCHROTRON AND NEUTRON TECHNIQUES\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=63269&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF THE FRANKLIN INSTITUTE-ENGINEERING AND APPLIED MATHEMATICS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=27959&tip=sid&clean=0\n",
    "\n",
    "Journal: JOURNAL OF TRIBOLOGY-TRANSACTIONS OF THE ASME\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=17014&tip=sid&clean=0\n",
    "\n",
    "Journal: Jove-Journal of Visualized Experiments\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19900191993&tip=sid&clean=0\n",
    "\n",
    "Journal: KOREAN JOURNAL OF HORTICULTURAL SCIENCE & TECHNOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19900192027&tip=sid&clean=0\n",
    "\n",
    "Journal: LWT-FOOD SCIENCE AND TECHNOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=22475&tip=sid&clean=0\n",
    "\n",
    "Journal: MARINE ECOLOGY-AN EVOLUTIONARY PERSPECTIVE\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=12169&tip=sid&clean=0\n",
    "\n",
    "Journal: MATERIALS SCIENCE-MEDZIAGOTYRA\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=19300157032&tip=sid\n",
    "\n",
    "Journal: MEMBRANE AND WATER TREATMENT\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100265344&tip=sid&clean=0\n",
    "\n",
    "Journal: MICROCHIMICA ACTA\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=24072&tip=sid\n",
    "\n",
    "Journal: Natural Hazards and Earth System Sciences Discussions\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=51166&tip=sid&clean=0\n",
    "\n",
    "Journal: PHYSICA STATUS SOLIDI B-BASIC SOLID STATE PHYSICS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=29140&tip=sid&clean=0\n",
    "\n",
    "Journal: PLANT GENETIC RESOURCES-CHARACTERIZATION AND UTILIZATION\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=98120&tip=sid&clean=0\n",
    "\n",
    "Journal: POLIMEROS-CIENCIA E TECNOLOGIA\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=5000156907&tip=sid&clean=0\n",
    "\n",
    "Journal: PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR PRECISION ENGINEERING AND NANOTECHNOLOGY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=13790&tip=sid&clean=0\n",
    "\n",
    "Journal: PUBLICATIONS OF THE ASTRONOMICAL SOCIETY OF JAPAN\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=27778&tip=sid&clean=0\n",
    "\n",
    "Journal: RETINA-THE JOURNAL OF RETINAL AND VITREOUS DISEASES\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=15094&tip=sid\n",
    "\n",
    "Journal: REVISTA BRASILEIRA DE FARMACOGNOSIA-BRAZILIAN JOURNAL OF PHARMACOGNOSY\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=5400152628&tip=sid&exact=no\n",
    "\n",
    "Journal: Sensors (Switzerland)\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=130124&tip=sid\n",
    "\n",
    "Journal: SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION INTERNATIONAL\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=14452&tip=sid&clean=0\n",
    "\n",
    "Journal: Spanish journal of agricultural research = Revista de investigaci oacute;n agraria : SJAR\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=5800179591&tip=sid\n",
    "\n",
    "Journal: STRUCTURAL HEALTH MONITORING-AN INTERNATIONAL JOURNAL\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=5800179591&tip=sid\n",
    "\n",
    "Journal: Sustainability (Switzerland)\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100240100&tip=sid\n",
    "\n",
    "Journal: SYNTHESIS-STUTTGART\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=26508&tip=sid&clean=0\n",
    "\n",
    "Journal: The American Journal of Chinese Medicine\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=18134&tip=sid&clean=0\n",
    "\n",
    "Journal: Turkish Journal of Biochemistry-Turk Biyokimya Dergisi\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=17600155132&tip=sid&clean=0\n",
    "\n",
    "Journal: WORLD JOURNAL OF MENS HEALTH\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=21100943924&tip=sid&clean=0\n",
    "\n",
    "Journal: WORLD WIDE WEB-INTERNET AND WEB INFORMATION SYSTEMS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=14965&tip=sid&clean=0\n",
    "\n",
    "Journal: ZEITSCHRIFT FUR PHYSIKALISCHE CHEMIE-INTERNATIONAL JOURNAL OF RESEARCH IN PHYSICAL CHEMISTRY & CHEMICAL PHYSICS\n",
    "Journal_Link: https://www.scimagojr.com/journalsearch.php?q=23775&tip=sid\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 줄 단위로 데이터를 나눠 리스트로 변환\n",
    "lines = input_data.strip().split(\"\\n\")\n",
    "\n",
    "# 딕셔너리로 변환\n",
    "journal_dict = {}\n",
    "for i in range(0, len(lines), 3):  # 3줄씩 처리\n",
    "    journal_name = lines[i].split(\"Journal: \")[1].strip()\n",
    "    journal_link = lines[i+1].split(\"Journal_Link: \")[1].strip()\n",
    "    journal_dict[journal_name] = journal_link\n",
    "\n",
    "# 원하는 형식으로 출력\n",
    "print(\"{\")\n",
    "for journal, link in journal_dict.items():\n",
    "    print(f\"    '{journal}': '{link}',\")\n",
    "print(\"}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'Journal_Link' fields have been filled based on your manual entries, and the updated dataset has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'Paper_Dataset.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 2: Define the dictionary of journal names and their corresponding links\n",
    "manual_journal_links = {\n",
    "    'Space Weather an AGU journal': 'https://www.scimagojr.com/journalsearch.php?q=4000151603&tip=sid&clean=0',\n",
    "    'SPACE WEATHER-THE INTERNATIONAL JOURNAL OF RESEARCH AND APPLICATIONS': 'https://www.scimagojr.com/journalsearch.php?q=4000151603&tip=sid&clean=0',\n",
    "    'ACTA CRYSTALLOGRAPHICA SECTION F-STRUCTURAL BIOLOGY AND CRYSTALLIZATION COMMUNICATIONS': 'https://www.scimagojr.com/journalsearch.php?q=21100326880&tip=sid&clean=0',\n",
    "    'ACTA CRYSTALLOGRAPHICA SECTION D-BIOLOGICAL CRYSTALLOGRAPHY': 'https://www.scimagojr.com/journalsearch.php?q=21100778657&tip=sid&clean=0',\n",
    "    'JOURNAL OF SURFACE INVESTIGATION-X-RAY SYNCHROTRON AND NEUTRON TECHNIQUES': 'https://www.scimagojr.com/journalsearch.php?q=63269&tip=sid&clean=0',\n",
    "    'MATERIALS SCIENCE AND ENGINEERING A-STRUCTURAL MATERIALS PROPERTIES MICROSTRUCTURE AND PROCESSING': 'https://www.scimagojr.com/journalsearch.php?q=17811&tip=sid&clean=0',\n",
    "    'VIRUSES-BASEL': 'https://www.scimagojr.com/journalsearch.php?q=19700188364&tip=sid&clean=0',\n",
    "    'ANNUAL REVIEW OF CHEMICAL AND BIOMOLECULAR ENGINEERING, VOL 3': 'https://www.scimagojr.com/journalsearch.php?q=19700188418&tip=sid&clean=0',\n",
    "    'CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND APPLICATIONS': 'https://www.scimagojr.com/journalsearch.php?q=24596&tip=sid&clean=0',\n",
    "    'JOURNAL OF AGRICULTURAL METEOROLOGY': 'https://www.scimagojr.com/journalsearch.php?q=30706&tip=sid&clean=0',\n",
    "    'SYMMETRY-BASEL': 'https://www.scimagojr.com/journalsearch.php?q=21100201542&tip=sid&clean=0',\n",
    "    'KOREAN JOURNAL OF HORTICULTURAL SCIENCE & TECHNOLOGY': 'https://www.scimagojr.com/journalsearch.php?q=19900192027&tip=sid&clean=0',\n",
    "    'Agronomy-Basel': 'http://scimagojr.com/journalsearch.php?q=15639&tip=sid&clean=0',\n",
    "    'Plants-Basel': 'https://www.scimagojr.com/journalsearch.php?q=21100788294&tip=sid&clean=0',\n",
    "    'JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO TECHNOLOGY': 'https://www.scimagojr.com/journalsearch.php?q=11400153333&tip=sid&clean=0',\n",
    "    'FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF GRID COMPUTING AND ESCIENCE': 'https://www.scimagojr.com/journalsearch.php?q=12264&tip=sid&clean=0',\n",
    "    'INTERNATIONAL JOURNAL OF REFRIGERATION-REVUE INTERNATIONALE DU FROID': 'http://scimagojr.com/journalsearch.php?q=16113&tip=sid&clean=0',\n",
    "    'JOURNAL OF THE KOREAN SURGICAL SOCIETY': 'https://www.scimagojr.com/journalsearch.php?q=21100326082&tip=sid&clean=0',\n",
    "    'JOURNAL OF ENGINEERING FOR GAS TURBINES AND POWER-TRANSACTIONS OF THE ASME': 'https://www.scimagojr.com/journalsearch.php?q=20962&tip=sid&clean=0',\n",
    "    'MICROSYSTEM TECHNOLOGIES-MICRO-AND NANOSYSTEMS-INFORMATION STORAGE AND PROCESSING SYSTEMS': 'https://www.scimagojr.com/journalsearch.php?q=26738&tip=sid&clean=0',\n",
    "    'ARCHIVES OF OTOLARYNGOLOGY-HEAD & NECK SURGERY': 'https://www.scimagojr.com/journalsearch.php?q=21100200823&tip=sid&clean=0',\n",
    "    'JOURNAL OF CHEMICAL AND ENGINEERING DATA': 'https://www.scimagojr.com/journalsearch.php?q=24158&tip=sid&clean=0',\n",
    "    'JOURNAL OF ENGINEERING MATERIALS AND TECHNOLOGY-TRANSACTIONS OF THE ASME': 'https://www.scimagojr.com/journalsearch.php?q=21155&tip=sid&clean=0',\n",
    "    'HYDROLOGICAL SCIENCES JOURNAL-JOURNAL DES SCIENCES HYDROLOGIQUES': 'https://www.scimagojr.com/journalsearch.php?q=29470&tip=sid&clean=0',\n",
    "    'CANADIAN JOURNAL OF OPHTHALMOLOGY-JOURNAL CANADIEN D OPHTALMOLOGIE': 'https://www.scimagojr.com/journalsearch.php?q=13766&tip=sid&clean=0',\n",
    "    'MATERIALS SCIENCE AND ENGINEERING B-ADVANCED FUNCTIONAL SOLID-STATE MATERIALS': 'https://www.scimagojr.com/journalsearch.php?q=17812&tip=sid&clean=0',\n",
    "    'JOURNAL OF MANUFACTURING SCIENCE AND ENGINEERING-TRANSACTIONS OF THE ASME': 'https://www.scimagojr.com/journalsearch.php?q=20966&tip=sid&clean=0',\n",
    "    'ANNALS OF OCCUPATIONAL HYGIENE': 'https://www.scimagojr.com/journalsearch.php?q=21100821119&tip=sid&clean=0',\n",
    "    'The Korean Journal of Physiology & Pharmacology': 'https://www.scimagojr.com/journalsearch.php?q=23173&tip=sid&clean=0',\n",
    "    'JOURNAL OF MOLECULAR CATALYSIS A-CHEMICAL': 'http://scimagojr.com/journalsearch.php?q=17619&tip=sid&clean=0',\n",
    "    'EARTHQUAKES AND STRUCTURES': 'https://www.scimagojr.com/journalsearch.php?q=19700188258&tip=sid',\n",
    "    'TEHNICKI VJESNIK-TECHNICAL GAZETTE': 'https://www.scimagojr.com/journalsearch.php?q=14569&tip=sid&clean=0',\n",
    "    'ADSORPTION-JOURNAL OF THE INTERNATIONAL ADSORPTION SOCIETY': 'https://www.scimagojr.com/journalsearch.php?q=16302&tip=sid&clean=0',\n",
    "    'JOURNAL OF INFLAMMATION-LONDON': 'https://www.scimagojr.com/journalsearch.php?q=40695&tip=sid&clean=0',\n",
    "    'Transactions of Nonferrous Metals Society of China (English Edition)': 'https://www.scimagojr.com/journalsearch.php?q=27854&tip=sid&clean=0',\n",
    "    'JOURNAL OF COATINGS TECHNOLOGY AND RESEARCH': 'http://scimagojr.com/journalsearch.php?q=12725&tip=sid&clean=0',\n",
    "    'Nanoscale reseach Letters': 'https://www.scimagojr.com/journalsearch.php?q=21101151625&tip=sid&clean=0',\n",
    "    'ACSPhotonics': 'https://www.scimagojr.com/journalsearch.php?q=21100368207&tip=sid&clean=0',\n",
    "    'ABSTRACTS OF PAPERS OF THE AMERICAN CHEMICAL SOCIETY': 'https://www.scimagojr.com/journalsearch.php?q=22680&tip=sid&clean=0',\n",
    "    'ACM SIGCOMM COMPUTER COMMUNICATION REVIEW': 'https://www.scimagojr.com/journalsearch.php?q=13683&tip=sid&clean=0',\n",
    "    'ACS Applied Materials and Interfaces': 'https://www.scimagojr.com/journalsearch.php?q=19700171101&tip=sid&clean=0',\n",
    "    'ACTA VETERINARIA-BEOGRAD': 'https://www.scimagojr.com/journalsearch.php?q=17060&tip=sid&clean=0',\n",
    "    'Agriculture-Basel': 'https://www.scimagojr.com/journalsearch.php?q=21100781511&tip=sid&clean=0',\n",
    "    'AGRONOMY-BASEL': 'https://www.scimagojr.com/journalsearch.php?q=21100447811&tip=sid&clean=0',\n",
    "    'ALGAL RESEARCH-BIOMASS BIOFUELS AND BIOPRODUCTS': 'https://www.scimagojr.com/journalsearch.php?q=21100201089&tip=sid&clean=0',\n",
    "    'ALTEX-ALTERNATIVES TO ANIMAL EXPERIMENTATION': 'https://www.scimagojr.com/journalsearch.php?q=26767&tip=sid&clean=0',\n",
    "    'Alzheimers & Dementia': 'https://www.scimagojr.com/journalsearch.php?q=3600148102&tip=sid&clean=0',\n",
    "    'Alzheimers Research & Therapy': 'https://www.scimagojr.com/journalsearch.php?q=19700174935&tip=sid&clean=0',\n",
    "    'ANNALS OF ANATOMY-ANATOMISCHER ANZEIGER': 'https://www.scimagojr.com/journalsearch.php?q=27472&tip=sid&clean=0',\n",
    "    'Antibiotics-Basel': 'https://www.scimagojr.com/journalsearch.php?q=21100469670&tip=sid&clean=0',\n",
    "    'ANTONIE VAN LEEUWENHOEK INTERNATIONAL JOURNAL OF GENERAL AND MOLECULAR MICROBIOLOGY': 'https://www.scimagojr.com/journalsearch.php?q=14944&tip=sid&clean=0',\n",
    "    'APPLIED SCIENCES BASEL': 'https://www.scimagojr.com/journalsearch.php?q=21100829268&tip=sid&clean=0',\n",
    "    'ATW-INTERNATIONAL JOURNAL FOR NUCLEAR POWER': 'http://scimagojr.com/journalsearch.php?q=29351&tip=sid&clean=0',\n",
    "    'Biosensors-Basel': 'https://www.scimagojr.com/journalsearch.php?q=21100293900&tip=sid&clean=0',\n",
    "    'CENTRAL EUROPEAN JOURNAL OF GEOSCIENCES': 'https://www.scimagojr.com/journalsearch.php?q=4400151402&tip=sid&clean=0',\n",
    "    'CMC-Computers Materials & Continua': 'https://www.scimagojr.com/journalsearch.php?q=24364&tip=sid&clean=0',\n",
    "    'CURRENT MEDICAL IMAGING REVIEWS': 'https://www.scimagojr.com/journalsearch.php?q=4700152432&tip=sid&clean=0',\n",
    "    'Diabetes Metabolic Syndrome and Obesity-Targets and Therapy': 'https://www.scimagojr.com/journalsearch.php?q=19700174905&tip=sid&clean=0',\n",
    "    'Diversity-Basel': 'https://www.scimagojr.com/journalsearch.php?q=6000187990&tip=sid&clean=0',\n",
    "    'EKSPLOATACJA I NIEZAWODNOSC-MAINTENANCE AND RELIABILITY': 'https://www.scimagojr.com/journalsearch.php?q=19700182638&tip=sid&clean=0',\n",
    "    'ENGINEERING SCIENCE AND TECHNOLOGY-AN INTERNATIONAL JOURNAL-JESTECH': 'https://www.scimagojr.com/journalsearch.php?q=21100806003&tip=sid&clean=0',\n",
    "    'FERMENTATION-BASEL': 'https://www.scimagojr.com/journalsearch.php?q=21100900055&tip=sid&clean=0',\n",
    "    'FUTURE GENERATION COMPUTER SYSTEMS-THE INTERNATIONAL JOURNAL OF ESCIENCE': 'https://www.scimagojr.com/journalsearch.php?q=12264&tip=sid&clean=0',\n",
    "    'GRAEFES ARCHIVE FOR CLINICAL AND EXPERIMENTAL OPHTHALMOLOGY': 'https://www.scimagojr.com/journalsearch.php?q=13904&tip=sid&clean=0',\n",
    "    'HEPATOBILIARY SURGERY AND NUTRITION': 'https://www.scimagojr.com/journalsearch.php?q=21101019769&tip=sid&clean=0',\n",
    "    'IEEE transactions on applied superconductivity : a publication of the IEEE Superconductivity Committee': 'https://www.scimagojr.com/journalsearch.php?q=26026&tip=sid&clean=0',\n",
    "    'IEEE TRANSACTIONS ON INFORMATION TECHNOLOGY IN BIOMEDICINE': 'https://www.scimagojr.com/journalsearch.php?q=21100256982&tip=sid&clean=0',\n",
    "    'IIE TRANSACTIONS': 'https://www.scimagojr.com/journalsearch.php?q=21100794597&tip=sid&clean=0',\n",
    "    'INJURY-INTERNATIONAL JOURNAL OF THE CARE OF THE INJURED': 'https://www.scimagojr.com/journalsearch.php?q=29801&tip=sid&clean=0',\n",
    "    'INORGANIC CHEMISTRY COMMUNICATIONS': 'https://www.scimagojr.com/journalsearch.php?q=25267&tip=sid&clean=0',\n",
    "    'J. Alloys Compd.': 'https://www.scimagojr.com/journalsearch.php?q=12325&tip=sid&clean=0',\n",
    "    'J. of Ceramic Precessing Research': 'https://www.scimagojr.com/journalsearch.php?q=22026&tip=sid&clean=0',\n",
    "    'JNCI-JOURNAL OF THE NATIONAL CANCER INSTITUTE': 'https://www.scimagojr.com/journalsearch.php?q=12459&tip=sid&clean=0',\n",
    "    'Jouranl of Ceramic Processing Research': 'https://www.scimagojr.com/journalsearch.php?q=22026&tip=sid&clean=0',\n",
    "    'JOURNAL OF ALZHEIMERS DISEASE': 'https://www.scimagojr.com/journalsearch.php?q=16246&tip=sid&clean=0',\n",
    "    'JOURNAL OF BONE AND JOINT SURGERY-AMERICAN VOLUME': 'https://www.scimagojr.com/journalsearch.php?q=12198&tip=sid&clean=0',\n",
    "    'JOURNAL OF CLOUD COMPUTING-ADVANCES SYSTEMS AND APPLICATIONS': 'https://www.scimagojr.com/journalsearch.php?q=21100383744&tip=sid&clean=0',\n",
    "    'JOURNAL OF CRANIO-MAXILLOFACIAL SURGERY': 'https://www.scimagojr.com/journalsearch.php?q=21658&tip=sid&clean=0',\n",
    "    'JOURNAL OF ENVIRONMENTAL SCIENCES-CHINA': 'https://www.scimagojr.com/journalsearch.php?q=23393&tip=sid&clean=0',\n",
    "    'JOURNAL OF FOOD SCIENCE AND TECHNOLOGY-MYSORE': 'https://www.scimagojr.com/journalsearch.php?q=20617&tip=sid&clean=0',\n",
    "    'JOURNAL OF MACROMOLECULAR SCIENCE PART A-PURE AND APPLIED CHEMISTRY': 'https://www.scimagojr.com/journalsearch.php?q=25891&tip=sid&clean=0',\n",
    "    'JOURNAL OF MACROMOLECULAR SCIENCE PART B-PHYSICS': 'https://www.scimagojr.com/journalsearch.php?q=28522&tip=sid&clean=0',\n",
    "    'JOURNAL OF MATERIALS RESEARCH AND TECHNOLOGY-JMR&T': 'https://www.scimagojr.com/journalsearch.php?q=21100383742&tip=sid&clean=0',\n",
    "    'JOURNAL OF MECHANISMS AND ROBOTICS-TRANSACTIONS OF THE ASME': 'https://www.scimagojr.com/journalsearch.php?q=19700186816&tip=sid&clean=0',\n",
    "    'JOURNAL OF OFFSHORE MECHANICS AND ARCTIC ENGINEERING-TRANSACTIONS OF THE ASME': 'https://www.scimagojr.com/journalsearch.php?q=20985&tip=sid&clean=0',\n",
    "    'JOURNAL OF PHYSIOLOGY-LONDON': 'https://www.scimagojr.com/journalsearch.php?q=23478&tip=sid&clean=0',\n",
    "    'JOURNAL OF PROSTHODONTICS-IMPLANT ESTHETIC AND RECONSTRUCTIVE DENTISTRY': 'https://www.scimagojr.com/journalsearch.php?q=26177&tip=sid&clean=0',\n",
    "    'JOURNAL OF THE FRANKLIN INSTITUTE-ENGINEERING AND APPLIED MATHEMATICS': 'https://www.scimagojr.com/journalsearch.php?q=27959&tip=sid&clean=0',\n",
    "    'JOURNAL OF TRIBOLOGY-TRANSACTIONS OF THE ASME': 'https://www.scimagojr.com/journalsearch.php?q=17014&tip=sid&clean=0',\n",
    "    'Jove-Journal of Visualized Experiments': 'https://www.scimagojr.com/journalsearch.php?q=19900191993&tip=sid&clean=0',\n",
    "    'LWT-FOOD SCIENCE AND TECHNOLOGY': 'https://www.scimagojr.com/journalsearch.php?q=22475&tip=sid&clean=0',\n",
    "    'MARINE ECOLOGY-AN EVOLUTIONARY PERSPECTIVE': 'https://www.scimagojr.com/journalsearch.php?q=12169&tip=sid&clean=0',\n",
    "    'MATERIALS SCIENCE-MEDZIAGOTYRA': 'https://www.scimagojr.com/journalsearch.php?q=19300157032&tip=sid',\n",
    "    'MEMBRANE AND WATER TREATMENT': 'https://www.scimagojr.com/journalsearch.php?q=21100265344&tip=sid&clean=0',\n",
    "    'MICROCHIMICA ACTA': 'https://www.scimagojr.com/journalsearch.php?q=24072&tip=sid',\n",
    "    'Natural Hazards and Earth System Sciences Discussions': 'https://www.scimagojr.com/journalsearch.php?q=51166&tip=sid&clean=0',\n",
    "    'PHYSICA STATUS SOLIDI B-BASIC SOLID STATE PHYSICS': 'https://www.scimagojr.com/journalsearch.php?q=29140&tip=sid&clean=0',\n",
    "    'PLANT GENETIC RESOURCES-CHARACTERIZATION AND UTILIZATION': 'https://www.scimagojr.com/journalsearch.php?q=98120&tip=sid&clean=0',\n",
    "    'POLIMEROS-CIENCIA E TECNOLOGIA': 'https://www.scimagojr.com/journalsearch.php?q=5000156907&tip=sid&clean=0',\n",
    "    'PRECISION ENGINEERING-JOURNAL OF THE INTERNATIONAL SOCIETIES FOR PRECISION ENGINEERING AND NANOTECHNOLOGY': 'https://www.scimagojr.com/journalsearch.php?q=13790&tip=sid&clean=0',\n",
    "    'PUBLICATIONS OF THE ASTRONOMICAL SOCIETY OF JAPAN': 'https://www.scimagojr.com/journalsearch.php?q=27778&tip=sid&clean=0',\n",
    "    'RETINA-THE JOURNAL OF RETINAL AND VITREOUS DISEASES': 'https://www.scimagojr.com/journalsearch.php?q=15094&tip=sid',\n",
    "    'REVISTA BRASILEIRA DE FARMACOGNOSIA-BRAZILIAN JOURNAL OF PHARMACOGNOSY': 'https://www.scimagojr.com/journalsearch.php?q=5400152628&tip=sid&exact=no',\n",
    "    'Sensors (Switzerland)': 'https://www.scimagojr.com/journalsearch.php?q=130124&tip=sid',\n",
    "    'SIMULATION-TRANSACTIONS OF THE SOCIETY FOR MODELING AND SIMULATION INTERNATIONAL': 'https://www.scimagojr.com/journalsearch.php?q=14452&tip=sid&clean=0',\n",
    "    'Spanish journal of agricultural research = Revista de investigaci oacute;n agraria : SJAR': 'https://www.scimagojr.com/journalsearch.php?q=5800179591&tip=sid',\n",
    "    'STRUCTURAL HEALTH MONITORING-AN INTERNATIONAL JOURNAL': 'https://www.scimagojr.com/journalsearch.php?q=5800179591&tip=sid',\n",
    "    'Sustainability (Switzerland)': 'https://www.scimagojr.com/journalsearch.php?q=21100240100&tip=sid',\n",
    "    'SYNTHESIS-STUTTGART': 'https://www.scimagojr.com/journalsearch.php?q=26508&tip=sid&clean=0',\n",
    "    'The American Journal of Chinese Medicine': 'https://www.scimagojr.com/journalsearch.php?q=18134&tip=sid&clean=0',\n",
    "    'Turkish Journal of Biochemistry-Turk Biyokimya Dergisi': 'https://www.scimagojr.com/journalsearch.php?q=17600155132&tip=sid&clean=0',\n",
    "    'WORLD JOURNAL OF MENS HEALTH': 'https://www.scimagojr.com/journalsearch.php?q=21100943924&tip=sid&clean=0',\n",
    "    'WORLD WIDE WEB-INTERNET AND WEB INFORMATION SYSTEMS': 'https://www.scimagojr.com/journalsearch.php?q=14965&tip=sid&clean=0',\n",
    "    'ZEITSCHRIFT FUR PHYSIKALISCHE CHEMIE-INTERNATIONAL JOURNAL OF RESEARCH IN PHYSICAL CHEMISTRY & CHEMICAL PHYSICS': 'https://www.scimagojr.com/journalsearch.php?q=23775&tip=sid',\n",
    "    'INFORMATION-AN INTERNATIONAL INTERDISCIPLINARY JOURNAL': 'https://www.scimagojr.com/journalsearch.php?q=21100201065&tip=sid&clean=0'\n",
    "}\n",
    "\n",
    "\n",
    "# Step 1: Normalize the Journal names in the DataFrame\n",
    "def normalize_journal_name(journal_name):\n",
    "    # Remove spaces and convert to lowercase\n",
    "    return journal_name.replace(\"-\", \"\").replace(\" \", \"\").lower()\n",
    "\n",
    "# Apply normalization to the 'Journal' column\n",
    "df['Normalized_Journal'] = df['Journal'].apply(normalize_journal_name)\n",
    "\n",
    "# Step 2: Create a normalized version of the manual_journal_links dictionary\n",
    "normalized_manual_links = {normalize_journal_name(k): v for k, v in manual_journal_links.items()}\n",
    "\n",
    "# Step 3: Fill missing 'Journal_Link' values using the normalized dictionary\n",
    "df['Journal_Link'] = df.apply(\n",
    "    lambda row: normalized_manual_links.get(normalize_journal_name(row['Journal']), row['Journal_Link']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop the normalized journal column if no longer needed\n",
    "df.drop(columns=['Normalized_Journal'], inplace=True)\n",
    "\n",
    "# Step 4: Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('Updated_Paper_Dataset.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Missing 'Journal_Link' fields have been filled based on your manual entries, and the updated dataset has been saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organization Name Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 0: Import required libraries\n",
    "# import pandas as pd\n",
    "# import deepl\n",
    "\n",
    "# # Step 1: Load the dataset\n",
    "# file_path = 'Project_Dataset.csv'\n",
    "# df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# # Step 2: Extract all organizations from the entire dataset and remove duplicates\n",
    "# def extract_unique_organizations(df_column):\n",
    "#     all_orgs = []\n",
    "#     for org_string in df_column:\n",
    "#         org_list = org_string.split(',')\n",
    "#         all_orgs.extend([org.strip() for org in org_list])  # Add each organization after stripping spaces\n",
    "#     unique_orgs = list(set(all_orgs))  # Get unique organizations\n",
    "#     return unique_orgs\n",
    "\n",
    "# # Get the unique organization list from the '협업_매트릭스' column\n",
    "# unique_org_list = extract_unique_organizations(df['협업_매트릭스'])\n",
    "\n",
    "# # Step 3: Translate the unique organization list using DeepL API\n",
    "# auth_key = '87c57d81-4f86-4fed-b8cc-39dbc3d0de99:fx'  # Replace with your actual DeepL API key\n",
    "# translator = deepl.Translator(auth_key)\n",
    "\n",
    "# # Step 3: Translate the unique organization list using DeepL API with progress tracking\n",
    "# def translate_organizations(org_list, translator):\n",
    "#     translated_orgs = []\n",
    "#     total_orgs = len(org_list)  # Get the total number of organizations\n",
    "#     ten_percent = total_orgs // 10  # Calculate the number of organizations that represent 10%\n",
    "\n",
    "#     for i, org in enumerate(org_list, 1):  # Enumerate to track the index (starting from 1)\n",
    "#         result = translator.translate_text(org, target_lang=\"EN-US\")  # Translate to English\n",
    "#         translated_orgs.append(result.text)  # Append translated text to the list\n",
    "        \n",
    "#         # Print progress every 10%\n",
    "#         if i % ten_percent == 0 or i == total_orgs:  # Check for 10% milestones or the last item\n",
    "#             progress_percentage = (i / total_orgs) * 100\n",
    "#             print(f\"Progress: {progress_percentage:.0f}% - Processed {i}/{total_orgs} organizations.\")\n",
    "\n",
    "#     return translated_orgs\n",
    "\n",
    "# # Use the translator to translate the organization list\n",
    "# translated_org_list = translate_organizations(unique_org_list, translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary to map Korean to English\n",
    "# org_translation_dict = dict(zip(unique_org_list, translated_org_list))\n",
    "\n",
    "# # Step 5: Replace Korean names with English names in '협업_매트릭스'\n",
    "# def replace_org_names(org_column, translation_dict):\n",
    "#     updated_orgs = []\n",
    "#     for org_string in org_column:\n",
    "#         for korean_org, english_org in translation_dict.items():\n",
    "#             org_string = org_string.replace(korean_org, english_org)  # Replace Korean org names with English\n",
    "#         updated_orgs.append(org_string)\n",
    "#     return updated_orgs\n",
    "\n",
    "# # Apply the replacement to the '협업_매트릭스' column\n",
    "# df['Org_List'] = replace_org_names(df['협업_매트릭스'], org_translation_dict)\n",
    "\n",
    "# # Step 6: Save the updated DataFrame to a new CSV file\n",
    "# output_file_path = 'Project_Dataset_Translated.csv'\n",
    "# df.to_csv(output_file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour à tous !\n"
     ]
    }
   ],
   "source": [
    "# import deepl\n",
    "\n",
    "# auth_key = \"87c57d81-4f86-4fed-b8cc-39dbc3d0de99:fx\"  # Replace with your key\n",
    "# translator = deepl.Translator(auth_key)\n",
    "\n",
    "# result = translator.translate_text(\"Hello, world!\", target_lang=\"FR\")\n",
    "# print(result.text)  # \"Bonjour, le monde !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation progress: 50 / 2768\n",
      "Translation progress: 100 / 2768\n",
      "Translation progress: 150 / 2768\n",
      "Translation progress: 200 / 2768\n",
      "Translation progress: 250 / 2768\n",
      "Translation progress: 300 / 2768\n",
      "Translation progress: 350 / 2768\n",
      "Translation progress: 400 / 2768\n",
      "Translation progress: 450 / 2768\n",
      "Translation progress: 500 / 2768\n",
      "Translation progress: 550 / 2768\n",
      "Translation progress: 600 / 2768\n",
      "Translation progress: 650 / 2768\n",
      "Translation progress: 700 / 2768\n",
      "Translation progress: 750 / 2768\n",
      "Translation progress: 800 / 2768\n",
      "Translation progress: 850 / 2768\n",
      "Translation progress: 900 / 2768\n",
      "Translation progress: 950 / 2768\n",
      "Translation progress: 1000 / 2768\n",
      "Translation progress: 1050 / 2768\n",
      "Translation progress: 1100 / 2768\n",
      "Translation progress: 1150 / 2768\n",
      "Translation progress: 1200 / 2768\n",
      "Translation progress: 1250 / 2768\n",
      "Translation progress: 1300 / 2768\n",
      "Translation progress: 1350 / 2768\n",
      "Translation progress: 1400 / 2768\n",
      "Translation progress: 1450 / 2768\n",
      "Translation progress: 1500 / 2768\n",
      "Translation progress: 1550 / 2768\n",
      "Translation progress: 1600 / 2768\n",
      "Translation progress: 1650 / 2768\n",
      "Translation progress: 1700 / 2768\n",
      "Translation progress: 1750 / 2768\n",
      "Translation progress: 1800 / 2768\n",
      "Translation progress: 1850 / 2768\n",
      "Translation progress: 1900 / 2768\n",
      "Translation progress: 1950 / 2768\n",
      "Translation progress: 2000 / 2768\n",
      "Translation progress: 2050 / 2768\n",
      "Translation progress: 2100 / 2768\n",
      "Translation progress: 2150 / 2768\n",
      "Translation progress: 2200 / 2768\n",
      "Translation progress: 2250 / 2768\n",
      "Translation progress: 2300 / 2768\n",
      "Translation progress: 2350 / 2768\n",
      "Translation progress: 2400 / 2768\n",
      "Translation progress: 2450 / 2768\n",
      "Translation progress: 2500 / 2768\n",
      "Translation progress: 2550 / 2768\n",
      "Translation progress: 2600 / 2768\n",
      "Translation progress: 2650 / 2768\n",
      "Translation progress: 2700 / 2768\n",
      "Translation progress: 2750 / 2768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_16236\\3748535855.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Affiliation_KR'] = translate_affiliations(df_filtered['Affiliation'], org_translation_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translated dataset has been saved as 'Paper_Dataset_translated.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import required libraries\n",
    "import pandas as pd\n",
    "import deepl\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'Paper_Dataset_rv.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 2: Filter the data for '육T관련기술' == 2\n",
    "df_filtered = df[df['육T관련기술'] == 2]\n",
    "\n",
    "# Step 3: Extract unique organizations from the 'Affiliation' column\n",
    "def extract_unique_organizations(df_column):\n",
    "    all_orgs = set()  # Using a set to avoid duplicates\n",
    "    for org_string in df_column.dropna():  # Handle any NaN values\n",
    "        org_list = org_string.split(';')  # Split by \";\"\n",
    "        all_orgs.update([org.strip() for org in org_list])  # Strip spaces and add to set\n",
    "    return list(all_orgs)  # Convert back to list for further processing\n",
    "\n",
    "# Create a unique organization list from the 'Affiliation' column\n",
    "unique_org_list = extract_unique_organizations(df_filtered['Affiliation'])\n",
    "\n",
    "# Step 4: Translate the unique organization list to Korean using DeepL API\n",
    "auth_key = '87c57d81-4f86-4fed-b8cc-39dbc3d0de99:fx'  # Replace with your actual DeepL API key\n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "def translate_organizations(org_list, translator):\n",
    "    translated_orgs = []\n",
    "    for idx, org in enumerate(org_list, start=1):  # start=1 to begin the count at 1\n",
    "        if org:  # Only translate if the organization name is not empty\n",
    "            result = translator.translate_text(org, target_lang=\"KO\")  # Translate to Korean\n",
    "            translated_orgs.append(result.text)\n",
    "        else:\n",
    "            translated_orgs.append('')  # Append an empty string if the organization name is empty\n",
    "        \n",
    "        # Print progress every 50 translations\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Translation progress: {idx} / {len(org_list)}\")\n",
    "\n",
    "    return translated_orgs\n",
    "\n",
    "# Translate the unique organization list\n",
    "translated_org_list = translate_organizations(unique_org_list, translator)\n",
    "\n",
    "# Step 5: Create a dictionary mapping English to Korean organizations\n",
    "org_translation_dict = dict(zip(unique_org_list, translated_org_list))\n",
    "\n",
    "# Step 6: Translate 'Affiliation' column in the filtered DataFrame\n",
    "def translate_affiliations(affiliation_column, translation_dict):\n",
    "    translated_affiliations = []\n",
    "    for org_string in affiliation_column.fillna(''):  # Handle NaN as empty strings\n",
    "        org_list = org_string.split(';')\n",
    "        translated_orgs = [translation_dict.get(org.strip(), org.strip()) for org in org_list]\n",
    "        translated_affiliations.append('; '.join(translated_orgs))\n",
    "    return translated_affiliations\n",
    "\n",
    "# Apply the translation to the 'Affiliation' column\n",
    "df_filtered['Affiliation_KR'] = translate_affiliations(df_filtered['Affiliation'], org_translation_dict)\n",
    "\n",
    "# Step 7: Save the updated DataFrame with translated affiliations to a new CSV file\n",
    "output_file_path = 'Paper_Dataset_translated.csv'\n",
    "df_filtered.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"The translated dataset has been saved as '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(translated_org_list, columns=['Organization_Deepl'])\n",
    "\n",
    "# Specify the file name\n",
    "file_name = 'translated_org_list.csv'\n",
    "\n",
    "# Write to CSV file\n",
    "df.to_csv(file_name, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique organization list saved as 'unique_organizations.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'Paper_Dataset_rv.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 2: Filter the data for '육T관련기술' == 2\n",
    "df_filtered = df[df['육T관련기술'] == 2]\n",
    "\n",
    "# Step 3: Extract unique organizations from the 'Affiliation' column and save to CSV\n",
    "def extract_unique_organizations(df_column):\n",
    "    all_orgs = set()  # Using a set to avoid duplicates\n",
    "    for org_string in df_column.dropna():  # Handle any NaN values\n",
    "        org_list = org_string.split(',')  # Split by \";\"\n",
    "        all_orgs.update([org.strip() for org in org_list])  # Strip spaces and add to set\n",
    "    return list(all_orgs)  # Convert back to list for further processing\n",
    "\n",
    "# Create and save unique organization list from 'Affiliation'\n",
    "unique_org_list = extract_unique_organizations(df_filtered['협업_매트릭스'])\n",
    "unique_org_df = pd.DataFrame(unique_org_list, columns=['Organization_KOR'])\n",
    "unique_org_df.to_csv('project_unique_organizations.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Unique organization list saved as 'unique_organizations.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched organizations saved to 'unmatched_organizations.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the datasets\n",
    "translated_org_df = pd.read_csv('translated_org_list.csv', encoding='utf-8-sig')\n",
    "matching_dict_df = pd.read_csv('matching_dict_Korean.csv', encoding='utf-8-sig')\n",
    "\n",
    "# Step 2: Identify unmatched Organization_Deepl entries\n",
    "# Check which entries in 'Organization_Deepl' are not in 'Organization_KOR'\n",
    "unmatched_deepl_orgs = translated_org_df[~translated_org_df['Organization_Deepl'].isin(matching_dict_df['Organization_KOR'])]\n",
    "\n",
    "# Step 3: Output the unmatched entries\n",
    "unmatched_deepl_orgs.to_csv('unmatched_organizations.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"Unmatched organizations saved to 'unmatched_organizations.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation progress: 50 / 2768\n",
      "Translation progress: 100 / 2768\n",
      "Translation progress: 150 / 2768\n",
      "Translation progress: 200 / 2768\n",
      "Translation progress: 250 / 2768\n",
      "Translation progress: 300 / 2768\n",
      "Translation progress: 350 / 2768\n",
      "Translation progress: 400 / 2768\n",
      "Translation progress: 450 / 2768\n",
      "Translation progress: 500 / 2768\n",
      "Translation progress: 550 / 2768\n",
      "Translation progress: 600 / 2768\n",
      "Translation progress: 650 / 2768\n",
      "Translation progress: 700 / 2768\n",
      "Translation progress: 750 / 2768\n",
      "Translation progress: 800 / 2768\n",
      "Translation progress: 850 / 2768\n",
      "Translation progress: 900 / 2768\n",
      "Translation progress: 950 / 2768\n",
      "Translation progress: 1000 / 2768\n",
      "Translation progress: 1050 / 2768\n",
      "Translation progress: 1100 / 2768\n",
      "Translation progress: 1150 / 2768\n",
      "Translation progress: 1200 / 2768\n",
      "Translation progress: 1250 / 2768\n",
      "Translation progress: 1300 / 2768\n",
      "Translation progress: 1350 / 2768\n",
      "Translation progress: 1400 / 2768\n",
      "Translation progress: 1450 / 2768\n",
      "Translation progress: 1500 / 2768\n",
      "Translation progress: 1550 / 2768\n",
      "Translation progress: 1600 / 2768\n",
      "Translation progress: 1650 / 2768\n",
      "Translation progress: 1700 / 2768\n",
      "Translation progress: 1750 / 2768\n",
      "Translation progress: 1800 / 2768\n",
      "Translation progress: 1850 / 2768\n",
      "Translation progress: 1900 / 2768\n",
      "Translation progress: 1950 / 2768\n",
      "Translation progress: 2000 / 2768\n",
      "Translation progress: 2050 / 2768\n",
      "Translation progress: 2100 / 2768\n",
      "Translation progress: 2150 / 2768\n",
      "Translation progress: 2200 / 2768\n",
      "Translation progress: 2250 / 2768\n",
      "Translation progress: 2300 / 2768\n",
      "Translation progress: 2350 / 2768\n",
      "Translation progress: 2400 / 2768\n",
      "Translation progress: 2450 / 2768\n",
      "Translation progress: 2500 / 2768\n",
      "Translation progress: 2550 / 2768\n",
      "Translation progress: 2600 / 2768\n",
      "Translation progress: 2650 / 2768\n",
      "Translation progress: 2700 / 2768\n",
      "Translation progress: 2750 / 2768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_16236\\3748535855.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['Affiliation_KR'] = translate_affiliations(df_filtered['Affiliation'], org_translation_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translated dataset has been saved as 'Paper_Dataset_translated.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Import required libraries\n",
    "import pandas as pd\n",
    "import deepl\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = 'Paper_Dataset_rv.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "\n",
    "# Step 2: Filter the data for '육T관련기술' == 2\n",
    "df_filtered = df[df['육T관련기술'] == 2]\n",
    "\n",
    "# Step 3: Extract unique organizations from the 'Affiliation' column\n",
    "def extract_unique_organizations(df_column):\n",
    "    all_orgs = set()  # Using a set to avoid duplicates\n",
    "    for org_string in df_column.dropna():  # Handle any NaN values\n",
    "        org_list = org_string.split(';')  # Split by \";\"\n",
    "        all_orgs.update([org.strip() for org in org_list])  # Strip spaces and add to set\n",
    "    return list(all_orgs)  # Convert back to list for further processing\n",
    "\n",
    "# Create a unique organization list from the 'Affiliation' column\n",
    "unique_org_list = extract_unique_organizations(df_filtered['Affiliation'])\n",
    "\n",
    "# Step 4: Translate the unique organization list to Korean using DeepL API\n",
    "auth_key = '87c57d81-4f86-4fed-b8cc-39dbc3d0de99:fx'  # Replace with your actual DeepL API key\n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "def translate_organizations(org_list, translator):\n",
    "    translated_orgs = []\n",
    "    for idx, org in enumerate(org_list, start=1):  # start=1 to begin the count at 1\n",
    "        if org:  # Only translate if the organization name is not empty\n",
    "            result = translator.translate_text(org, target_lang=\"KO\")  # Translate to Korean\n",
    "            translated_orgs.append(result.text)\n",
    "        else:\n",
    "            translated_orgs.append('')  # Append an empty string if the organization name is empty\n",
    "        \n",
    "        # Print progress every 50 translations\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Translation progress: {idx} / {len(org_list)}\")\n",
    "\n",
    "    return translated_orgs\n",
    "\n",
    "# Translate the unique organization list\n",
    "translated_org_list = translate_organizations(unique_org_list, translator)\n",
    "\n",
    "# Step 5: Create a dictionary mapping English to Korean organizations\n",
    "org_translation_dict = dict(zip(unique_org_list, translated_org_list))\n",
    "\n",
    "# Step 6: Translate 'Affiliation' column in the filtered DataFrame\n",
    "def translate_affiliations(affiliation_column, translation_dict):\n",
    "    translated_affiliations = []\n",
    "    for org_string in affiliation_column.fillna(''):  # Handle NaN as empty strings\n",
    "        org_list = org_string.split(';')\n",
    "        translated_orgs = [translation_dict.get(org.strip(), org.strip()) for org in org_list]\n",
    "        translated_affiliations.append('; '.join(translated_orgs))\n",
    "    return translated_affiliations\n",
    "\n",
    "# Apply the translation to the 'Affiliation' column\n",
    "df_filtered['Affiliation_KR'] = translate_affiliations(df_filtered['Affiliation'], org_translation_dict)\n",
    "\n",
    "# Step 7: Save the updated DataFrame with translated affiliations to a new CSV file\n",
    "output_file_path = 'Paper_Dataset_translated.csv'\n",
    "df_filtered.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"The translated dataset has been saved as '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translated dataset has been saved as 'Paper_Dataset_translated.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Translate 'Affiliation' column in the filtered DataFrame\n",
    "def translate_affiliations(affiliation_column, translation_dict):\n",
    "    translated_affiliations = []\n",
    "    for org_string in affiliation_column.fillna(''):  # Handle NaN as empty strings\n",
    "        org_list = org_string.split(';')\n",
    "        translated_orgs = [translation_dict.get(org.strip(), org.strip()) for org in org_list]\n",
    "        translated_affiliations.append('; '.join(translated_orgs))\n",
    "    return translated_affiliations\n",
    "\n",
    "# Apply the translation to the 'Affiliation' column\n",
    "df_filtered['Affiliation_KR'] = translate_affiliations(df_filtered['Affiliation'], org_translation_dict)\n",
    "\n",
    "# Step 7: Save the updated DataFrame with translated affiliations to a new CSV file\n",
    "output_file_path = 'Paper_Dataset_translated.csv'\n",
    "df_filtered.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"The translated dataset has been saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링된 데이터가 'Filtered_Project_Dataset.csv'에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로에 'Project_Dataset.csv'를 사용하세요.\n",
    "dataset_path = 'Project_Dataset.csv'  # 파일이 있는 경로로 변경하세요\n",
    "project_data = pd.read_csv(dataset_path)\n",
    "\n",
    "# 조건: '육T관련기술'이 2이고 '논문_개수'가 1 이상인 데이터 필터링\n",
    "filtered_data = project_data[(project_data['육T관련기술'] == 2) & (project_data['논문_개수'] >= 1)]\n",
    "\n",
    "# 결과를 새로운 CSV 파일로 저장\n",
    "output_path = 'Filtered_Project_Dataset.csv'  # 저장할 경로를 원하는 대로 설정하세요\n",
    "filtered_data.to_csv(output_path, index=False, encoding = 'utf-8-sig')\n",
    "\n",
    "print(f\"필터링된 데이터가 '{output_path}'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching-Dict (Deepl - KOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_16236\\3031113090.py:4: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_filtered = pd.read_csv('Paper_Dataset_rv.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translated dataset has been saved as 'Paper_Dataset_translated.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the main dataset\n",
    "df_filtered = pd.read_csv('Paper_Dataset_rv.csv')\n",
    "\n",
    "# Load the updated organization matching dictionary with new translations\n",
    "translation_df = pd.read_csv('matching_dict_kor.csv')\n",
    "translation_dict = dict(zip(translation_df['Organization_ENG'], translation_df['Organization_KOR']))\n",
    "\n",
    "# Define the function to translate affiliations using the updated dictionary\n",
    "def translate_affiliations(affiliation_column, translation_dict):\n",
    "    translated_affiliations = []\n",
    "    for org_string in affiliation_column.fillna(''):  # Handle NaN as empty strings\n",
    "        # Ensure each organization name is treated as a string before processing\n",
    "        org_list = str(org_string).split(';')\n",
    "        translated_orgs = [translation_dict.get(org.strip(), org.strip()) for org in org_list]\n",
    "        # Filter out any non-string items in translated_orgs before joining\n",
    "        translated_affiliations.append('; '.join([org for org in translated_orgs if isinstance(org, str)]))\n",
    "    return translated_affiliations\n",
    "\n",
    "# Apply the updated translation to the 'Affiliation' column\n",
    "df_filtered['Affiliation'] = translate_affiliations(df_filtered['Affiliation'], translation_dict)\n",
    "\n",
    "# Save the updated DataFrame with new translations\n",
    "output_file_path = 'Paper_Dataset_420.csv'\n",
    "df_filtered.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"The translated dataset has been saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved as 'Merged_Output.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "org_a = pd.read_csv('ORG-A(Project).csv')\n",
    "org_b = pd.read_csv('ORG-B(Paper).csv')\n",
    "\n",
    "# Create the desired column order for the output\n",
    "column_order = ['과제고유번호', '협업_매트릭스', 'Affiliation_KR', 'Affiliation', 'Title', 'Authors', 'blau_index']\n",
    "\n",
    "# Merge based on the '과제고유번호' column using a left join to retain all rows from ORG-A\n",
    "merged_df = pd.merge(org_a, org_b, on='과제고유번호', how='left')\n",
    "\n",
    "# Sort values by '과제고유번호' to ensure rows with the same identifier appear together\n",
    "merged_df = merged_df.sort_values(by='과제고유번호')\n",
    "\n",
    "# Set the '협업_매트릭스', 'blau_index' columns to display only once per '과제고유번호' and leave others as empty\n",
    "merged_df['협업_매트릭스'] = merged_df['협업_매트릭스'].where(~merged_df.duplicated('과제고유번호'))\n",
    "merged_df['blau_index'] = merged_df['blau_index'].where(~merged_df.duplicated('과제고유번호'))\n",
    "\n",
    "# Rearrange the columns in the specified order\n",
    "merged_df = merged_df[column_order]\n",
    "\n",
    "# Export the result to a new CSV file\n",
    "merged_df.to_csv('Merged_Output.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Merged CSV saved as 'Merged_Output.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved as 'Updated_matching_dict_kor.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both CSV files\n",
    "file1 = pd.read_csv('matching_dict_kor.csv')\n",
    "file2 = pd.read_csv('translated_org_list.csv')\n",
    "\n",
    "# Find missing rows from file2 in file1 based on 'Organization_ENG'\n",
    "missing_rows = file2[~file2['Organization_ENG'].isin(file1['Organization_ENG'])]\n",
    "\n",
    "# Append missing rows to file1\n",
    "updated_file1 = pd.concat([file1, missing_rows], ignore_index=True)\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "updated_file1.to_csv('Updated_matching_dict_kor.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Updated CSV saved as 'Updated_matching_dict_kor.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변환이 완료되었습니다. DTA 파일로 저장되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_10816\\855627331.py:5: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  paper_df = pd.read_csv('Paper_Dataset_All.csv')\n",
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_10816\\855627331.py:17: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    과제고유번호   ->   ______\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata('Blau_Scope_Test.dta')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일을 읽어옵니다.\n",
    "project_df = pd.read_csv('Project_Dataset.csv')\n",
    "paper_df = pd.read_csv('Paper_Dataset_All.csv')\n",
    "\n",
    "# '육T관련기술'이 2인 행만 필터링합니다.\n",
    "filtered_paper_df = paper_df[paper_df['육T관련기술'] == 2]\n",
    "\n",
    "# '과제고유번호'를 기준으로 merge하여 'blau_index' 값을 가져옵니다.\n",
    "merged_df = filtered_paper_df.merge(project_df[['과제고유번호', 'blau_index']], on='과제고유번호', how='left')\n",
    "\n",
    "# 필요한 열만 선택합니다.\n",
    "final_df = merged_df[['ID', '과제고유번호', 'Scope_Top_Count', 'blau_index']]\n",
    "\n",
    "# 선택한 DataFrame을 DTA 파일로 내보냅니다.\n",
    "final_df.to_stata('Blau_Scope_Test.dta')\n",
    "\n",
    "print(\"변환이 완료되었습니다. DTA 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV files\n",
    "paper_df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "matching_dict_df = pd.read_csv(\"dict_kor.csv\")\n",
    "\n",
    "# Step 2: Create a dictionary for English-Korean name mapping\n",
    "eng_to_kor_dict = dict(zip(matching_dict_df['Organization_ENG'], matching_dict_df['Organization_KOR']))\n",
    "\n",
    "def translate_affiliation(affiliation):\n",
    "    # 리스트로 분리\n",
    "    org_names = affiliation.split(';')  \n",
    "    korean_names = []\n",
    "\n",
    "    for name in org_names:\n",
    "        name = name.strip()  # 공백 제거\n",
    "        \n",
    "        if isinstance(name, str):  # 문자열인지 확인\n",
    "            # 영어 이름이 Org_ENG에 있는지 확인\n",
    "            if name in eng_to_kor_dict:  # Org_ENG에 존재하면\n",
    "                korean_translation = eng_to_kor_dict[name]\n",
    "                if korean_translation:  # 번역이 있을 경우\n",
    "                    korean_names.append(korean_translation)\n",
    "            else:\n",
    "                # Org_ENG에 없으면 영어 그대로 반환\n",
    "                korean_names.append(name)\n",
    "\n",
    "    # 번역된 조직명만 반환 (빈 값은 제외)\n",
    "    return '; '.join([str(korean_name) for korean_name in korean_names if isinstance(korean_name, str)])\n",
    "\n",
    "\n",
    "# Apply the translation function to create the 'Affiliation_KR' column\n",
    "paper_df['Affiliation_KR'] = paper_df['Affiliation'].apply(translate_affiliation)\n",
    "\n",
    "# Step 4: Save the result to a new CSV file if needed\n",
    "paper_df.to_csv(\"Translated_Paper_Dataset_Bio.csv\", index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
