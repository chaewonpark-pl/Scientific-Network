{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOI Mapping with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the .dta files\n",
    "paper_230130 = pd.read_stata('3.Paper_230130.dta')\n",
    "paper_bio = pd.read_stata('Paper_Dataset_Bio.dta')\n",
    "\n",
    "# 2. Normalization function\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', text).lower()\n",
    "\n",
    "# Apply normalization\n",
    "paper_230130['Paper_norm'] = paper_230130['pa030'].apply(normalize_text)\n",
    "paper_bio['Title_norm'] = paper_bio['Title'].apply(normalize_text)\n",
    "\n",
    "# 3. Create a dictionary for quick lookup\n",
    "doi_mapping = paper_230130.set_index('Paper_norm')['pa140'].to_dict()\n",
    "\n",
    "# 4. Map DOI based on Title_norm\n",
    "paper_bio['DOI'] = paper_bio['Title_norm'].map(doi_mapping)\n",
    "\n",
    "# Save the updated dataset\n",
    "paper_bio.to_csv('Updated_doi.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch DOI using Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI for 'Population genetic study of 10 short tandem repeat loci from 600 domestic dogs in Korea': None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Semantic Scholar API settings\n",
    "API_KEY = \"d9uN8uXwqK2FkU7VYFGJr9ECqcyAYh3Gx0QRyOy7\"\n",
    "HEADERS = {\"x-api-key\": API_KEY}\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "# Paper title to search\n",
    "title = \"Population genetic study of 10 short tandem repeat loci from 600 domestic dogs in Korea\"\n",
    "\n",
    "# Function to fetch DOI from Semantic Scholar\n",
    "def fetch_doi(title):\n",
    "    params = {\"query\": title, \"fields\": \"externalIds\", \"limit\": 1}\n",
    "    response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"data\" in data and data[\"data\"]:\n",
    "            external_ids = data[\"data\"][0].get(\"externalIds\", {})\n",
    "            return external_ids.get(\"DOI\", None)\n",
    "    return None\n",
    "\n",
    "# Fetch DOI\n",
    "doi = fetch_doi(title)\n",
    "print(f\"DOI for '{title}': {doi}\")\n",
    "\n",
    "time.sleep(1)  # Avoid rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching DOI using Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Enhanced sensitivity of CpG island search and primer design based on predicted CpG island position': '10.1016/j.fsigen.2018.02.013'}\n"
     ]
    }
   ],
   "source": [
    "from habanero import Crossref\n",
    "cr = Crossref()\n",
    "titles = [\"Enhanced sensitivity of CpG island search and primer design based on predicted CpG island position\"]  # Replace with your list of titles\n",
    "dois = {}\n",
    "for title in titles:\n",
    "    result = cr.works(query=title)\n",
    "    if result['message']['items']:\n",
    "        dois[title] = result['message']['items'][0]['DOI']\n",
    "    else:\n",
    "        dois[title] = None\n",
    "print(dois)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching DOI for A lectin-coupled, multiple reaction monitoring based quantitative analysis of human plasma glycoproteins by mass spectrometry: The read operation timed out\n",
      "DOI collection completed for first 10 missing entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from habanero import Crossref\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# Filter papers with missing DOIs\n",
    "missing_doi_df = df[df['DOI'].isna()]  # Select first 10 missing DOIs\n",
    "\n",
    "# Initialize Crossref API\n",
    "cr = Crossref()\n",
    "\n",
    "# Function to fetch DOI\n",
    "def get_doi(title):\n",
    "    try:\n",
    "        result = cr.works(query=title)\n",
    "        if result['message']['items']:\n",
    "            return result['message']['items'][0]['DOI']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI for {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Fetch DOIs sequentially\n",
    "for idx, row in missing_doi_df.iterrows():\n",
    "    title = row['Title']\n",
    "    doi = get_doi(title)\n",
    "    df.at[idx, 'DOI'] = doi  # Update original DataFrame\n",
    "\n",
    "# Save updated dataset\n",
    "df.to_csv(\"Paper_Dataset_Bio_Updated.csv\", index=False)\n",
    "\n",
    "print(\"DOI collection completed for first 10 missing entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching DOI for Kinetic studies on the formation of various II-VI semiconductor nanocrystals and synthesis of gradient alloy quantum dots emitting in the entire visible range: The read operation timed out\n",
      "Error fetching DOI for Globoside promotes activation of ERK by interaction with the epidermal growth factor receptor: The read operation timed out\n",
      "Error fetching DOI for Scanometric analysis of DNA microarrays using DNA intercalator-conjugated gold nanoparticles: The read operation timed out\n",
      "Error fetching DOI for A label-free fluorescence immunoassay system for the sensitive detection of the mycotoxin, ochratoxin A: The read operation timed out\n",
      "Error fetching DOI for Highly Efficient Enzyme Immobilization and Stabilization within Meso-Structured Onion-Like Silica for Biodiesel Production: The read operation timed out\n",
      "Error fetching DOI for Mutational complex genotype of the hepatitis B virus X/precore regions as a novel predictive marker for hepatocellular carcinoma: The read operation timed out\n",
      "Error fetching DOI for Electrochemical oxidation of glucose at nanoporous black gold surfaces in the presence of high concentration of chloride ions and application to amperometric detection: The read operation timed out\n",
      "Error fetching DOI for Electron Paramagnetic Resonance Investigation of Different Plant Organs after Gamma Irradiation: The read operation timed out\n",
      "Error fetching DOI for Proteomic characterization of Kunitz trypsin inhibitor variants, Tia and Tib, in soybean [Glycine max (L.) Merrill]: The read operation timed out\n",
      "Error fetching DOI for Development of a Qualitative Dose Indicator for Gamma Radiation Using Lyophilized Deinococcus: The read operation timed out\n",
      "Error fetching DOI for Genome-wide transcriptome analysis of rice genes responsive to chilling stress: The read operation timed out\n",
      "Error fetching DOI for Isolation of cellulose fibers from kenaf using electron beam: The read operation timed out\n",
      "Error fetching DOI for Deinococcus daejeonensis sp nov., isolated from sludge in a sewage disposal plant: The read operation timed out\n",
      "Error fetching DOI for Anti-obesity effect of Schisandra chinensis in 3T3-L1 cells and high fat diet-induced obese rats: The read operation timed out\n",
      "Processed 50 papers...\n",
      "Error fetching DOI for Self-Fluorescence of Chemically Crosslinked MRI Nanoprobes to Enable Multimodal Imaging of Therapeutic Cells: The read operation timed out\n",
      "Error fetching DOI for Virulence and transmissibility of H1N2 influenza virus in ferrets imply the continuing threat of triple-reassortant swine viruses: The read operation timed out\n",
      "Error fetching DOI for Dual Role of Respiratory Syncytial Virus Glycoprotein Fragment as a Mucosal Immunogen and Chemotactic Adjuvant: The read operation timed out\n",
      "Error fetching DOI for Antibacterial activity of LCB01-0062, a novel oxazolidinone: The read operation timed out\n",
      "Error fetching DOI for Draft Genome Sequence of Paenisporosarcina sp Strain TG-20, a Psychrophilic Bacterium Isolated from the Basal Ice of Taylor Glacier: The read operation timed out\n",
      "Error fetching DOI for Draft Genome Sequence of Moritella dasanensis Strain ArB 0140, a Psychrophilic Bacterium Isolated from the Arctic Ocean: The read operation timed out\n",
      "Error fetching DOI for Effect of decursinol angelate on the pharmacokinetics of theophylline and its metabolites in rats: The read operation timed out\n",
      "Error fetching DOI for Anti-platelet activity of diacetylated obovatol through regulating cyclooxygenase and lipoxygenase activities: The read operation timed out\n",
      "Error fetching DOI for Anti-inflammatory effect of sinomenine by inhibition of pro-inflammatory mediators in PMA plus A23187-stimulated HMC-1 Cells: The read operation timed out\n",
      "Error fetching DOI for In Vitro and In Vivo Genotoxicity Assessment of Aristolochia manshuriensis Kom.: The read operation timed out\n",
      "Error fetching DOI for Inhibitory Effect of Arctigenin from Fructus Arctii Extract on Melanin Synthesis via Repression of Tyrosinase Expression: The read operation timed out\n",
      "Processed 100 papers...\n",
      "Error fetching DOI for Cytochrome P450-mediated herb-drug interaction potential of Galgeun-tang: The read operation timed out\n",
      "Error fetching DOI for Anti-inflammatory effect of Lycium Fruit water extract in lipopolysaccharide-stimulated RAW 264.7 macrophage cells: The read operation timed out\n",
      "Error fetching DOI for Fermentation by Lactobacillus enhances anti-inflammatory effect of Oyaksungisan on LPS-stimulated RAW 264.7 mouse macrophage cells: The read operation timed out\n",
      "Error fetching DOI for Tmem64 Modulates Calcium Signaling during RANKL-Mediated Osteoclast Differentiation: The read operation timed out\n",
      "Error fetching DOI for beta-Arrestin 2 Mediates G Protein-Coupled Receptor 43 Signals to Nuclear Factor-kappa B: The read operation timed out\n",
      "Error fetching DOI for Fluorescent peptide indicator displacement assay for monitoring interactions between RNA and RNA binding proteins: The read operation timed out\n",
      "Error fetching DOI for One-step detection of circulating tumor cells in ovarian cancer using enhanced fluorescent silica nanoparticles: The read operation timed out\n",
      "Error fetching DOI for Zinc oxide nanoparticle induced autophagic cell death and mitochondrial damage via reactive oxygen species generation: The read operation timed out\n",
      "Error fetching DOI for An efficient synthesis of LipidGreen and its derivatives via microwave assisted reaction and their live lipid imaging in zebrafish: The read operation timed out\n",
      "Error fetching DOI for Identification of potential serum biomarkers for gastric cancer by a novel computational method, multiple normal tissues corrected differential analysis: The read operation timed out\n",
      "Error fetching DOI for The radiosensitivity of endothelial cells isolated from human breast cancer and normal tissue in vitro: The read operation timed out\n",
      "Error fetching DOI for Rate of Pulmonary Metastasis Varies with Location of Rectal Cancer in the Patients Undergoing Curative Resection: The read operation timed out\n",
      "Error fetching DOI for Applicability of Histoculture Drug Response Assays in Colorectal Cancer Chemotherapy: The read operation timed out\n",
      "Error fetching DOI for Clinical implications of mucinous components correlated with microsatellite instability in patients with colorectal cancer: The read operation timed out\n",
      "Processed 150 papers...\n",
      "Error fetching DOI for Phase I study of neoadjuvant chemoradiotherapy with S-1 and oxaliplatin in patients with locally advanced gastric cancer: The read operation timed out\n",
      "Error fetching DOI for Immunocytes as a Biocarrier to Delivery Therapeutic and Imaging Contrast Agents to Tumors: The read operation timed out\n",
      "Error fetching DOI for Novel histone deacetylase inhibitor CG200745 induces clonogenic cell death by modulating acetylation of p53 in cancer cells: The read operation timed out\n",
      "Error fetching DOI for Theranostic nanoparticles based on PEGylated hyaluronic acid for the diagnosis, therapy and monitoring of colon cancer: The read operation timed out\n",
      "Error fetching DOI for Use of macrophages to deliver therapeutic and imaging contrast agents to tumors: The read operation timed out\n",
      "Error fetching DOI for Metformin kills and radiosensitizes cancer cells and preferentially kills cancer stem cells: The read operation timed out\n",
      "Error fetching DOI for Open versus robot-assisted sphincter-saving operations in rectal cancer patients: techniques and comparison of outcomes between groups of 100 matched patients: The read operation timed out\n",
      "Error fetching DOI for Comparison of Three-Year Clinical Outcomes with Nonextended Versus Extended Dual Antiplatelet Therapy Between First- and Second-Generation Drug-Eluting Stent Implantation in Patients with Acute Myocardial Infarction: Data from the Infarct Prognosis Study Registry: The read operation timed out\n",
      "Error fetching DOI for Comparison between Measured and Calculated Length of Side Branch Ostium in Coronary Bifurcation Lesions with Intravascular Ultrasound: The read operation timed out\n",
      "Error fetching DOI for Effects of Combination Therapy with Celecoxib and Doxycycline on Neointimal Hyperplasia and Inflammatory Biomarkers in Coronary Artery Disease Patients Treated with Bare Metal Stents: The read operation timed out\n",
      "Error fetching DOI for Post-shock sinus node recovery time is an independent predictor of recurrence after catheter ablation of longstanding persistent atrial fibrillation: The read operation timed out\n",
      "Error fetching DOI for Prolonged Atrial Effective Refractory Periods in Atrial Fibrillation Patients Associated with Structural Heart Disease or Sinus Node Dysfunction Compared with Lone Atrial Fibrillation: The read operation timed out\n",
      "Processed 200 papers...\n",
      "Error fetching DOI for The clinical significance of the atrial subendocardial smooth muscle layer and cardiac myofibroblasts in human atrial tissue with valvular atrial fibrillation: The read operation timed out\n",
      "Error fetching DOI for High mobility group box-1 is phosphorylated by protein kinase C zeta and secreted in colon cancer cells: The read operation timed out\n",
      "Error fetching DOI for Enhancement of MSC adhesion and therapeutic efficiency in ischemic heart using lentivirus delivery with periostin: The read operation timed out\n",
      "Error fetching DOI for Consecutive Targetable Smart Nanoprobe for Molecular Recognition of Cytoplasmic microRNA in Metastatic Breast Cancer: The read operation timed out\n",
      "Error fetching DOI for Flecainide-Associated Bradycardia-Dependent Torsade de Pointes: Another Potential Mechanism of Proarrhythmia: The read operation timed out\n",
      "Error fetching DOI for Early neurological outcomes according to CHADS2 score in stroke patients with non-valvular atrial fibrillation: The read operation timed out\n",
      "Error fetching DOI for Does additional linear ablation after circumferential pulmonary vein isolation improve clinical outcome in patients with paroxysmal atrial fibrillation? Prospective randomised study: The read operation timed out\n",
      "Error fetching DOI for Tumor size predicts survival in mucinous gastric carcinoma: The read operation timed out\n",
      "Error fetching DOI for Clinical significance of changes in peripheral lymphocyte count after surgery in early cervical cancer: The read operation timed out\n",
      "Error fetching DOI for Sequential array comparative genomic hybridization analysis identifies copy number changes during blastic transformation of chronic myeloid leukemia: The read operation timed out\n",
      "Error fetching DOI for TPA-induced p21 expression augments G2/M arrest through a p53-independent mechanism in human breast cancer cells: The read operation timed out\n",
      "Processed 250 papers...\n",
      "Error fetching DOI for High-Throughput Mutation Profiling Identifies Frequent Somatic Mutations in Advanced Gastric Adenocarcinoma: The read operation timed out\n",
      "Error fetching DOI for Berberine Suppresses the TPA-Induced MMP-1 and MMP-9 Expressions Through the Inhibition of PKC-alpha in Breast Cancer Cells: The read operation timed out\n",
      "Error fetching DOI for Epidemiology of egg drop syndrome virus in ducks from South Korea: The read operation timed out\n",
      "Error fetching DOI for Genetic localization and in vivo characterization of a Monascus azaphilone pigment biosynthetic gene cluster: The read operation timed out\n",
      "Error fetching DOI for The Cyclic Peptide Ecumicin Targeting ClpC1 Is Active against Mycobacterium tuberculosis In Vivo: The read operation timed out\n",
      "Error fetching DOI for Reduced birth weight, cleft palate and preputial abnormalities in a cloned dog: The read operation timed out\n",
      "Error fetching DOI for Enhanced Hydrogen Production by Co-cultures of Hydrogenase and Nitrogenase in Escherichia coli: The read operation timed out\n",
      "Error fetching DOI for Toxicity detection using lysosomal enzymes, glycoamylase and thioredoxin fused with fluorescent protein in Saccharomyces cerevisiae: The read operation timed out\n",
      "Processed 300 papers...\n",
      "Error fetching DOI for Lysosome based toxic detection in Saccharomyces cerevisiae using novel portable fluorometer: The read operation timed out\n",
      "Error fetching DOI for Kinematic analysis of upper extremity movement during drinking in hemiplegic subjects: The read operation timed out\n",
      "Error fetching DOI for Correlation-Based Scan Matching Using Ultrasonic Sensors for EKF Localization: The read operation timed out\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (idx, row) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_doi_df\u001b[38;5;241m.\u001b[39miterrows(), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     26\u001b[0m     title \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m     doi \u001b[38;5;241m=\u001b[39m \u001b[43mget_doi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m doi  \u001b[38;5;66;03m# Update original DataFrame\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Print progress every 50 papers\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mget_doi\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_doi\u001b[39m(title):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     19\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\habanero\\crossref\\crossref.py:385\u001b[0m, in \u001b[0;36mCrossref.works\u001b[1;34m(self, ids, query, filter, offset, limit, sample, sort, order, facet, select, cursor, cursor_max, progress_bar, warn, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/works/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmailto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mua_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/works/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfacet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 385\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshould_warn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\habanero\\request_class.py:103\u001b[0m, in \u001b[0;36mRequest.do_request\u001b[1;34m(self, should_warn)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# rename query filters\u001b[39;00m\n\u001b[0;32m    101\u001b[0m payload \u001b[38;5;241m=\u001b[39m rename_query_filters(payload)\n\u001b[1;32m--> 103\u001b[0m js \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_warn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_warn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m js \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m js\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\habanero\\request_class.py:149\u001b[0m, in \u001b[0;36mRequest._req\u001b[1;34m(self, payload, should_warn)\u001b[0m\n\u001b[0;32m    147\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mhttpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_ua\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmailto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mua_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_api.py:195\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[0;32m    175\u001b[0m     url: URL \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m     trust_env: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    Sends a `GET` request.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    on this function, as `GET` requests should not include a request body.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_api.py:109\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03mSends an HTTP request.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Client(\n\u001b[0;32m    103\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mcookies,\n\u001b[0;32m    104\u001b[0m     proxy\u001b[38;5;241m=\u001b[39mproxy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     trust_env\u001b[38;5;241m=\u001b[39mtrust_env,\n\u001b[0;32m    108\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    824\u001b[0m )\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\ssl.py:1233\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1232\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\ssl.py:1106\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from habanero import Crossref\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# Filter papers with missing DOIs\n",
    "missing_doi_df = df[df['DOI'].isna()]  # Only papers where DOI is missing\n",
    "\n",
    "# Initialize Crossref API\n",
    "cr = Crossref()\n",
    "\n",
    "# Function to fetch DOI\n",
    "def get_doi(title):\n",
    "    try:\n",
    "        result = cr.works(query=title)\n",
    "        if result and 'message' in result and 'items' in result['message'] and result['message']['items']:\n",
    "            return result['message']['items'][0].get('DOI', None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI for {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Fetch DOIs sequentially\n",
    "for i, (idx, row) in enumerate(missing_doi_df.iterrows(), start=1):\n",
    "    title = row['Title']\n",
    "    doi = get_doi(title)\n",
    "    df.at[idx, 'DOI'] = doi  # Update original DataFrame\n",
    "\n",
    "    # Print progress every 50 papers\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processed {i} papers...\")\n",
    "\n",
    "# Save updated dataset with only 'Title' and 'DOI'\n",
    "df[['Title', 'DOI']].to_csv(\"Paper_Dataset_Bio_DOI.csv\", index=False)\n",
    "\n",
    "print(\"DOI collection completed and saved to 'Paper_Dataset_Bio_DOI.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current state of the DataFrame (before interrupting the process)\n",
    "df[['Title', 'DOI']].to_csv(\"Paper_Dataset_Bio_DOI.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching DOI for Enhanced therapeutic efficacy of an adenovirus-PEI-bile-acid complex in tumors with low coxsackie and adenovirus receptor expression: The read operation timed out\n",
      "Error fetching DOI for New indoles from the roots of Brassica rapa ssp campestris: The read operation timed out\n",
      "Error fetching DOI for Flavonoid Glycosides from the Fruit of Rhus parviflora and Inhibition of Cyclin Dependent Kinases by Hyperin: The read operation timed out\n",
      "Error fetching DOI for Pancreatic Islet-Like Three-Dimensional Aggregates Derived From Human Embryonic Stem Cells Ameliorate Hyperglycemia in Streptozotocin-Induced Diabetic Mice: The read operation timed out\n",
      "DOI collection completed and saved to 'Paper_Dataset_Bio_DOI.csv'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from habanero import Crossref\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# Filter papers with missing DOIs \n",
    "missing_doi_df = df[df['DOI'].isna()]  \n",
    "\n",
    "# Initialize Crossref API\n",
    "cr = Crossref()\n",
    "\n",
    "# Function to fetch DOI\n",
    "def get_doi(title):\n",
    "    try:\n",
    "        result = cr.works(query=title)\n",
    "        if result and 'message' in result and 'items' in result['message'] and result['message']['items']:\n",
    "            return result['message']['items'][0].get('DOI', None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI for {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Fetch DOIs sequentially\n",
    "for i, (idx, row) in enumerate(missing_doi_df.iterrows(), start=1):\n",
    "    title = row['Title']\n",
    "    doi = get_doi(title)\n",
    "    df.at[idx, 'DOI'] = doi  # Update original DataFrame\n",
    "\n",
    "    # Print progress every 50 papers\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processed {i} papers...\")\n",
    "\n",
    "# Save updated dataset with only 'Title' and 'DOI'\n",
    "df[['Title', 'DOI']].to_csv(\"Paper_Dataset_Bio_DOI.csv\", index=False)\n",
    "\n",
    "print(\"DOI collection completed and saved to 'Paper_Dataset_Bio_DOI.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation using Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward_Citation\n",
      "{10.1186/s44342-024-00013-4; 2024; Shared alleles and genetic structures in different Thai domestic cat breeds: the possible influence of common racial origins}, {10.1007/s13258-024-01510-0; 2024; Optimizing Bangkaew dog breed identification using DNA technology.}, {10.1016/j.fsigen.2024.103056; 2024; Development and validation of a novel 30-plex STR assay for canine individual identification and parentage testing.}, {10.1007/s11033-019-04601-4; 2019; Polymorphism analyses of 19 STRs in Labrador Retriever population from China and its heterozygosity comparisons with other retriever breeds}, {10.1021/acs.analchem.8b05318; 2018; Forensic DNA Analysis.}, {10.1186/s13104-017-2722-6; 2017; The use of genetic markers to estimate relationships between dogs in the course of criminal investigations}\n",
      "\n",
      "Backward_Citation\n",
      "{10.1016/j.fsigen.2011.04.015; 2012; Genetic data from 15 STR loci for forensic individual identification and parentage analyses in UK domestic dogs (Canis lupus familiaris).}, {10.1016/J.FSIGSS.2009.08.068; 2009; Genetic diversity analysis of 10 STR's loci used for forensic identification in canine hair samples}, {10.1111/j.1556-4029.2009.01080.x; 2009; Canine Population Data Generated from a Multiplex STR Kit for Use in Forensic Casework *}, {10.3325/CMJ.2009.50.268; 2009; Developmental validation of short tandem repeat reagent kit for forensic DNA profiling of canine biological material.}, {10.1534/genetics.107.084954; 2008; Population Structure and Inbreeding From Pedigree Analysis of Purebred Dogs}, {10.1111/j.1556-4029.2006.00046.x; 2006; Genetics and Genomics of Core Short Tandem Repeat Loci Used in Human Identity Testing}, {10.1111/J.1471-8286.2005.01155.X; 2006; genalex 6: genetic analysis in Excel. Population genetic software for teaching and research}, {None; 2005; Forensic DNA identification of animal-derived trace evidence: tools for linking victims and suspects.}, {10.1016/J.FORSCIINT.2004.07.002; 2005; Estimating the probability of identity in a random dog population using 15 highly polymorphic canine STR markers.}, {10.1520/JFS2004207; 2005; A PCR multiplex and database for forensic DNA identification of dogs.}, {10.1046/J.1365-2052.2003.01074.X; 2004; Power of exclusion for parentage verification and probability of match for identity in American Kennel Club breeds using 17 canine microsatellite markers.}, {10.1046/j.1365-294X.2001.01185.x; 2001; Estimating the probability of identity among genotypes in natural populations: cautions and guidelines}, {10.1111/J.1748-5827.1995.TB02791.X; 1995; Dog parentage testing using canine microsatellites.}, {10.1016/0379-0738(94)90222-4; 1994; DNA profile match probability calculation: how to allow for population stratification, relatedness, database selection and single bands.}, {10.1016/j.fsigen.2013.07.002; 2014; Validation of two canine STR multiplex-assays following the ISFG recommendations for non-human DNA analysis.}, {10.1016/j.fsigen.2012.07.001; 2013; Developmental validation of DogFiler, a novel multiplex for canine DNA profiling in forensic casework.}, {10.1016/j.fsigen.2010.03.013; 2011; Population genetic study in Hungarian canine populations using forensically informative STR loci.}, {10.1007/BF01225493; 2005; The validation of short tandem repeat (STR) loci for use in forensic casework}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching data\")\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    def extract_info(citation_list):\n",
    "        extracted = []\n",
    "        for paper in citation_list:\n",
    "            if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                extracted.append(f\"{{{paper['doi']}; {paper['year']}; {paper['title']}}}\")\n",
    "        return extracted\n",
    "    \n",
    "    forward_citations = extract_info(data.get(\"citations\", []))\n",
    "    backward_references = extract_info(data.get(\"references\", []))\n",
    "    \n",
    "    print(\"Forward_Citation\")\n",
    "    print(\", \".join(forward_citations) if forward_citations else \"No Forward Citations Found\")\n",
    "    \n",
    "    print(\"\\nBackward_Citation\")\n",
    "    print(\", \".join(backward_references) if backward_references else \"No Backward Citations Found\")\n",
    "    \n",
    "# Example usage\n",
    "doi = \"10.4142/jvs.2016.17.3.391\"\n",
    "get_citation_info(doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Paper_Doi  Publish_Year Citation_Type  \\\n",
      "0   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "1   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "2   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "3   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "4   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "5   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "6   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "7   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "8   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "9   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "10  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "11  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "12  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "13  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "14  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "15  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "16  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "17  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "18  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "19  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "20  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "21  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "22  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "23  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "\n",
      "                             Cited_Doi  Year  \\\n",
      "0           10.1186/s44342-024-00013-4  2024   \n",
      "1           10.1007/s13258-024-01510-0  2024   \n",
      "2         10.1016/j.fsigen.2024.103056  2024   \n",
      "3           10.1007/s11033-019-04601-4  2019   \n",
      "4         10.1021/acs.analchem.8b05318  2018   \n",
      "5            10.1186/s13104-017-2722-6  2017   \n",
      "6         10.1016/j.fsigen.2011.04.015  2012   \n",
      "7         10.1016/J.FSIGSS.2009.08.068  2009   \n",
      "8     10.1111/j.1556-4029.2009.01080.x  2009   \n",
      "9              10.3325/CMJ.2009.50.268  2009   \n",
      "10         10.1534/genetics.107.084954  2008   \n",
      "11    10.1111/j.1556-4029.2006.00046.x  2006   \n",
      "12    10.1111/J.1471-8286.2005.01155.X  2006   \n",
      "13                                None  2005   \n",
      "14     10.1016/J.FORSCIINT.2004.07.002  2005   \n",
      "15                  10.1520/JFS2004207  2005   \n",
      "16    10.1046/J.1365-2052.2003.01074.X  2004   \n",
      "17    10.1046/j.1365-294X.2001.01185.x  2001   \n",
      "18  10.1111/J.1748-5827.1995.TB02791.X  1995   \n",
      "19        10.1016/0379-0738(94)90222-4  1994   \n",
      "20        10.1016/j.fsigen.2013.07.002  2014   \n",
      "21        10.1016/j.fsigen.2012.07.001  2013   \n",
      "22        10.1016/j.fsigen.2010.03.013  2011   \n",
      "23                  10.1007/BF01225493  2005   \n",
      "\n",
      "                                                Title  \n",
      "0   Shared alleles and genetic structures in diffe...  \n",
      "1   Optimizing Bangkaew dog breed identification u...  \n",
      "2   Development and validation of a novel 30-plex ...  \n",
      "3   Polymorphism analyses of 19 STRs in Labrador R...  \n",
      "4                              Forensic DNA Analysis.  \n",
      "5   The use of genetic markers to estimate relatio...  \n",
      "6   Genetic data from 15 STR loci for forensic ind...  \n",
      "7   Genetic diversity analysis of 10 STR's loci us...  \n",
      "8   Canine Population Data Generated from a Multip...  \n",
      "9   Developmental validation of short tandem repea...  \n",
      "10  Population Structure and Inbreeding From Pedig...  \n",
      "11  Genetics and Genomics of Core Short Tandem Rep...  \n",
      "12  genalex 6: genetic analysis in Excel. Populati...  \n",
      "13  Forensic DNA identification of animal-derived ...  \n",
      "14  Estimating the probability of identity in a ra...  \n",
      "15  A PCR multiplex and database for forensic DNA ...  \n",
      "16  Power of exclusion for parentage verification ...  \n",
      "17  Estimating the probability of identity among g...  \n",
      "18  Dog parentage testing using canine microsatell...  \n",
      "19  DNA profile match probability calculation: how...  \n",
      "20  Validation of two canine STR multiplex-assays ...  \n",
      "21  Developmental validation of DogFiler, a novel ...  \n",
      "22  Population genetic study in Hungarian canine p...  \n",
      "23  The validation of short tandem repeat (STR) lo...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data for DOI: {doi}\")\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "    \n",
    "    citation_data = []\n",
    "    \n",
    "    def extract_info(citation_list, citation_type):\n",
    "        for paper in citation_list:\n",
    "            if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                citation_data.append({\n",
    "                    \"Paper_Doi\": doi,\n",
    "                    \"Publish_Year\": paper_year,\n",
    "                    \"Citation_Type\": citation_type,\n",
    "                    \"Cited_Doi\": paper[\"doi\"],\n",
    "                    \"Year\": paper[\"year\"],\n",
    "                    \"Title\": paper[\"title\"]\n",
    "                })\n",
    "    \n",
    "    # Forward citations (이 논문을 인용한 논문들)\n",
    "    extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "    \n",
    "    # Backward citations (이 논문이 참고한 논문들)\n",
    "    extract_info(data.get(\"references\", []), \"Backward\")\n",
    "    \n",
    "    return pd.DataFrame(citation_data)\n",
    "\n",
    "# Example usage\n",
    "doi = \"10.4142/jvs.2016.17.3.391\"\n",
    "df = get_citation_info(doi)\n",
    "\n",
    "# 결과 출력\n",
    "if df is not None and not df.empty:\n",
    "    print(df)\n",
    "    # CSV로 저장 (선택)\n",
    "    df.to_csv(\"citation_data.csv\", index=False)\n",
    "else:\n",
    "    print(\"No citations found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data for DOI: {doi}\")\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "    \n",
    "    citation_data = []\n",
    "    \n",
    "    def extract_info(citation_list, citation_type):\n",
    "        for paper in citation_list:\n",
    "            if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                citation_data.append({\n",
    "                    \"Paper_Doi\": doi,\n",
    "                    \"Publish_Year\": paper_year,\n",
    "                    \"Citation_Type\": citation_type,\n",
    "                    \"Cited_Doi\": paper[\"doi\"],\n",
    "                    \"Year\": paper[\"year\"],\n",
    "                    \"Title\": paper[\"title\"]\n",
    "                })\n",
    "    \n",
    "    # Forward citations (이 논문을 인용한 논문들)\n",
    "    extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "    \n",
    "    # Backward citations (이 논문이 참고한 논문들)\n",
    "    extract_info(data.get(\"references\", []), \"Backward\")\n",
    "    \n",
    "    return citation_data\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"citation_data.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "\n",
    "# 결과 저장용 리스트\n",
    "all_citations = []\n",
    "\n",
    "# DOI 기준으로 데이터 수집\n",
    "for idx, doi in enumerate(df[\"DOI\"].dropna().unique()):  # 중복 제거 후 수집\n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    \n",
    "    # 50개마다 진행 상황 출력\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1} papers...\")\n",
    "\n",
    "# DataFrame으로 변환 후 CSV 저장\n",
    "citation_df = pd.DataFrame(all_citations)\n",
    "citation_df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 404 for DOI: 10.1021/ac0618730.s002\n",
      "Error 404 for DOI: 10.1021/acs.langmuir.0c00729.s001\n",
      "Error 404 for DOI: 10.1039/c2jm16448e\n",
      "Error 404 for DOI: 10.4141/cjps2011-165\n",
      "Error 404 for DOI: 10.1002/ange.201204989\n",
      "Error 404 for DOI: 10.1093/infdis/jir731\n",
      "Error 404 for DOI: 10.1016/j.imr.2015.04.053\n",
      "Error 404 for DOI: 10.20944/preprints202307.1316.v1\n",
      "Error 404 for DOI: 10.26226/morressier.578f37f9d462b8028d88f59d\n",
      "Error 404 for DOI: 10.1201/9781003220329-20\n",
      "Error 404 for DOI: 10.1021/acsanm.0c00474.s001\n",
      "Error 404 for DOI: 10.1002/ange.201106758\n",
      "Error 404 for DOI: 10.1161/blog.20200612.193056\n",
      "Error 404 for DOI: 10.1016/j.ygyno.2013.04.368\n",
      "Error 404 for DOI: 10.26226/morressier.599bdc78d462b80296ca0b33\n",
      "Error 404 for DOI: 10.1093/jxb/erw002\n",
      "Processed 350/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.langmuir.5b03945.s001\n",
      "Processed 400/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201302881\n",
      "Processed 450/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201108977\n",
      "Error 404 for DOI: 10.1021/ac061058k.s001\n",
      "Error 404 for DOI: 10.32657/10356/72574\n",
      "Processed 500/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201207129\n",
      "Error 404 for DOI: 10.1021/ja0672167.s001\n",
      "Error 404 for DOI: 10.1002/ange.201105986\n",
      "Error 404 for DOI: 10.4324/9781003077343-15\n",
      "Error 404 for DOI: 10.26226/morressier.5acc8ad3d462b8028d89b4e3\n",
      "Processed 550/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.biomaterials.2020.120412\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)37280-x\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36400-0\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36405-x\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)35665-9\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)35194-2\n",
      "Processed 600/6243 papers...\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36406-1\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36103-2\n",
      "Error 404 for DOI: 10.1021/acs.biomac.6b00098.s001\n",
      "Error 404 for DOI: 10.1021/acsbiomaterials.9b01790.s001\n",
      "Processed 650/6243 papers...\n",
      "Error 404 for DOI: 10.5772/30646\n",
      "Processed 700/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acssuschemeng.6b00014.s001\n",
      "Processed 750/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.jnatprod.3c00143.s001\n",
      "Error 404 for DOI: 10.1002/chin.201229206\n",
      "Error 404 for DOI: 10.1097/00007890-201211271-00861\n",
      "Processed 800/6243 papers...\n",
      "Error 404 for DOI: 10.5109/25207\n",
      "Processed 850/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsomega.6b00495.s001\n",
      "Error 404 for DOI: 10.1021/acs.molpharmaceut.1c00782.s001\n",
      "Processed 900/6243 papers...\n",
      "Error 404 for DOI: 10.59350/snhcs-hm281\n",
      "Error 404 for DOI: 10.5423/ppj.si.11.2012.0173\n",
      "Error 404 for DOI: 10.3410/f.718046737.793486577\n",
      "Error 404 for DOI: 10.1109/nssmic.2010.5874130\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)35671-4\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36404-8\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)34969-3\n",
      "Processed 950/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acschemneuro.6b00082.s001\n",
      "Error 404 for DOI: 10.4062/biomolther.10.4062/biomolther.2014.051\n",
      "Error 404 for DOI: 10.1002/chin.201343209\n",
      "Processed 1000/6243 papers...\n",
      "Processed 1050/6243 papers...\n",
      "Processed 1100/6243 papers...\n",
      "Error 404 for DOI: 10.1021/jo0612978.s001\n",
      "Processed 1150/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2254151.5361903405001\n",
      "Processed 1200/6243 papers...\n",
      "Processed 1250/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.nmd.2016.06.430\n",
      "Processed 1300/6243 papers...\n",
      "Processed 1350/6243 papers...\n",
      "Error 404 for DOI: 10.1504/ijdmb.2016.079803\n",
      "Processed 1400/6243 papers...\n",
      "Error 404 for DOI: 10.17817/2018.11.07.111345\n",
      "Processed 1450/6243 papers...\n",
      "Error 404 for DOI: 10.1016/0031-9422(82)85074-7\n",
      "Error 404 for DOI: 10.1016/b978-0-323-99144-5.00022-6\n",
      "Error 404 for DOI: 10.1038/ncomms13637\n",
      "Processed 1500/6243 papers...\n",
      "Error 404 for DOI: 10.1016/0169-5002(94)94354-0\n",
      "Error 404 for DOI: 10.1097/01.ede.0000392153.88998.b9\n",
      "Error 404 for DOI: 10.1364/ecbo.2009.7372_21\n",
      "Error 404 for DOI: 10.18692/1810-4800-2018-2-66-7\n",
      "Processed 1550/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsami.6b12618.s001\n",
      "Processed 1600/6243 papers...\n",
      "Processed 1650/6243 papers...\n",
      "Error 404 for DOI: 10.36076/ppj/2016.19.e787\n",
      "Error 404 for DOI: 10.1021/acs.jafc.6b03588.s001\n",
      "Error 404 for DOI: 10.1021/acs.analchem.6b03255.s001\n",
      "Error 404 for DOI: 10.1021/acs.jafc.0c03255.s001\n",
      "Error 404 for DOI: 10.1093/eurjpc/zwad301\n",
      "Processed 1700/6243 papers...\n",
      "Error 404 for DOI: 10.37473/dac/10.1038/s41598-023-34151-6\n",
      "Error 404 for DOI: 10.1021/jacs.7b00390.s001\n",
      "Processed 1750/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.langmuir.0c00893.s001\n",
      "Processed 1800/6243 papers...\n",
      "Processed 1850/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2573886.6195484922001\n",
      "Processed 1900/6243 papers...\n",
      "Processed 1950/6243 papers...\n",
      "Error 404 for DOI: 10.2196/12070\n",
      "Error 404 for DOI: 10.18297/etd/3639\n",
      "Processed 2000/6243 papers...\n",
      "Processed 2050/6243 papers...\n",
      "Error 404 for DOI: 10.1101/gr.231837.117\n",
      "Processed 2100/6243 papers...\n",
      "Error 404 for DOI: 10.3389/fmicb.2018.02414\n",
      "Error 404 for DOI: 10.3389/fmicb.2018.0069\n",
      "Processed 2150/6243 papers...\n",
      "Processed 2200/6243 papers...\n",
      "Processed 2250/6243 papers...\n",
      "Processed 2300/6243 papers...\n",
      "Error 404 for DOI: 10.1371/joumal.pone.0207373\n",
      "Processed 2350/6243 papers...\n",
      "Processed 2400/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.jatc.9b04304\n",
      "Error 404 for DOI: 10.1016/j.toxlet.2017.07.124\n",
      "Error 404 for DOI: 10.1016/j.ymthe.2019.05.013\n",
      "Processed 2450/6243 papers...\n",
      "Processed 2500/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.clinbiochem.2019.06.003\n",
      "Processed 2550/6243 papers...\n",
      "Processed 2600/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.jocn.2020.09.036\n",
      "Processed 2650/6243 papers...\n",
      "Error 404 for DOI: 10.1038/s41598-019-52399-9\n",
      "Processed 2700/6243 papers...\n",
      "Error 404 for DOI: 10.31525/cmr-14804dc\n",
      "Processed 2750/6243 papers...\n",
      "Processed 2800/6243 papers...\n",
      "Processed 2850/6243 papers...\n",
      "Processed 2900/6243 papers...\n",
      "Processed 2950/6243 papers...\n",
      "Processed 3000/6243 papers...\n",
      "Error 404 for DOI: 10.1007/springerreference_67742\n",
      "Error 404 for DOI: 10.1002/ange.201301646\n",
      "Processed 3050/6243 papers...\n",
      "Error 404 for DOI: 10.3390/ijms140917986\n",
      "Error 404 for DOI: 10.21037/tcr.2018.01.09\n",
      "Error 404 for DOI: 10.1021/acschembio.0c00032.s001\n",
      "Error 404 for DOI: 10.3410/f.718329967.793502313\n",
      "Processed 3100/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.jcyt.2014.01.302\n",
      "Error 404 for DOI: 10.31525/cmr-9739bb\n",
      "Error 404 for DOI: 10.14341/dm9776-4140\n",
      "Error 404 for DOI: 10.1161/STROKEAHA.116.015428\n",
      "Processed 3150/6243 papers...\n",
      "Error 404 for DOI: 10.26226/morressier.58e389b4d462b80292384937\n",
      "Error 404 for DOI: 10.3410/f.725829912.793529969\n",
      "Error 404 for DOI: 10.1016/s0167-8140(04)82477-0\n",
      "Processed 3200/6243 papers...\n",
      "Error 404 for DOI: 10.1016/s0923-7534(20)33189-6\n",
      "Error 404 for DOI: 10.1002/ange.201600209\n",
      "Error 404 for DOI: 10.1117/12.2288492.5751463109001\n",
      "Processed 3250/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2214882.4848635731001\n",
      "Error 404 for DOI: 10.1021/acs.biomac.5b01756.s001\n",
      "Error 404 for DOI: 10.21070/ups.5219\n",
      "Error 404 for DOI: 10.1201/9780849351068-23\n",
      "Processed 3300/6243 papers...\n",
      "Error 404 for DOI: 10.26226/morressier.59a6b345d462b80290b547c2\n",
      "Processed 3350/6243 papers...\n",
      "Error 404 for DOI: 10.4048/jbc.2017.203.270\n",
      "Processed 3400/6243 papers...\n",
      "Processed 3450/6243 papers...\n",
      "Processed 3500/6243 papers...\n",
      "Processed 3550/6243 papers...\n",
      "Processed 3600/6243 papers...\n",
      "Processed 3650/6243 papers...\n",
      "Error 404 for DOI: 10.2174/9789815136081123010003\n",
      "Processed 3700/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201210365\n",
      "Error 404 for DOI: 10.1002/ange.201209187\n",
      "Error 404 for DOI: 10.1021/acs.jnatprod.5b01071.s001\n",
      "Processed 3750/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsami.6b08826.s001\n",
      "Error 404 for DOI: 10.1002/ange.201484961\n",
      "Error 404 for DOI: 10.1021/acs.biomac.3c01025.s001\n",
      "Processed 3800/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201501748\n",
      "Error 404 for DOI: 10.1021/acs.biomac.5b01622.s001\n",
      "Error 404 for DOI: 10.1021/acsnano.5b07787.s001\n",
      "Error 404 for DOI: 10.1021/acscatal.6b01884.s001\n",
      "Error 404 for DOI: 10.1021/acs.chemmater.6b03676.s001\n",
      "Error 404 for DOI: 10.1002/ange.201510319\n",
      "Error 404 for DOI: 10.1021/acs.macromol.2c01205.s001\n",
      "Processed 3850/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsabm.0c00614.s001\n",
      "Error 404 for DOI: 10.1186/s13068-017-07072\n",
      "Processed 3900/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.intimp.2018.03,034\n",
      "Processed 3950/6243 papers...\n",
      "Error 404 for DOI: 10.1002/bit.27175\n",
      "Processed 4000/6243 papers...\n",
      "Processed 4050/6243 papers...\n",
      "Error 404 for DOI: 10.3965/j.ijabe.20171001.2113\n",
      "Error 404 for DOI: 10.7554/elife.01911.015\n",
      "Error 404 for DOI: 10.4141/cjss2012-101\n",
      "Processed 4100/6243 papers...\n",
      "Error 404 for DOI: 10.15210//sufix\n",
      "Error 404 for DOI: 10.21203/rs.3.rs-4845823/v1\n",
      "Error 404 for DOI: 10.1016/0016-5085(95)27405-7\n",
      "Processed 4150/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.jnatprod.6b00267.s001\n",
      "Processed 4200/6243 papers...\n",
      "Processed 4250/6243 papers...\n",
      "Error 404 for DOI: 10.5851/kosfa.2018.e10\n",
      "Error 404 for DOI: 10.5851/kosfa.2018.38.2.338\n",
      "Processed 4300/6243 papers...\n",
      "Processed 4350/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.compag.2019.105174\n",
      "Processed 4400/6243 papers...\n",
      "Error 404 for DOI: 10.17179/excli2020-2162\n",
      "Processed 4450/6243 papers...\n",
      "Processed 4500/6243 papers...\n",
      "Error 404 for DOI: 10.3410/f.718187642.793488801\n",
      "Processed 4550/6243 papers...\n",
      "Error 404 for DOI: 10.3410/f.718475804.793499467\n",
      "Processed 4600/6243 papers...\n",
      "Error 404 for DOI: 10.1007/springerreference_174316\n",
      "Error 404 for DOI: 10.1038/srep07467\n",
      "Error 404 for DOI: 10.7287/peerj.13038v0.1/reviews/3\n",
      "Processed 4650/6243 papers...\n",
      "Error 404 for DOI: 10.3410/f.718520452.793498333\n",
      "Processed 4700/6243 papers...\n",
      "Processed 4750/6243 papers...\n",
      "Processed 4800/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsnano.6b02592.s001\n",
      "Error 404 for DOI: 10.1364/pibm.2017.w3a.77\n",
      "Processed 4850/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.amjcard.2009.08.520\n",
      "Error 404 for DOI: 10.3410/f.726927815.793525980\n",
      "Error 404 for DOI: 10.1021/acs.bioconjchem.6b00305.s001\n",
      "Processed 4900/6243 papers...\n",
      "Error 404 for DOI: 10.1016/0169-5002(90)90327-i\n",
      "Error 404 for DOI: 10.3791/22380\n",
      "Processed 4950/6243 papers...\n",
      "Error 404 for DOI: 10.18632/ncotarget.20329\n",
      "Error 404 for DOI: 10.1021/acscombsci.6b00127.s001\n",
      "Error 404 for DOI: 10.1021/acs.orglett.6b00115.s001\n",
      "Error 404 for DOI: 10.1021/acscombsci.6b00071.s001\n",
      "Processed 5000/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.bpj.2015.11.1314\n",
      "Processed 5050/6243 papers...\n",
      "Processed 5100/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2549356.6142075687001\n",
      "Processed 5150/6243 papers...\n",
      "Processed 5200/6243 papers...\n",
      "Error 404 for DOI: 10.3938/jkps.71.593\n",
      "Processed 5250/6243 papers...\n",
      "Error 404 for DOI: 10.1371/journal.pone.0180418\n",
      "Processed 5300/6243 papers...\n",
      "Error 404 for DOI: 10.3892/o1.2017.6169\n",
      "Processed 5350/6243 papers...\n",
      "Processed 5400/6243 papers...\n",
      "Processed 5450/6243 papers...\n",
      "Processed 5500/6243 papers...\n",
      "Processed 5550/6243 papers...\n",
      "Processed 5600/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.molcel.2018.05.017\n",
      "Processed 5650/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.est.9b03231.s001\n",
      "Processed 5700/6243 papers...\n",
      "Processed 5750/6243 papers...\n",
      "Processed 5800/6243 papers...\n",
      "Processed 5850/6243 papers...\n",
      "Processed 5900/6243 papers...\n",
      "Processed 5950/6243 papers...\n",
      "Error 404 for DOI: 10.1002/mp.14147\n",
      "Error 404 for DOI: 10.1371/journal.pone.0238290\n",
      "Processed 6000/6243 papers...\n",
      "Error 404 for DOI: 10.1097/MD.0000000000021217\n",
      "Processed 6050/6243 papers...\n",
      "Error 404 for DOI: 10.1109/TMI.2020.3007520\n",
      "Error 404 for DOI: 10.2196/20891\n",
      "Processed 6100/6243 papers...\n",
      "Processed 6150/6243 papers...\n",
      "Processed 6200/6243 papers...\n",
      "✅ Citation data collection complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Delay 추가\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# API 요청 함수\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Publish_Year\": paper_year,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:  # Too Many Requests\n",
    "            print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 대기 시간 증가 (지수적 백오프)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for DOI: {doi}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Failed to fetch data for DOI: {doi} after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"citation_data.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "# 기존 데이터 로드 (이미 수집된 DOI 중복 방지)\n",
    "try:\n",
    "    existing_data = pd.read_csv(output_file, encoding=\"utf-8\")\n",
    "    collected_dois = set(existing_data[\"Paper_Doi\"].unique())\n",
    "except FileNotFoundError:\n",
    "    collected_dois = set()\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(df[\"DOI\"].dropna().unique())\n",
    "\n",
    "for idx, doi in enumerate(df[\"DOI\"].dropna().unique()):\n",
    "    if doi in collected_dois:\n",
    "        continue  # 이미 수집된 DOI는 건너뛰기\n",
    "    \n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 논문 DOI를 이용해 인용 정보 가져오기\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if \"doi\" in paper and \"year\" in paper and \"title\" in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 지수적 백오프 적용\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"Citation_Dataset_Bio.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8-sig\").head(100)\n",
    "dois = df[\"DOI\"].dropna().unique()\n",
    "\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(dois)\n",
    "for idx, doi in enumerate(dois, 1):\n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed {idx}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # API 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI: 10.3109/00016489.2012.720031\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_paper_doi(title, max_retries=5):\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\n",
    "        \"query\": title,\n",
    "        \"fields\": \"externalIds,title\"\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "                for paper in data[\"data\"]:\n",
    "                    if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n",
    "                        return paper[\"externalIds\"][\"DOI\"]\n",
    "            return \"DOI not found\"\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded. Retrying in {2 ** attempt} seconds...\")\n",
    "            time.sleep(2 ** attempt)  # 지수적 백오프 (1s, 2s, 4s, ...)\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    \n",
    "    return \"Failed after multiple retries\"\n",
    "\n",
    "# 예제 논문 제목\n",
    "title = \"The auditory and speech performance of children with intellectual disability after cochlear implantation\"\n",
    "doi = get_paper_doi(title)\n",
    "print(\"DOI:\", doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'error_log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m doi_to_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]))  \u001b[38;5;66;03m# DOI -> Title 매핑\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. error_log.txt에서 잘못된 DOI 목록 로드\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror_log_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     18\u001b[0m     error_dois \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mreadlines() \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Semantic Scholar API에서 논문 제목으로 새로운 DOI 찾기\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'error_log.txt'"
     ]
    }
   ],
   "source": [
    "#1차\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# 파일 경로 설정\n",
    "paper_dataset_path = \"Paper_Dataset_Bio.csv\"\n",
    "error_log_path = \"error_log.txt\"\n",
    "output_path = \"Updated_DOI_Dataset.csv\"\n",
    "\n",
    "# 1. Paper_Dataset_Bio.csv에서 DOI와 Title 로드\n",
    "df = pd.read_csv(paper_dataset_path, dtype=str)  # 문자열로 읽기\n",
    "doi_to_title = dict(zip(df[\"DOI\"], df[\"Title\"]))  # DOI -> Title 매핑\n",
    "\n",
    "# 2. error_log.txt에서 잘못된 DOI 목록 로드\n",
    "with open(error_log_path, \"r\") as file:\n",
    "    error_dois = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "# Semantic Scholar API에서 논문 제목으로 새로운 DOI 찾기\n",
    "def get_paper_doi(title, max_retries=5):\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": title, \"fields\": \"externalIds,title\"}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "                for paper in data[\"data\"]:\n",
    "                    if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n",
    "                        return paper[\"externalIds\"][\"DOI\"]\n",
    "            return \"DOI not found\"\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded. Retrying in {2 ** attempt} seconds...\")\n",
    "            time.sleep(2 ** attempt)  # 지수적 백오프 (1s, 2s, 4s, ...)\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "    return \"Failed after multiple retries\"\n",
    "\n",
    "# 새로운 DOI 검색 및 저장\n",
    "updated_data = []\n",
    "\n",
    "for doi in error_dois:\n",
    "    title = doi_to_title.get(doi, \"Title not found\")\n",
    "    if title == \"Title not found\":\n",
    "        print(f\"Skipping DOI {doi} - No matching title found in dataset.\")\n",
    "        continue\n",
    "\n",
    "    new_doi = get_paper_doi(title)\n",
    "    updated_data.append({\"Title\": title, \"Original_DOI\": doi, \"New_DOI\": new_doi})\n",
    "    print(f\"Updated DOI for '{title}': {new_doi}\")\n",
    "\n",
    "# 새로운 데이터프레임 생성 및 저장\n",
    "updated_df = pd.DataFrame(updated_data, encoding=\"utf-8\")\n",
    "updated_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Updated DOI dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "Rate limit exceeded. Retrying in 2 seconds...\n",
      "Rate limit exceeded. Retrying in 4 seconds...\n",
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "Rate limit exceeded. Retrying in 2 seconds...\n",
      "Rate limit exceeded. Retrying in 4 seconds...\n",
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "DOI 업데이트 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# 'New_DOI'가 \"Failed after multiple retries\"인 경우 필터링\n",
    "retry_df = updated_df[updated_df[\"New_DOI\"] == \"Failed after multiple retries\"].copy()\n",
    "\n",
    "# Semantic Scholar API에서 논문 제목으로 새로운 DOI 찾기\n",
    "def get_paper_doi(title, max_retries=5):\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": title, \"fields\": \"externalIds,title\"}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "                for paper in data[\"data\"]:\n",
    "                    if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n",
    "                        return paper[\"externalIds\"][\"DOI\"]\n",
    "            return \"DOI not found\"\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded. Retrying in {2 ** attempt} seconds...\")\n",
    "            time.sleep(2 ** attempt)  # 지수적 백오프 (1s, 2s, 4s, ...)\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "    return \"Failed after multiple retries\"\n",
    "\n",
    "# 새로운 DOI 검색 및 업데이트\n",
    "retry_df[\"New_DOI\"] = retry_df[\"Title\"].apply(get_paper_doi)\n",
    "\n",
    "# 원본 updated_df에 반영\n",
    "updated_df.loc[updated_df[\"New_DOI\"] == \"Failed after multiple retries\", \"New_DOI\"] = retry_df[\"New_DOI\"]\n",
    "\n",
    "print(\"DOI 업데이트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load Updated_DOI_Dataset.csv\n",
    "updated_df = pd.read_csv(\"Updated_DOI_Dataset.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# 2. Load Paper_Dataset_Bio.csv\n",
    "paper_df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# 3. Merge the two dataframes on the 'Title' column to match the records\n",
    "merged_df = pd.merge(paper_df, updated_df[['Title', 'New_DOI']], on='Title', how='left')\n",
    "\n",
    "# 4. Update the DOI column with the New_DOI from the merged data\n",
    "merged_df['DOI'] = merged_df['New_DOI'].combine_first(merged_df['DOI'])\n",
    "\n",
    "# 5. Drop the 'New_DOI' column as it's no longer needed\n",
    "merged_df.drop(columns=['New_DOI'], inplace=True)\n",
    "\n",
    "# 6. Save the updated Paper_Dataset_Bio.csv\n",
    "merged_df.to_csv(\"Updated_Paper_Dataset_Bio.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 404 for DOI: 10.1039/c2jm16448e\n",
      "Error 404 for DOI: 10.1093/infdis/jir731\n",
      "Error 404 for DOI: 10.1093/jxb/erw002\n",
      "Error 404 for DOI: 10.1016/j.biomaterials.2020.120412\n",
      "Processed 50/407 papers...\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u2018' in position 94: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_papers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_citations\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollected_dois\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     all_citations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     81\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 요청 간격 조정\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 324\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mwriters.pyx:73\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u2018' in position 94: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# API 요청 함수\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Publish_Year\": paper_year,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:  # Too Many Requests\n",
    "            print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 대기 시간 증가 (지수적 백오프)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for DOI: {doi}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Failed to fetch data for DOI: {doi} after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Updated_DOI_Dataset.csv\"\n",
    "output_file = \"citation_data.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "# 기존 데이터 로드 (이미 수집된 DOI 중복 방지)\n",
    "try:\n",
    "    existing_data = pd.read_csv(output_file, encoding=\"utf-8-sig\")\n",
    "    collected_dois = set(existing_data[\"Paper_Doi\"].unique())\n",
    "except FileNotFoundError:\n",
    "    collected_dois = set()\n",
    "\n",
    "# Updated_DOI_Dataset 로드\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8-sig\")\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(df[\"DOI\"].dropna().unique())\n",
    "\n",
    "for idx, doi in enumerate(df[\"DOI\"].dropna().unique()):\n",
    "    if doi in collected_dois:\n",
    "        continue  # 이미 수집된 DOI는 건너뛰기\n",
    "    \n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8-sig\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 papers...\n",
      "Processed 100/100 papers...\n",
      "✅ Citation data collection complete!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 논문 DOI를 이용해 인용 정보 가져오기\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if \"doi\" in paper and \"year\" in paper and \"title\" in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 지수적 백오프 적용\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"Citation_Dataset_Bio.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8-sig\")\n",
    "dois = df[\"DOI\"].dropna().unique()\n",
    "\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(dois)\n",
    "for idx, doi in enumerate(dois, 1):\n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed {idx}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # API 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
