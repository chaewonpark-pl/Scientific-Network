{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOI Mapping with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the .dta files\n",
    "paper_230130 = pd.read_stata('3.Paper_230130.dta')\n",
    "paper_bio = pd.read_stata('Paper_Dataset_Bio.dta')\n",
    "\n",
    "# 2. Normalization function\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    return re.sub(r'[^a-zA-Z0-9]', '', text).lower()\n",
    "\n",
    "# Apply normalization\n",
    "paper_230130['Paper_norm'] = paper_230130['pa030'].apply(normalize_text)\n",
    "paper_bio['Title_norm'] = paper_bio['Title'].apply(normalize_text)\n",
    "\n",
    "# 3. Create a dictionary for quick lookup\n",
    "doi_mapping = paper_230130.set_index('Paper_norm')['pa140'].to_dict()\n",
    "\n",
    "# 4. Map DOI based on Title_norm\n",
    "paper_bio['DOI'] = paper_bio['Title_norm'].map(doi_mapping)\n",
    "\n",
    "# Save the updated dataset\n",
    "paper_bio.to_csv('Updated_doi.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch DOI using Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI for 'Population genetic study of 10 short tandem repeat loci from 600 domestic dogs in Korea': None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# Semantic Scholar API settings\n",
    "API_KEY = \"d9uN8uXwqK2FkU7VYFGJr9ECqcyAYh3Gx0QRyOy7\"\n",
    "HEADERS = {\"x-api-key\": API_KEY}\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "# Paper title to search\n",
    "title = \"Population genetic study of 10 short tandem repeat loci from 600 domestic dogs in Korea\"\n",
    "\n",
    "# Function to fetch DOI from Semantic Scholar\n",
    "def fetch_doi(title):\n",
    "    params = {\"query\": title, \"fields\": \"externalIds\", \"limit\": 1}\n",
    "    response = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"data\" in data and data[\"data\"]:\n",
    "            external_ids = data[\"data\"][0].get(\"externalIds\", {})\n",
    "            return external_ids.get(\"DOI\", None)\n",
    "    return None\n",
    "\n",
    "# Fetch DOI\n",
    "doi = fetch_doi(title)\n",
    "print(f\"DOI for '{title}': {doi}\")\n",
    "\n",
    "time.sleep(1)  # Avoid rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching DOI using Crossref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Enhanced sensitivity of CpG island search and primer design based on predicted CpG island position': '10.1016/j.fsigen.2018.02.013'}\n"
     ]
    }
   ],
   "source": [
    "from habanero import Crossref\n",
    "cr = Crossref()\n",
    "titles = [\"Enhanced sensitivity of CpG island search and primer design based on predicted CpG island position\"]  # Replace with your list of titles\n",
    "dois = {}\n",
    "for title in titles:\n",
    "    result = cr.works(query=title)\n",
    "    if result['message']['items']:\n",
    "        dois[title] = result['message']['items'][0]['DOI']\n",
    "    else:\n",
    "        dois[title] = None\n",
    "print(dois)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching DOI for A lectin-coupled, multiple reaction monitoring based quantitative analysis of human plasma glycoproteins by mass spectrometry: The read operation timed out\n",
      "DOI collection completed for first 10 missing entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from habanero import Crossref\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# Filter papers with missing DOIs\n",
    "missing_doi_df = df[df['DOI'].isna()]  # Select first 10 missing DOIs\n",
    "\n",
    "# Initialize Crossref API\n",
    "cr = Crossref()\n",
    "\n",
    "# Function to fetch DOI\n",
    "def get_doi(title):\n",
    "    try:\n",
    "        result = cr.works(query=title)\n",
    "        if result['message']['items']:\n",
    "            return result['message']['items'][0]['DOI']\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI for {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Fetch DOIs sequentially\n",
    "for idx, row in missing_doi_df.iterrows():\n",
    "    title = row['Title']\n",
    "    doi = get_doi(title)\n",
    "    df.at[idx, 'DOI'] = doi  # Update original DataFrame\n",
    "\n",
    "# Save updated dataset\n",
    "df.to_csv(\"Paper_Dataset_Bio_Updated.csv\", index=False)\n",
    "\n",
    "print(\"DOI collection completed for first 10 missing entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching DOI for Kinetic studies on the formation of various II-VI semiconductor nanocrystals and synthesis of gradient alloy quantum dots emitting in the entire visible range: The read operation timed out\n",
      "Error fetching DOI for Globoside promotes activation of ERK by interaction with the epidermal growth factor receptor: The read operation timed out\n",
      "Error fetching DOI for Scanometric analysis of DNA microarrays using DNA intercalator-conjugated gold nanoparticles: The read operation timed out\n",
      "Error fetching DOI for A label-free fluorescence immunoassay system for the sensitive detection of the mycotoxin, ochratoxin A: The read operation timed out\n",
      "Error fetching DOI for Highly Efficient Enzyme Immobilization and Stabilization within Meso-Structured Onion-Like Silica for Biodiesel Production: The read operation timed out\n",
      "Error fetching DOI for Mutational complex genotype of the hepatitis B virus X/precore regions as a novel predictive marker for hepatocellular carcinoma: The read operation timed out\n",
      "Error fetching DOI for Electrochemical oxidation of glucose at nanoporous black gold surfaces in the presence of high concentration of chloride ions and application to amperometric detection: The read operation timed out\n",
      "Error fetching DOI for Electron Paramagnetic Resonance Investigation of Different Plant Organs after Gamma Irradiation: The read operation timed out\n",
      "Error fetching DOI for Proteomic characterization of Kunitz trypsin inhibitor variants, Tia and Tib, in soybean [Glycine max (L.) Merrill]: The read operation timed out\n",
      "Error fetching DOI for Development of a Qualitative Dose Indicator for Gamma Radiation Using Lyophilized Deinococcus: The read operation timed out\n",
      "Error fetching DOI for Genome-wide transcriptome analysis of rice genes responsive to chilling stress: The read operation timed out\n",
      "Error fetching DOI for Isolation of cellulose fibers from kenaf using electron beam: The read operation timed out\n",
      "Error fetching DOI for Deinococcus daejeonensis sp nov., isolated from sludge in a sewage disposal plant: The read operation timed out\n",
      "Error fetching DOI for Anti-obesity effect of Schisandra chinensis in 3T3-L1 cells and high fat diet-induced obese rats: The read operation timed out\n",
      "Processed 50 papers...\n",
      "Error fetching DOI for Self-Fluorescence of Chemically Crosslinked MRI Nanoprobes to Enable Multimodal Imaging of Therapeutic Cells: The read operation timed out\n",
      "Error fetching DOI for Virulence and transmissibility of H1N2 influenza virus in ferrets imply the continuing threat of triple-reassortant swine viruses: The read operation timed out\n",
      "Error fetching DOI for Dual Role of Respiratory Syncytial Virus Glycoprotein Fragment as a Mucosal Immunogen and Chemotactic Adjuvant: The read operation timed out\n",
      "Error fetching DOI for Antibacterial activity of LCB01-0062, a novel oxazolidinone: The read operation timed out\n",
      "Error fetching DOI for Draft Genome Sequence of Paenisporosarcina sp Strain TG-20, a Psychrophilic Bacterium Isolated from the Basal Ice of Taylor Glacier: The read operation timed out\n",
      "Error fetching DOI for Draft Genome Sequence of Moritella dasanensis Strain ArB 0140, a Psychrophilic Bacterium Isolated from the Arctic Ocean: The read operation timed out\n",
      "Error fetching DOI for Effect of decursinol angelate on the pharmacokinetics of theophylline and its metabolites in rats: The read operation timed out\n",
      "Error fetching DOI for Anti-platelet activity of diacetylated obovatol through regulating cyclooxygenase and lipoxygenase activities: The read operation timed out\n",
      "Error fetching DOI for Anti-inflammatory effect of sinomenine by inhibition of pro-inflammatory mediators in PMA plus A23187-stimulated HMC-1 Cells: The read operation timed out\n",
      "Error fetching DOI for In Vitro and In Vivo Genotoxicity Assessment of Aristolochia manshuriensis Kom.: The read operation timed out\n",
      "Error fetching DOI for Inhibitory Effect of Arctigenin from Fructus Arctii Extract on Melanin Synthesis via Repression of Tyrosinase Expression: The read operation timed out\n",
      "Processed 100 papers...\n",
      "Error fetching DOI for Cytochrome P450-mediated herb-drug interaction potential of Galgeun-tang: The read operation timed out\n",
      "Error fetching DOI for Anti-inflammatory effect of Lycium Fruit water extract in lipopolysaccharide-stimulated RAW 264.7 macrophage cells: The read operation timed out\n",
      "Error fetching DOI for Fermentation by Lactobacillus enhances anti-inflammatory effect of Oyaksungisan on LPS-stimulated RAW 264.7 mouse macrophage cells: The read operation timed out\n",
      "Error fetching DOI for Tmem64 Modulates Calcium Signaling during RANKL-Mediated Osteoclast Differentiation: The read operation timed out\n",
      "Error fetching DOI for beta-Arrestin 2 Mediates G Protein-Coupled Receptor 43 Signals to Nuclear Factor-kappa B: The read operation timed out\n",
      "Error fetching DOI for Fluorescent peptide indicator displacement assay for monitoring interactions between RNA and RNA binding proteins: The read operation timed out\n",
      "Error fetching DOI for One-step detection of circulating tumor cells in ovarian cancer using enhanced fluorescent silica nanoparticles: The read operation timed out\n",
      "Error fetching DOI for Zinc oxide nanoparticle induced autophagic cell death and mitochondrial damage via reactive oxygen species generation: The read operation timed out\n",
      "Error fetching DOI for An efficient synthesis of LipidGreen and its derivatives via microwave assisted reaction and their live lipid imaging in zebrafish: The read operation timed out\n",
      "Error fetching DOI for Identification of potential serum biomarkers for gastric cancer by a novel computational method, multiple normal tissues corrected differential analysis: The read operation timed out\n",
      "Error fetching DOI for The radiosensitivity of endothelial cells isolated from human breast cancer and normal tissue in vitro: The read operation timed out\n",
      "Error fetching DOI for Rate of Pulmonary Metastasis Varies with Location of Rectal Cancer in the Patients Undergoing Curative Resection: The read operation timed out\n",
      "Error fetching DOI for Applicability of Histoculture Drug Response Assays in Colorectal Cancer Chemotherapy: The read operation timed out\n",
      "Error fetching DOI for Clinical implications of mucinous components correlated with microsatellite instability in patients with colorectal cancer: The read operation timed out\n",
      "Processed 150 papers...\n",
      "Error fetching DOI for Phase I study of neoadjuvant chemoradiotherapy with S-1 and oxaliplatin in patients with locally advanced gastric cancer: The read operation timed out\n",
      "Error fetching DOI for Immunocytes as a Biocarrier to Delivery Therapeutic and Imaging Contrast Agents to Tumors: The read operation timed out\n",
      "Error fetching DOI for Novel histone deacetylase inhibitor CG200745 induces clonogenic cell death by modulating acetylation of p53 in cancer cells: The read operation timed out\n",
      "Error fetching DOI for Theranostic nanoparticles based on PEGylated hyaluronic acid for the diagnosis, therapy and monitoring of colon cancer: The read operation timed out\n",
      "Error fetching DOI for Use of macrophages to deliver therapeutic and imaging contrast agents to tumors: The read operation timed out\n",
      "Error fetching DOI for Metformin kills and radiosensitizes cancer cells and preferentially kills cancer stem cells: The read operation timed out\n",
      "Error fetching DOI for Open versus robot-assisted sphincter-saving operations in rectal cancer patients: techniques and comparison of outcomes between groups of 100 matched patients: The read operation timed out\n",
      "Error fetching DOI for Comparison of Three-Year Clinical Outcomes with Nonextended Versus Extended Dual Antiplatelet Therapy Between First- and Second-Generation Drug-Eluting Stent Implantation in Patients with Acute Myocardial Infarction: Data from the Infarct Prognosis Study Registry: The read operation timed out\n",
      "Error fetching DOI for Comparison between Measured and Calculated Length of Side Branch Ostium in Coronary Bifurcation Lesions with Intravascular Ultrasound: The read operation timed out\n",
      "Error fetching DOI for Effects of Combination Therapy with Celecoxib and Doxycycline on Neointimal Hyperplasia and Inflammatory Biomarkers in Coronary Artery Disease Patients Treated with Bare Metal Stents: The read operation timed out\n",
      "Error fetching DOI for Post-shock sinus node recovery time is an independent predictor of recurrence after catheter ablation of longstanding persistent atrial fibrillation: The read operation timed out\n",
      "Error fetching DOI for Prolonged Atrial Effective Refractory Periods in Atrial Fibrillation Patients Associated with Structural Heart Disease or Sinus Node Dysfunction Compared with Lone Atrial Fibrillation: The read operation timed out\n",
      "Processed 200 papers...\n",
      "Error fetching DOI for The clinical significance of the atrial subendocardial smooth muscle layer and cardiac myofibroblasts in human atrial tissue with valvular atrial fibrillation: The read operation timed out\n",
      "Error fetching DOI for High mobility group box-1 is phosphorylated by protein kinase C zeta and secreted in colon cancer cells: The read operation timed out\n",
      "Error fetching DOI for Enhancement of MSC adhesion and therapeutic efficiency in ischemic heart using lentivirus delivery with periostin: The read operation timed out\n",
      "Error fetching DOI for Consecutive Targetable Smart Nanoprobe for Molecular Recognition of Cytoplasmic microRNA in Metastatic Breast Cancer: The read operation timed out\n",
      "Error fetching DOI for Flecainide-Associated Bradycardia-Dependent Torsade de Pointes: Another Potential Mechanism of Proarrhythmia: The read operation timed out\n",
      "Error fetching DOI for Early neurological outcomes according to CHADS2 score in stroke patients with non-valvular atrial fibrillation: The read operation timed out\n",
      "Error fetching DOI for Does additional linear ablation after circumferential pulmonary vein isolation improve clinical outcome in patients with paroxysmal atrial fibrillation? Prospective randomised study: The read operation timed out\n",
      "Error fetching DOI for Tumor size predicts survival in mucinous gastric carcinoma: The read operation timed out\n",
      "Error fetching DOI for Clinical significance of changes in peripheral lymphocyte count after surgery in early cervical cancer: The read operation timed out\n",
      "Error fetching DOI for Sequential array comparative genomic hybridization analysis identifies copy number changes during blastic transformation of chronic myeloid leukemia: The read operation timed out\n",
      "Error fetching DOI for TPA-induced p21 expression augments G2/M arrest through a p53-independent mechanism in human breast cancer cells: The read operation timed out\n",
      "Processed 250 papers...\n",
      "Error fetching DOI for High-Throughput Mutation Profiling Identifies Frequent Somatic Mutations in Advanced Gastric Adenocarcinoma: The read operation timed out\n",
      "Error fetching DOI for Berberine Suppresses the TPA-Induced MMP-1 and MMP-9 Expressions Through the Inhibition of PKC-alpha in Breast Cancer Cells: The read operation timed out\n",
      "Error fetching DOI for Epidemiology of egg drop syndrome virus in ducks from South Korea: The read operation timed out\n",
      "Error fetching DOI for Genetic localization and in vivo characterization of a Monascus azaphilone pigment biosynthetic gene cluster: The read operation timed out\n",
      "Error fetching DOI for The Cyclic Peptide Ecumicin Targeting ClpC1 Is Active against Mycobacterium tuberculosis In Vivo: The read operation timed out\n",
      "Error fetching DOI for Reduced birth weight, cleft palate and preputial abnormalities in a cloned dog: The read operation timed out\n",
      "Error fetching DOI for Enhanced Hydrogen Production by Co-cultures of Hydrogenase and Nitrogenase in Escherichia coli: The read operation timed out\n",
      "Error fetching DOI for Toxicity detection using lysosomal enzymes, glycoamylase and thioredoxin fused with fluorescent protein in Saccharomyces cerevisiae: The read operation timed out\n",
      "Processed 300 papers...\n",
      "Error fetching DOI for Lysosome based toxic detection in Saccharomyces cerevisiae using novel portable fluorometer: The read operation timed out\n",
      "Error fetching DOI for Kinematic analysis of upper extremity movement during drinking in hemiplegic subjects: The read operation timed out\n",
      "Error fetching DOI for Correlation-Based Scan Matching Using Ultrasonic Sensors for EKF Localization: The read operation timed out\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (idx, row) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_doi_df\u001b[38;5;241m.\u001b[39miterrows(), start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     26\u001b[0m     title \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m     doi \u001b[38;5;241m=\u001b[39m \u001b[43mget_doi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m doi  \u001b[38;5;66;03m# Update original DataFrame\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Print progress every 50 papers\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36mget_doi\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_doi\u001b[39m(title):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     19\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\habanero\\crossref\\crossref.py:385\u001b[0m, in \u001b[0;36mCrossref.works\u001b[1;34m(self, ids, query, filter, offset, limit, sample, sort, order, facet, select, cursor, cursor_max, progress_bar, warn, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\n\u001b[0;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/works/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmailto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mua_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/works/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfacet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcursor_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 385\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshould_warn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\habanero\\request_class.py:103\u001b[0m, in \u001b[0;36mRequest.do_request\u001b[1;34m(self, should_warn)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# rename query filters\u001b[39;00m\n\u001b[0;32m    101\u001b[0m payload \u001b[38;5;241m=\u001b[39m rename_query_filters(payload)\n\u001b[1;32m--> 103\u001b[0m js \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_req\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_warn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_warn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m js \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m js\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\habanero\\request_class.py:149\u001b[0m, in \u001b[0;36mRequest._req\u001b[1;34m(self, payload, should_warn)\u001b[0m\n\u001b[0;32m    147\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mhttpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_ua\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmailto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mua_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_api.py:195\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\n\u001b[0;32m    175\u001b[0m     url: URL \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m     trust_env: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    186\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m    Sends a `GET` request.\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    on this function, as `GET` requests should not include a request body.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_api.py:109\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03mSends an HTTP request.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Client(\n\u001b[0;32m    103\u001b[0m     cookies\u001b[38;5;241m=\u001b[39mcookies,\n\u001b[0;32m    104\u001b[0m     proxy\u001b[38;5;241m=\u001b[39mproxy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     trust_env\u001b[38;5;241m=\u001b[39mtrust_env,\n\u001b[0;32m    108\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    810\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    824\u001b[0m )\n\u001b[1;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\ssl.py:1233\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1232\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\ssl.py:1106\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from habanero import Crossref\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# Filter papers with missing DOIs\n",
    "missing_doi_df = df[df['DOI'].isna()]  # Only papers where DOI is missing\n",
    "\n",
    "# Initialize Crossref API\n",
    "cr = Crossref()\n",
    "\n",
    "# Function to fetch DOI\n",
    "def get_doi(title):\n",
    "    try:\n",
    "        result = cr.works(query=title)\n",
    "        if result and 'message' in result and 'items' in result['message'] and result['message']['items']:\n",
    "            return result['message']['items'][0].get('DOI', None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI for {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Fetch DOIs sequentially\n",
    "for i, (idx, row) in enumerate(missing_doi_df.iterrows(), start=1):\n",
    "    title = row['Title']\n",
    "    doi = get_doi(title)\n",
    "    df.at[idx, 'DOI'] = doi  # Update original DataFrame\n",
    "\n",
    "    # Print progress every 50 papers\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processed {i} papers...\")\n",
    "\n",
    "# Save updated dataset with only 'Title' and 'DOI'\n",
    "df[['Title', 'DOI']].to_csv(\"Paper_Dataset_Bio_DOI.csv\", index=False)\n",
    "\n",
    "print(\"DOI collection completed and saved to 'Paper_Dataset_Bio_DOI.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current state of the DataFrame (before interrupting the process)\n",
    "df[['Title', 'DOI']].to_csv(\"Paper_Dataset_Bio_DOI.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching DOI for Enhanced therapeutic efficacy of an adenovirus-PEI-bile-acid complex in tumors with low coxsackie and adenovirus receptor expression: The read operation timed out\n",
      "Error fetching DOI for New indoles from the roots of Brassica rapa ssp campestris: The read operation timed out\n",
      "Error fetching DOI for Flavonoid Glycosides from the Fruit of Rhus parviflora and Inhibition of Cyclin Dependent Kinases by Hyperin: The read operation timed out\n",
      "Error fetching DOI for Pancreatic Islet-Like Three-Dimensional Aggregates Derived From Human Embryonic Stem Cells Ameliorate Hyperglycemia in Streptozotocin-Induced Diabetic Mice: The read operation timed out\n",
      "DOI collection completed and saved to 'Paper_Dataset_Bio_DOI.csv'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from habanero import Crossref\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# Filter papers with missing DOIs \n",
    "missing_doi_df = df[df['DOI'].isna()]  \n",
    "\n",
    "# Initialize Crossref API\n",
    "cr = Crossref()\n",
    "\n",
    "# Function to fetch DOI\n",
    "def get_doi(title):\n",
    "    try:\n",
    "        result = cr.works(query=title)\n",
    "        if result and 'message' in result and 'items' in result['message'] and result['message']['items']:\n",
    "            return result['message']['items'][0].get('DOI', None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI for {title}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Fetch DOIs sequentially\n",
    "for i, (idx, row) in enumerate(missing_doi_df.iterrows(), start=1):\n",
    "    title = row['Title']\n",
    "    doi = get_doi(title)\n",
    "    df.at[idx, 'DOI'] = doi  # Update original DataFrame\n",
    "\n",
    "    # Print progress every 50 papers\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Processed {i} papers...\")\n",
    "\n",
    "# Save updated dataset with only 'Title' and 'DOI'\n",
    "df[['Title', 'DOI']].to_csv(\"Paper_Dataset_Bio_DOI.csv\", index=False)\n",
    "\n",
    "print(\"DOI collection completed and saved to 'Paper_Dataset_Bio_DOI.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citation using Semantic Scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward_Citation\n",
      "{10.1186/s44342-024-00013-4; 2024; Shared alleles and genetic structures in different Thai domestic cat breeds: the possible influence of common racial origins}, {10.1007/s13258-024-01510-0; 2024; Optimizing Bangkaew dog breed identification using DNA technology.}, {10.1016/j.fsigen.2024.103056; 2024; Development and validation of a novel 30-plex STR assay for canine individual identification and parentage testing.}, {10.1007/s11033-019-04601-4; 2019; Polymorphism analyses of 19 STRs in Labrador Retriever population from China and its heterozygosity comparisons with other retriever breeds}, {10.1021/acs.analchem.8b05318; 2018; Forensic DNA Analysis.}, {10.1186/s13104-017-2722-6; 2017; The use of genetic markers to estimate relationships between dogs in the course of criminal investigations}\n",
      "\n",
      "Backward_Citation\n",
      "{10.1016/j.fsigen.2011.04.015; 2012; Genetic data from 15 STR loci for forensic individual identification and parentage analyses in UK domestic dogs (Canis lupus familiaris).}, {10.1016/J.FSIGSS.2009.08.068; 2009; Genetic diversity analysis of 10 STR's loci used for forensic identification in canine hair samples}, {10.1111/j.1556-4029.2009.01080.x; 2009; Canine Population Data Generated from a Multiplex STR Kit for Use in Forensic Casework *}, {10.3325/CMJ.2009.50.268; 2009; Developmental validation of short tandem repeat reagent kit for forensic DNA profiling of canine biological material.}, {10.1534/genetics.107.084954; 2008; Population Structure and Inbreeding From Pedigree Analysis of Purebred Dogs}, {10.1111/j.1556-4029.2006.00046.x; 2006; Genetics and Genomics of Core Short Tandem Repeat Loci Used in Human Identity Testing}, {10.1111/J.1471-8286.2005.01155.X; 2006; genalex 6: genetic analysis in Excel. Population genetic software for teaching and research}, {None; 2005; Forensic DNA identification of animal-derived trace evidence: tools for linking victims and suspects.}, {10.1016/J.FORSCIINT.2004.07.002; 2005; Estimating the probability of identity in a random dog population using 15 highly polymorphic canine STR markers.}, {10.1520/JFS2004207; 2005; A PCR multiplex and database for forensic DNA identification of dogs.}, {10.1046/J.1365-2052.2003.01074.X; 2004; Power of exclusion for parentage verification and probability of match for identity in American Kennel Club breeds using 17 canine microsatellite markers.}, {10.1046/j.1365-294X.2001.01185.x; 2001; Estimating the probability of identity among genotypes in natural populations: cautions and guidelines}, {10.1111/J.1748-5827.1995.TB02791.X; 1995; Dog parentage testing using canine microsatellites.}, {10.1016/0379-0738(94)90222-4; 1994; DNA profile match probability calculation: how to allow for population stratification, relatedness, database selection and single bands.}, {10.1016/j.fsigen.2013.07.002; 2014; Validation of two canine STR multiplex-assays following the ISFG recommendations for non-human DNA analysis.}, {10.1016/j.fsigen.2012.07.001; 2013; Developmental validation of DogFiler, a novel multiplex for canine DNA profiling in forensic casework.}, {10.1016/j.fsigen.2010.03.013; 2011; Population genetic study in Hungarian canine populations using forensically informative STR loci.}, {10.1007/BF01225493; 2005; The validation of short tandem repeat (STR) loci for use in forensic casework}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching data\")\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    def extract_info(citation_list):\n",
    "        extracted = []\n",
    "        for paper in citation_list:\n",
    "            if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                extracted.append(f\"{{{paper['doi']}; {paper['year']}; {paper['title']}}}\")\n",
    "        return extracted\n",
    "    \n",
    "    forward_citations = extract_info(data.get(\"citations\", []))\n",
    "    backward_references = extract_info(data.get(\"references\", []))\n",
    "    \n",
    "    print(\"Forward_Citation\")\n",
    "    print(\", \".join(forward_citations) if forward_citations else \"No Forward Citations Found\")\n",
    "    \n",
    "    print(\"\\nBackward_Citation\")\n",
    "    print(\", \".join(backward_references) if backward_references else \"No Backward Citations Found\")\n",
    "    \n",
    "# Example usage\n",
    "doi = \"10.4142/jvs.2016.17.3.391\"\n",
    "get_citation_info(doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Paper_Doi  Publish_Year Citation_Type  \\\n",
      "0   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "1   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "2   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "3   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "4   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "5   10.4142/jvs.2016.17.3.391          2016       Forward   \n",
      "6   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "7   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "8   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "9   10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "10  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "11  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "12  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "13  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "14  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "15  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "16  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "17  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "18  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "19  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "20  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "21  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "22  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "23  10.4142/jvs.2016.17.3.391          2016      Backward   \n",
      "\n",
      "                             Cited_Doi  Year  \\\n",
      "0           10.1186/s44342-024-00013-4  2024   \n",
      "1           10.1007/s13258-024-01510-0  2024   \n",
      "2         10.1016/j.fsigen.2024.103056  2024   \n",
      "3           10.1007/s11033-019-04601-4  2019   \n",
      "4         10.1021/acs.analchem.8b05318  2018   \n",
      "5            10.1186/s13104-017-2722-6  2017   \n",
      "6         10.1016/j.fsigen.2011.04.015  2012   \n",
      "7         10.1016/J.FSIGSS.2009.08.068  2009   \n",
      "8     10.1111/j.1556-4029.2009.01080.x  2009   \n",
      "9              10.3325/CMJ.2009.50.268  2009   \n",
      "10         10.1534/genetics.107.084954  2008   \n",
      "11    10.1111/j.1556-4029.2006.00046.x  2006   \n",
      "12    10.1111/J.1471-8286.2005.01155.X  2006   \n",
      "13                                None  2005   \n",
      "14     10.1016/J.FORSCIINT.2004.07.002  2005   \n",
      "15                  10.1520/JFS2004207  2005   \n",
      "16    10.1046/J.1365-2052.2003.01074.X  2004   \n",
      "17    10.1046/j.1365-294X.2001.01185.x  2001   \n",
      "18  10.1111/J.1748-5827.1995.TB02791.X  1995   \n",
      "19        10.1016/0379-0738(94)90222-4  1994   \n",
      "20        10.1016/j.fsigen.2013.07.002  2014   \n",
      "21        10.1016/j.fsigen.2012.07.001  2013   \n",
      "22        10.1016/j.fsigen.2010.03.013  2011   \n",
      "23                  10.1007/BF01225493  2005   \n",
      "\n",
      "                                                Title  \n",
      "0   Shared alleles and genetic structures in diffe...  \n",
      "1   Optimizing Bangkaew dog breed identification u...  \n",
      "2   Development and validation of a novel 30-plex ...  \n",
      "3   Polymorphism analyses of 19 STRs in Labrador R...  \n",
      "4                              Forensic DNA Analysis.  \n",
      "5   The use of genetic markers to estimate relatio...  \n",
      "6   Genetic data from 15 STR loci for forensic ind...  \n",
      "7   Genetic diversity analysis of 10 STR's loci us...  \n",
      "8   Canine Population Data Generated from a Multip...  \n",
      "9   Developmental validation of short tandem repea...  \n",
      "10  Population Structure and Inbreeding From Pedig...  \n",
      "11  Genetics and Genomics of Core Short Tandem Rep...  \n",
      "12  genalex 6: genetic analysis in Excel. Populati...  \n",
      "13  Forensic DNA identification of animal-derived ...  \n",
      "14  Estimating the probability of identity in a ra...  \n",
      "15  A PCR multiplex and database for forensic DNA ...  \n",
      "16  Power of exclusion for parentage verification ...  \n",
      "17  Estimating the probability of identity among g...  \n",
      "18  Dog parentage testing using canine microsatell...  \n",
      "19  DNA profile match probability calculation: how...  \n",
      "20  Validation of two canine STR multiplex-assays ...  \n",
      "21  Developmental validation of DogFiler, a novel ...  \n",
      "22  Population genetic study in Hungarian canine p...  \n",
      "23  The validation of short tandem repeat (STR) lo...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data for DOI: {doi}\")\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "    \n",
    "    citation_data = []\n",
    "    \n",
    "    def extract_info(citation_list, citation_type):\n",
    "        for paper in citation_list:\n",
    "            if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                citation_data.append({\n",
    "                    \"Paper_Doi\": doi,\n",
    "                    \"Publish_Year\": paper_year,\n",
    "                    \"Citation_Type\": citation_type,\n",
    "                    \"Cited_Doi\": paper[\"doi\"],\n",
    "                    \"Year\": paper[\"year\"],\n",
    "                    \"Title\": paper[\"title\"]\n",
    "                })\n",
    "    \n",
    "    # Forward citations (이 논문을 인용한 논문들)\n",
    "    extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "    \n",
    "    # Backward citations (이 논문이 참고한 논문들)\n",
    "    extract_info(data.get(\"references\", []), \"Backward\")\n",
    "    \n",
    "    return pd.DataFrame(citation_data)\n",
    "\n",
    "# Example usage\n",
    "doi = \"10.4142/jvs.2016.17.3.391\"\n",
    "df = get_citation_info(doi)\n",
    "\n",
    "# 결과 출력\n",
    "if df is not None and not df.empty:\n",
    "    print(df)\n",
    "    # CSV로 저장 (선택)\n",
    "    df.to_csv(\"citation_data.csv\", index=False)\n",
    "else:\n",
    "    print(\"No citations found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data for DOI: {doi}\")\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "    \n",
    "    citation_data = []\n",
    "    \n",
    "    def extract_info(citation_list, citation_type):\n",
    "        for paper in citation_list:\n",
    "            if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                citation_data.append({\n",
    "                    \"Paper_Doi\": doi,\n",
    "                    \"Publish_Year\": paper_year,\n",
    "                    \"Citation_Type\": citation_type,\n",
    "                    \"Cited_Doi\": paper[\"doi\"],\n",
    "                    \"Year\": paper[\"year\"],\n",
    "                    \"Title\": paper[\"title\"]\n",
    "                })\n",
    "    \n",
    "    # Forward citations (이 논문을 인용한 논문들)\n",
    "    extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "    \n",
    "    # Backward citations (이 논문이 참고한 논문들)\n",
    "    extract_info(data.get(\"references\", []), \"Backward\")\n",
    "    \n",
    "    return citation_data\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"citation_data.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "\n",
    "# 결과 저장용 리스트\n",
    "all_citations = []\n",
    "\n",
    "# DOI 기준으로 데이터 수집\n",
    "for idx, doi in enumerate(df[\"DOI\"].dropna().unique()):  # 중복 제거 후 수집\n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    \n",
    "    # 50개마다 진행 상황 출력\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1} papers...\")\n",
    "\n",
    "# DataFrame으로 변환 후 CSV 저장\n",
    "citation_df = pd.DataFrame(all_citations)\n",
    "citation_df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 404 for DOI: 10.1021/ac0618730.s002\n",
      "Error 404 for DOI: 10.1021/acs.langmuir.0c00729.s001\n",
      "Error 404 for DOI: 10.1039/c2jm16448e\n",
      "Error 404 for DOI: 10.4141/cjps2011-165\n",
      "Error 404 for DOI: 10.1002/ange.201204989\n",
      "Error 404 for DOI: 10.1093/infdis/jir731\n",
      "Error 404 for DOI: 10.1016/j.imr.2015.04.053\n",
      "Error 404 for DOI: 10.20944/preprints202307.1316.v1\n",
      "Error 404 for DOI: 10.26226/morressier.578f37f9d462b8028d88f59d\n",
      "Error 404 for DOI: 10.1201/9781003220329-20\n",
      "Error 404 for DOI: 10.1021/acsanm.0c00474.s001\n",
      "Error 404 for DOI: 10.1002/ange.201106758\n",
      "Error 404 for DOI: 10.1161/blog.20200612.193056\n",
      "Error 404 for DOI: 10.1016/j.ygyno.2013.04.368\n",
      "Error 404 for DOI: 10.26226/morressier.599bdc78d462b80296ca0b33\n",
      "Error 404 for DOI: 10.1093/jxb/erw002\n",
      "Processed 350/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.langmuir.5b03945.s001\n",
      "Processed 400/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201302881\n",
      "Processed 450/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201108977\n",
      "Error 404 for DOI: 10.1021/ac061058k.s001\n",
      "Error 404 for DOI: 10.32657/10356/72574\n",
      "Processed 500/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201207129\n",
      "Error 404 for DOI: 10.1021/ja0672167.s001\n",
      "Error 404 for DOI: 10.1002/ange.201105986\n",
      "Error 404 for DOI: 10.4324/9781003077343-15\n",
      "Error 404 for DOI: 10.26226/morressier.5acc8ad3d462b8028d89b4e3\n",
      "Processed 550/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.biomaterials.2020.120412\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)37280-x\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36400-0\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36405-x\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)35665-9\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)35194-2\n",
      "Processed 600/6243 papers...\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36406-1\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36103-2\n",
      "Error 404 for DOI: 10.1021/acs.biomac.6b00098.s001\n",
      "Error 404 for DOI: 10.1021/acsbiomaterials.9b01790.s001\n",
      "Processed 650/6243 papers...\n",
      "Error 404 for DOI: 10.5772/30646\n",
      "Processed 700/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acssuschemeng.6b00014.s001\n",
      "Processed 750/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.jnatprod.3c00143.s001\n",
      "Error 404 for DOI: 10.1002/chin.201229206\n",
      "Error 404 for DOI: 10.1097/00007890-201211271-00861\n",
      "Processed 800/6243 papers...\n",
      "Error 404 for DOI: 10.5109/25207\n",
      "Processed 850/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsomega.6b00495.s001\n",
      "Error 404 for DOI: 10.1021/acs.molpharmaceut.1c00782.s001\n",
      "Processed 900/6243 papers...\n",
      "Error 404 for DOI: 10.59350/snhcs-hm281\n",
      "Error 404 for DOI: 10.5423/ppj.si.11.2012.0173\n",
      "Error 404 for DOI: 10.3410/f.718046737.793486577\n",
      "Error 404 for DOI: 10.1109/nssmic.2010.5874130\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)35671-4\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)36404-8\n",
      "Error 404 for DOI: 10.1016/s1525-0016(16)34969-3\n",
      "Processed 950/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acschemneuro.6b00082.s001\n",
      "Error 404 for DOI: 10.4062/biomolther.10.4062/biomolther.2014.051\n",
      "Error 404 for DOI: 10.1002/chin.201343209\n",
      "Processed 1000/6243 papers...\n",
      "Processed 1050/6243 papers...\n",
      "Processed 1100/6243 papers...\n",
      "Error 404 for DOI: 10.1021/jo0612978.s001\n",
      "Processed 1150/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2254151.5361903405001\n",
      "Processed 1200/6243 papers...\n",
      "Processed 1250/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.nmd.2016.06.430\n",
      "Processed 1300/6243 papers...\n",
      "Processed 1350/6243 papers...\n",
      "Error 404 for DOI: 10.1504/ijdmb.2016.079803\n",
      "Processed 1400/6243 papers...\n",
      "Error 404 for DOI: 10.17817/2018.11.07.111345\n",
      "Processed 1450/6243 papers...\n",
      "Error 404 for DOI: 10.1016/0031-9422(82)85074-7\n",
      "Error 404 for DOI: 10.1016/b978-0-323-99144-5.00022-6\n",
      "Error 404 for DOI: 10.1038/ncomms13637\n",
      "Processed 1500/6243 papers...\n",
      "Error 404 for DOI: 10.1016/0169-5002(94)94354-0\n",
      "Error 404 for DOI: 10.1097/01.ede.0000392153.88998.b9\n",
      "Error 404 for DOI: 10.1364/ecbo.2009.7372_21\n",
      "Error 404 for DOI: 10.18692/1810-4800-2018-2-66-7\n",
      "Processed 1550/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsami.6b12618.s001\n",
      "Processed 1600/6243 papers...\n",
      "Processed 1650/6243 papers...\n",
      "Error 404 for DOI: 10.36076/ppj/2016.19.e787\n",
      "Error 404 for DOI: 10.1021/acs.jafc.6b03588.s001\n",
      "Error 404 for DOI: 10.1021/acs.analchem.6b03255.s001\n",
      "Error 404 for DOI: 10.1021/acs.jafc.0c03255.s001\n",
      "Error 404 for DOI: 10.1093/eurjpc/zwad301\n",
      "Processed 1700/6243 papers...\n",
      "Error 404 for DOI: 10.37473/dac/10.1038/s41598-023-34151-6\n",
      "Error 404 for DOI: 10.1021/jacs.7b00390.s001\n",
      "Processed 1750/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.langmuir.0c00893.s001\n",
      "Processed 1800/6243 papers...\n",
      "Processed 1850/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2573886.6195484922001\n",
      "Processed 1900/6243 papers...\n",
      "Processed 1950/6243 papers...\n",
      "Error 404 for DOI: 10.2196/12070\n",
      "Error 404 for DOI: 10.18297/etd/3639\n",
      "Processed 2000/6243 papers...\n",
      "Processed 2050/6243 papers...\n",
      "Error 404 for DOI: 10.1101/gr.231837.117\n",
      "Processed 2100/6243 papers...\n",
      "Error 404 for DOI: 10.3389/fmicb.2018.02414\n",
      "Error 404 for DOI: 10.3389/fmicb.2018.0069\n",
      "Processed 2150/6243 papers...\n",
      "Processed 2200/6243 papers...\n",
      "Processed 2250/6243 papers...\n",
      "Processed 2300/6243 papers...\n",
      "Error 404 for DOI: 10.1371/joumal.pone.0207373\n",
      "Processed 2350/6243 papers...\n",
      "Processed 2400/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.jatc.9b04304\n",
      "Error 404 for DOI: 10.1016/j.toxlet.2017.07.124\n",
      "Error 404 for DOI: 10.1016/j.ymthe.2019.05.013\n",
      "Processed 2450/6243 papers...\n",
      "Processed 2500/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.clinbiochem.2019.06.003\n",
      "Processed 2550/6243 papers...\n",
      "Processed 2600/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.jocn.2020.09.036\n",
      "Processed 2650/6243 papers...\n",
      "Error 404 for DOI: 10.1038/s41598-019-52399-9\n",
      "Processed 2700/6243 papers...\n",
      "Error 404 for DOI: 10.31525/cmr-14804dc\n",
      "Processed 2750/6243 papers...\n",
      "Processed 2800/6243 papers...\n",
      "Processed 2850/6243 papers...\n",
      "Processed 2900/6243 papers...\n",
      "Processed 2950/6243 papers...\n",
      "Processed 3000/6243 papers...\n",
      "Error 404 for DOI: 10.1007/springerreference_67742\n",
      "Error 404 for DOI: 10.1002/ange.201301646\n",
      "Processed 3050/6243 papers...\n",
      "Error 404 for DOI: 10.3390/ijms140917986\n",
      "Error 404 for DOI: 10.21037/tcr.2018.01.09\n",
      "Error 404 for DOI: 10.1021/acschembio.0c00032.s001\n",
      "Error 404 for DOI: 10.3410/f.718329967.793502313\n",
      "Processed 3100/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.jcyt.2014.01.302\n",
      "Error 404 for DOI: 10.31525/cmr-9739bb\n",
      "Error 404 for DOI: 10.14341/dm9776-4140\n",
      "Error 404 for DOI: 10.1161/STROKEAHA.116.015428\n",
      "Processed 3150/6243 papers...\n",
      "Error 404 for DOI: 10.26226/morressier.58e389b4d462b80292384937\n",
      "Error 404 for DOI: 10.3410/f.725829912.793529969\n",
      "Error 404 for DOI: 10.1016/s0167-8140(04)82477-0\n",
      "Processed 3200/6243 papers...\n",
      "Error 404 for DOI: 10.1016/s0923-7534(20)33189-6\n",
      "Error 404 for DOI: 10.1002/ange.201600209\n",
      "Error 404 for DOI: 10.1117/12.2288492.5751463109001\n",
      "Processed 3250/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2214882.4848635731001\n",
      "Error 404 for DOI: 10.1021/acs.biomac.5b01756.s001\n",
      "Error 404 for DOI: 10.21070/ups.5219\n",
      "Error 404 for DOI: 10.1201/9780849351068-23\n",
      "Processed 3300/6243 papers...\n",
      "Error 404 for DOI: 10.26226/morressier.59a6b345d462b80290b547c2\n",
      "Processed 3350/6243 papers...\n",
      "Error 404 for DOI: 10.4048/jbc.2017.203.270\n",
      "Processed 3400/6243 papers...\n",
      "Processed 3450/6243 papers...\n",
      "Processed 3500/6243 papers...\n",
      "Processed 3550/6243 papers...\n",
      "Processed 3600/6243 papers...\n",
      "Processed 3650/6243 papers...\n",
      "Error 404 for DOI: 10.2174/9789815136081123010003\n",
      "Processed 3700/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201210365\n",
      "Error 404 for DOI: 10.1002/ange.201209187\n",
      "Error 404 for DOI: 10.1021/acs.jnatprod.5b01071.s001\n",
      "Processed 3750/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsami.6b08826.s001\n",
      "Error 404 for DOI: 10.1002/ange.201484961\n",
      "Error 404 for DOI: 10.1021/acs.biomac.3c01025.s001\n",
      "Processed 3800/6243 papers...\n",
      "Error 404 for DOI: 10.1002/ange.201501748\n",
      "Error 404 for DOI: 10.1021/acs.biomac.5b01622.s001\n",
      "Error 404 for DOI: 10.1021/acsnano.5b07787.s001\n",
      "Error 404 for DOI: 10.1021/acscatal.6b01884.s001\n",
      "Error 404 for DOI: 10.1021/acs.chemmater.6b03676.s001\n",
      "Error 404 for DOI: 10.1002/ange.201510319\n",
      "Error 404 for DOI: 10.1021/acs.macromol.2c01205.s001\n",
      "Processed 3850/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsabm.0c00614.s001\n",
      "Error 404 for DOI: 10.1186/s13068-017-07072\n",
      "Processed 3900/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.intimp.2018.03,034\n",
      "Processed 3950/6243 papers...\n",
      "Error 404 for DOI: 10.1002/bit.27175\n",
      "Processed 4000/6243 papers...\n",
      "Processed 4050/6243 papers...\n",
      "Error 404 for DOI: 10.3965/j.ijabe.20171001.2113\n",
      "Error 404 for DOI: 10.7554/elife.01911.015\n",
      "Error 404 for DOI: 10.4141/cjss2012-101\n",
      "Processed 4100/6243 papers...\n",
      "Error 404 for DOI: 10.15210//sufix\n",
      "Error 404 for DOI: 10.21203/rs.3.rs-4845823/v1\n",
      "Error 404 for DOI: 10.1016/0016-5085(95)27405-7\n",
      "Processed 4150/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.jnatprod.6b00267.s001\n",
      "Processed 4200/6243 papers...\n",
      "Processed 4250/6243 papers...\n",
      "Error 404 for DOI: 10.5851/kosfa.2018.e10\n",
      "Error 404 for DOI: 10.5851/kosfa.2018.38.2.338\n",
      "Processed 4300/6243 papers...\n",
      "Processed 4350/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.compag.2019.105174\n",
      "Processed 4400/6243 papers...\n",
      "Error 404 for DOI: 10.17179/excli2020-2162\n",
      "Processed 4450/6243 papers...\n",
      "Processed 4500/6243 papers...\n",
      "Error 404 for DOI: 10.3410/f.718187642.793488801\n",
      "Processed 4550/6243 papers...\n",
      "Error 404 for DOI: 10.3410/f.718475804.793499467\n",
      "Processed 4600/6243 papers...\n",
      "Error 404 for DOI: 10.1007/springerreference_174316\n",
      "Error 404 for DOI: 10.1038/srep07467\n",
      "Error 404 for DOI: 10.7287/peerj.13038v0.1/reviews/3\n",
      "Processed 4650/6243 papers...\n",
      "Error 404 for DOI: 10.3410/f.718520452.793498333\n",
      "Processed 4700/6243 papers...\n",
      "Processed 4750/6243 papers...\n",
      "Processed 4800/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acsnano.6b02592.s001\n",
      "Error 404 for DOI: 10.1364/pibm.2017.w3a.77\n",
      "Processed 4850/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.amjcard.2009.08.520\n",
      "Error 404 for DOI: 10.3410/f.726927815.793525980\n",
      "Error 404 for DOI: 10.1021/acs.bioconjchem.6b00305.s001\n",
      "Processed 4900/6243 papers...\n",
      "Error 404 for DOI: 10.1016/0169-5002(90)90327-i\n",
      "Error 404 for DOI: 10.3791/22380\n",
      "Processed 4950/6243 papers...\n",
      "Error 404 for DOI: 10.18632/ncotarget.20329\n",
      "Error 404 for DOI: 10.1021/acscombsci.6b00127.s001\n",
      "Error 404 for DOI: 10.1021/acs.orglett.6b00115.s001\n",
      "Error 404 for DOI: 10.1021/acscombsci.6b00071.s001\n",
      "Processed 5000/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.bpj.2015.11.1314\n",
      "Processed 5050/6243 papers...\n",
      "Processed 5100/6243 papers...\n",
      "Error 404 for DOI: 10.1117/12.2549356.6142075687001\n",
      "Processed 5150/6243 papers...\n",
      "Processed 5200/6243 papers...\n",
      "Error 404 for DOI: 10.3938/jkps.71.593\n",
      "Processed 5250/6243 papers...\n",
      "Error 404 for DOI: 10.1371/journal.pone.0180418\n",
      "Processed 5300/6243 papers...\n",
      "Error 404 for DOI: 10.3892/o1.2017.6169\n",
      "Processed 5350/6243 papers...\n",
      "Processed 5400/6243 papers...\n",
      "Processed 5450/6243 papers...\n",
      "Processed 5500/6243 papers...\n",
      "Processed 5550/6243 papers...\n",
      "Processed 5600/6243 papers...\n",
      "Error 404 for DOI: 10.1016/j.molcel.2018.05.017\n",
      "Processed 5650/6243 papers...\n",
      "Error 404 for DOI: 10.1021/acs.est.9b03231.s001\n",
      "Processed 5700/6243 papers...\n",
      "Processed 5750/6243 papers...\n",
      "Processed 5800/6243 papers...\n",
      "Processed 5850/6243 papers...\n",
      "Processed 5900/6243 papers...\n",
      "Processed 5950/6243 papers...\n",
      "Error 404 for DOI: 10.1002/mp.14147\n",
      "Error 404 for DOI: 10.1371/journal.pone.0238290\n",
      "Processed 6000/6243 papers...\n",
      "Error 404 for DOI: 10.1097/MD.0000000000021217\n",
      "Processed 6050/6243 papers...\n",
      "Error 404 for DOI: 10.1109/TMI.2020.3007520\n",
      "Error 404 for DOI: 10.2196/20891\n",
      "Processed 6100/6243 papers...\n",
      "Processed 6150/6243 papers...\n",
      "Processed 6200/6243 papers...\n",
      "✅ Citation data collection complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Delay 추가\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# API 요청 함수\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Publish_Year\": paper_year,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:  # Too Many Requests\n",
    "            print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 대기 시간 증가 (지수적 백오프)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for DOI: {doi}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Failed to fetch data for DOI: {doi} after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"citation_data.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "# 기존 데이터 로드 (이미 수집된 DOI 중복 방지)\n",
    "try:\n",
    "    existing_data = pd.read_csv(output_file, encoding=\"utf-8\")\n",
    "    collected_dois = set(existing_data[\"Paper_Doi\"].unique())\n",
    "except FileNotFoundError:\n",
    "    collected_dois = set()\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8\")\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(df[\"DOI\"].dropna().unique())\n",
    "\n",
    "for idx, doi in enumerate(df[\"DOI\"].dropna().unique()):\n",
    "    if doi in collected_dois:\n",
    "        continue  # 이미 수집된 DOI는 건너뛰기\n",
    "    \n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 논문 DOI를 이용해 인용 정보 가져오기\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if \"doi\" in paper and \"year\" in paper and \"title\" in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 지수적 백오프 적용\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"Citation_Dataset_Bio.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8-sig\").head(100)\n",
    "dois = df[\"DOI\"].dropna().unique()\n",
    "\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(dois)\n",
    "for idx, doi in enumerate(dois, 1):\n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed {idx}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # API 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI: 10.3109/00016489.2012.720031\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def get_paper_doi(title, max_retries=5):\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\n",
    "        \"query\": title,\n",
    "        \"fields\": \"externalIds,title\"\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "                for paper in data[\"data\"]:\n",
    "                    if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n",
    "                        return paper[\"externalIds\"][\"DOI\"]\n",
    "            return \"DOI not found\"\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded. Retrying in {2 ** attempt} seconds...\")\n",
    "            time.sleep(2 ** attempt)  # 지수적 백오프 (1s, 2s, 4s, ...)\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "    \n",
    "    return \"Failed after multiple retries\"\n",
    "\n",
    "# 예제 논문 제목\n",
    "title = \"The auditory and speech performance of children with intellectual disability after cochlear implantation\"\n",
    "doi = get_paper_doi(title)\n",
    "print(\"DOI:\", doi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'error_log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m doi_to_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m\"\u001b[39m], df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]))  \u001b[38;5;66;03m# DOI -> Title 매핑\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. error_log.txt에서 잘못된 DOI 목록 로드\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror_log_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     18\u001b[0m     error_dois \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39mreadlines() \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Semantic Scholar API에서 논문 제목으로 새로운 DOI 찾기\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'error_log.txt'"
     ]
    }
   ],
   "source": [
    "#1차\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# 파일 경로 설정\n",
    "paper_dataset_path = \"Paper_Dataset_Bio.csv\"\n",
    "error_log_path = \"error_log.txt\"\n",
    "output_path = \"Updated_DOI_Dataset.csv\"\n",
    "\n",
    "# 1. Paper_Dataset_Bio.csv에서 DOI와 Title 로드\n",
    "df = pd.read_csv(paper_dataset_path, dtype=str)  # 문자열로 읽기\n",
    "doi_to_title = dict(zip(df[\"DOI\"], df[\"Title\"]))  # DOI -> Title 매핑\n",
    "\n",
    "# 2. error_log.txt에서 잘못된 DOI 목록 로드\n",
    "with open(error_log_path, \"r\") as file:\n",
    "    error_dois = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "# Semantic Scholar API에서 논문 제목으로 새로운 DOI 찾기\n",
    "def get_paper_doi(title, max_retries=5):\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": title, \"fields\": \"externalIds,title\"}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "                for paper in data[\"data\"]:\n",
    "                    if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n",
    "                        return paper[\"externalIds\"][\"DOI\"]\n",
    "            return \"DOI not found\"\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded. Retrying in {2 ** attempt} seconds...\")\n",
    "            time.sleep(2 ** attempt)  # 지수적 백오프 (1s, 2s, 4s, ...)\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "    return \"Failed after multiple retries\"\n",
    "\n",
    "# 새로운 DOI 검색 및 저장\n",
    "updated_data = []\n",
    "\n",
    "for doi in error_dois:\n",
    "    title = doi_to_title.get(doi, \"Title not found\")\n",
    "    if title == \"Title not found\":\n",
    "        print(f\"Skipping DOI {doi} - No matching title found in dataset.\")\n",
    "        continue\n",
    "\n",
    "    new_doi = get_paper_doi(title)\n",
    "    updated_data.append({\"Title\": title, \"Original_DOI\": doi, \"New_DOI\": new_doi})\n",
    "    print(f\"Updated DOI for '{title}': {new_doi}\")\n",
    "\n",
    "# 새로운 데이터프레임 생성 및 저장\n",
    "updated_df = pd.DataFrame(updated_data, encoding=\"utf-8\")\n",
    "updated_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Updated DOI dataset saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "Rate limit exceeded. Retrying in 2 seconds...\n",
      "Rate limit exceeded. Retrying in 4 seconds...\n",
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "Rate limit exceeded. Retrying in 2 seconds...\n",
      "Rate limit exceeded. Retrying in 4 seconds...\n",
      "Rate limit exceeded. Retrying in 1 seconds...\n",
      "DOI 업데이트 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# 'New_DOI'가 \"Failed after multiple retries\"인 경우 필터링\n",
    "retry_df = updated_df[updated_df[\"New_DOI\"] == \"Failed after multiple retries\"].copy()\n",
    "\n",
    "# Semantic Scholar API에서 논문 제목으로 새로운 DOI 찾기\n",
    "def get_paper_doi(title, max_retries=5):\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\"query\": title, \"fields\": \"externalIds,title\"}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if \"data\" in data and len(data[\"data\"]) > 0:\n",
    "                for paper in data[\"data\"]:\n",
    "                    if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n",
    "                        return paper[\"externalIds\"][\"DOI\"]\n",
    "            return \"DOI not found\"\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded. Retrying in {2 ** attempt} seconds...\")\n",
    "            time.sleep(2 ** attempt)  # 지수적 백오프 (1s, 2s, 4s, ...)\n",
    "        else:\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "    return \"Failed after multiple retries\"\n",
    "\n",
    "# 새로운 DOI 검색 및 업데이트\n",
    "retry_df[\"New_DOI\"] = retry_df[\"Title\"].apply(get_paper_doi)\n",
    "\n",
    "# 원본 updated_df에 반영\n",
    "updated_df.loc[updated_df[\"New_DOI\"] == \"Failed after multiple retries\", \"New_DOI\"] = retry_df[\"New_DOI\"]\n",
    "\n",
    "print(\"DOI 업데이트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load Updated_DOI_Dataset.csv\n",
    "updated_df = pd.read_csv(\"Updated_DOI_Dataset.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# 2. Load Paper_Dataset_Bio.csv\n",
    "paper_df = pd.read_csv(\"Paper_Dataset_Bio.csv\")\n",
    "\n",
    "# 3. Merge the two dataframes on the 'Title' column to match the records\n",
    "merged_df = pd.merge(paper_df, updated_df[['Title', 'New_DOI']], on='Title', how='left')\n",
    "\n",
    "# 4. Update the DOI column with the New_DOI from the merged data\n",
    "merged_df['DOI'] = merged_df['New_DOI'].combine_first(merged_df['DOI'])\n",
    "\n",
    "# 5. Drop the 'New_DOI' column as it's no longer needed\n",
    "merged_df.drop(columns=['New_DOI'], inplace=True)\n",
    "\n",
    "# 6. Save the updated Paper_Dataset_Bio.csv\n",
    "merged_df.to_csv(\"Updated_Paper_Dataset_Bio.csv\", index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 404 for DOI: 10.1039/c2jm16448e\n",
      "Error 404 for DOI: 10.1093/infdis/jir731\n",
      "Error 404 for DOI: 10.1093/jxb/erw002\n",
      "Error 404 for DOI: 10.1016/j.biomaterials.2020.120412\n",
      "Processed 50/407 papers...\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode character '\\u2018' in position 94: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_papers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m papers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_citations\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollected_dois\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mISO-8859-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     all_citations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     81\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 요청 간격 조정\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:270\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:275\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\formats\\csvs.py:324\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[1;34m(self, start_i, end_i)\u001b[0m\n\u001b[0;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(res\u001b[38;5;241m.\u001b[39m_iter_column_arrays())\n\u001b[0;32m    323\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_get_values_for_csv(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n\u001b[1;32m--> 324\u001b[0m \u001b[43mlibwriters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_csv_rows\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mwriters.pyx:73\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u2018' in position 94: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# API 요청 함수\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Publish_Year\": paper_year,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:  # Too Many Requests\n",
    "            print(f\"Rate limit exceeded. Retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 대기 시간 증가 (지수적 백오프)\n",
    "        else:\n",
    "            print(f\"Error {response.status_code} for DOI: {doi}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Failed to fetch data for DOI: {doi} after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Updated_DOI_Dataset.csv\"\n",
    "output_file = \"citation_data.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "# 기존 데이터 로드 (이미 수집된 DOI 중복 방지)\n",
    "try:\n",
    "    existing_data = pd.read_csv(output_file, encoding=\"utf-8-sig\")\n",
    "    collected_dois = set(existing_data[\"Paper_Doi\"].unique())\n",
    "except FileNotFoundError:\n",
    "    collected_dois = set()\n",
    "\n",
    "# Updated_DOI_Dataset 로드\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8-sig\")\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(df[\"DOI\"].dropna().unique())\n",
    "\n",
    "for idx, doi in enumerate(df[\"DOI\"].dropna().unique()):\n",
    "    if doi in collected_dois:\n",
    "        continue  # 이미 수집된 DOI는 건너뛰기\n",
    "    \n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Processed {idx + 1}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8-sig\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_citations).to_csv(output_file, mode='a', header=not bool(collected_dois), index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/6244 papers...\n",
      "Processed 100/6244 papers...\n",
      "Processed 150/6244 papers...\n",
      "Processed 200/6244 papers...\n",
      "Processed 250/6244 papers...\n",
      "Processed 300/6244 papers...\n",
      "Processed 350/6244 papers...\n",
      "Processed 400/6244 papers...\n",
      "Processed 450/6244 papers...\n",
      "Processed 500/6244 papers...\n",
      "Processed 550/6244 papers...\n",
      "Processed 600/6244 papers...\n",
      "Processed 650/6244 papers...\n",
      "Processed 700/6244 papers...\n",
      "Processed 750/6244 papers...\n",
      "Processed 800/6244 papers...\n",
      "Processed 850/6244 papers...\n",
      "Processed 900/6244 papers...\n",
      "Processed 950/6244 papers...\n",
      "Processed 1000/6244 papers...\n",
      "Processed 1050/6244 papers...\n",
      "Processed 1100/6244 papers...\n",
      "Processed 1150/6244 papers...\n",
      "Processed 1200/6244 papers...\n",
      "Processed 1250/6244 papers...\n",
      "Processed 1300/6244 papers...\n",
      "Processed 1350/6244 papers...\n",
      "Processed 1400/6244 papers...\n",
      "Processed 1450/6244 papers...\n",
      "Processed 1500/6244 papers...\n",
      "Processed 1550/6244 papers...\n",
      "Processed 1600/6244 papers...\n",
      "Processed 1650/6244 papers...\n",
      "Processed 1700/6244 papers...\n",
      "Processed 1750/6244 papers...\n",
      "Processed 1800/6244 papers...\n",
      "Processed 1850/6244 papers...\n",
      "Processed 1900/6244 papers...\n",
      "Processed 1950/6244 papers...\n",
      "Processed 2000/6244 papers...\n",
      "Processed 2050/6244 papers...\n",
      "Processed 2100/6244 papers...\n",
      "Processed 2150/6244 papers...\n",
      "Processed 2200/6244 papers...\n",
      "Processed 2250/6244 papers...\n",
      "Processed 2300/6244 papers...\n",
      "Processed 2350/6244 papers...\n",
      "Processed 2400/6244 papers...\n",
      "Processed 2450/6244 papers...\n",
      "Processed 2500/6244 papers...\n",
      "Processed 2550/6244 papers...\n",
      "Processed 2600/6244 papers...\n",
      "Processed 2650/6244 papers...\n",
      "Processed 2700/6244 papers...\n",
      "Processed 2750/6244 papers...\n",
      "Processed 2800/6244 papers...\n",
      "Processed 2850/6244 papers...\n",
      "Processed 2900/6244 papers...\n",
      "Processed 2950/6244 papers...\n",
      "Processed 3000/6244 papers...\n",
      "Processed 3050/6244 papers...\n",
      "Processed 3100/6244 papers...\n",
      "Processed 3150/6244 papers...\n",
      "Processed 3200/6244 papers...\n",
      "Processed 3250/6244 papers...\n",
      "Processed 3300/6244 papers...\n",
      "Processed 3350/6244 papers...\n",
      "Processed 3400/6244 papers...\n",
      "Processed 3450/6244 papers...\n",
      "Processed 3500/6244 papers...\n",
      "Processed 3550/6244 papers...\n",
      "Processed 3600/6244 papers...\n",
      "Processed 3650/6244 papers...\n",
      "Processed 3700/6244 papers...\n",
      "Processed 3750/6244 papers...\n",
      "Processed 3800/6244 papers...\n",
      "Processed 3850/6244 papers...\n",
      "Processed 3900/6244 papers...\n",
      "Processed 3950/6244 papers...\n",
      "Processed 4000/6244 papers...\n",
      "Processed 4050/6244 papers...\n",
      "Processed 4100/6244 papers...\n",
      "Processed 4150/6244 papers...\n",
      "Processed 4200/6244 papers...\n",
      "Processed 4250/6244 papers...\n",
      "Processed 4300/6244 papers...\n",
      "Processed 4350/6244 papers...\n",
      "Processed 4400/6244 papers...\n",
      "Processed 4450/6244 papers...\n",
      "Processed 4500/6244 papers...\n",
      "Processed 4550/6244 papers...\n",
      "Processed 4600/6244 papers...\n",
      "Processed 4650/6244 papers...\n",
      "Processed 4700/6244 papers...\n",
      "Processed 4750/6244 papers...\n",
      "Processed 4800/6244 papers...\n",
      "Processed 4850/6244 papers...\n",
      "Processed 4900/6244 papers...\n",
      "Processed 4950/6244 papers...\n",
      "Processed 5000/6244 papers...\n",
      "Processed 5050/6244 papers...\n",
      "Processed 5100/6244 papers...\n",
      "Processed 5150/6244 papers...\n",
      "Processed 5200/6244 papers...\n",
      "Processed 5250/6244 papers...\n",
      "Processed 5300/6244 papers...\n",
      "Processed 5350/6244 papers...\n",
      "Processed 5400/6244 papers...\n",
      "Processed 5450/6244 papers...\n",
      "Processed 5500/6244 papers...\n",
      "Processed 5550/6244 papers...\n",
      "Processed 5600/6244 papers...\n",
      "Processed 5650/6244 papers...\n",
      "Processed 5700/6244 papers...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /v1/paper/10.1186/s12934-019-1213-y (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002D4361A5760>: Failed to resolve 'api.semanticscholar.org' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\socket.py:963\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    962\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    964\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    615\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002D4361A5760>: Failed to resolve 'api.semanticscholar.org' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /v1/paper/10.1186/s12934-019-1213-y (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002D4361A5760>: Failed to resolve 'api.semanticscholar.org' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m total_papers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dois)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, doi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dois, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     citation_info \u001b[38;5;241m=\u001b[39m \u001b[43mget_citation_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m citation_info:\n\u001b[0;32m     57\u001b[0m         all_citations\u001b[38;5;241m.\u001b[39mextend(citation_info)\n",
      "Cell \u001b[1;32mIn[39], line 12\u001b[0m, in \u001b[0;36mget_citation_info\u001b[1;34m(doi)\u001b[0m\n\u001b[0;32m      9\u001b[0m delay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# 초기 대기 시간 (초)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[1;32m---> 12\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     15\u001b[0m         data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /v1/paper/10.1186/s12934-019-1213-y (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002D4361A5760>: Failed to resolve 'api.semanticscholar.org' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 논문 DOI를 이용해 인용 정보 가져오기\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if \"doi\" in paper and \"year\" in paper and \"title\" in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 지수적 백오프 적용\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Paper_Dataset_Bio.csv\"\n",
    "output_file = \"Citation_Dataset_Bio.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8-sig\")\n",
    "dois = df[\"DOI\"].dropna().unique()\n",
    "\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(dois)\n",
    "for idx, doi in enumerate(dois, 1):\n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed {idx}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # API 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV로 저장\n",
    "df_citation.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID_B                  ID_B_Org  ID_Project_Unique  ID_Paper  Year  \\\n",
      "28     172          201107011390401C         1345171157       165  2012   \n",
      "32     229          201207011580401A         1345171791       180  2012   \n",
      "56     135          200807020340401A         1345194033       322  2012   \n",
      "69     135          200807020340401A         1345194033       329  2012   \n",
      "71     135          200807020340401A         1345194033       339  2012   \n",
      "...    ...                       ...                ...       ...   ...   \n",
      "6271    75                  1.31E+22         1711105377     40735  2020   \n",
      "6285    75                  1.31E+22         1711105377     40751  2020   \n",
      "6297    85                  1.31E+22         1711113344     40842  2020   \n",
      "6361    67  13111015015222002241428A         1711121217     41666  2020   \n",
      "6486   270                  2.11E+22         9991006004     42135  2015   \n",
      "\n",
      "      Year_Perf  Department  \\\n",
      "28         2012          15   \n",
      "32         2012          15   \n",
      "56         2012          15   \n",
      "69         2012          15   \n",
      "71         2012          15   \n",
      "...         ...         ...   \n",
      "6271       2020          15   \n",
      "6285       2020          15   \n",
      "6297       2020          15   \n",
      "6361       2020          15   \n",
      "6486       2016          17   \n",
      "\n",
      "                                                  Title  \\\n",
      "28    Kinetic studies on the formation of various II...   \n",
      "32    Current state and perspectives on erythropoiet...   \n",
      "56    Proteomic characterization of Kunitz trypsin i...   \n",
      "69    A screening method for β-glucan hydrolase empl...   \n",
      "71    Transcriptional analysis of the rho-coumarate ...   \n",
      "...                                                 ...   \n",
      "6271  Efficacy of breast MRI for surgical decision i...   \n",
      "6285  Acellular dermal matrix combined with oxidized...   \n",
      "6297  Automatic Registration Between Dental Cone-Bea...   \n",
      "6361  An adverse outcome pathway for immune-mediated...   \n",
      "6486  In vivo monitoring of angiogenesis in a mouse ...   \n",
      "\n",
      "                              DOI                                 Journal  \\\n",
      "28             10.1039/c2jm16448e          JOURNAL OF MATERIALS CHEMISTRY   \n",
      "32      10.1007/s00253-012-4291-x  APPLIED MICROBIOLOGY AND BIOTECHNOLOGY   \n",
      "56      10.1007/s00726-011-1092-y                             AMINO ACIDS   \n",
      "69      10.1007/s10529-012-0873-z                   BIOTECHNOLOGY LETTERS   \n",
      "71                            NaN       RESEARCH JOURNAL OF BIOTECHNOLOGY   \n",
      "...                           ...                                     ...   \n",
      "6271   10.1186/s12885-020-07443-7                              BMC CANCER   \n",
      "6285  10.1097/MD.0000000000021217                                MEDICINE   \n",
      "6297     10.1109/TMI.2020.3007520    IEEE TRANSACTIONS ON MEDICAL IMAGING   \n",
      "6361   10.1007/s00204-020-02767-6                  ARCHIVES OF TOXICOLOGY   \n",
      "6486    10.1007/s00726-016-2225-0                             AMINO ACIDS   \n",
      "\n",
      "      ... Num_Foreign_Applications  Num_Foreign_Registrations  Num_Perf  \\\n",
      "28    ...                        0                          0        12   \n",
      "32    ...                        3                          0        12   \n",
      "56    ...                        0                          0        28   \n",
      "69    ...                        0                          0        28   \n",
      "71    ...                        0                          0        28   \n",
      "...   ...                      ...                        ...       ...   \n",
      "6271  ...                       10                          0        61   \n",
      "6285  ...                       10                          0        61   \n",
      "6297  ...                        4                          0        22   \n",
      "6361  ...                        2                          0        12   \n",
      "6486  ...                        0                          0         5   \n",
      "\n",
      "      SCI_YN  Joint_or_Contract  Project_Phase Citation_Count  \\\n",
      "28       1.0                  2              4             45   \n",
      "32       1.0                  2              4             30   \n",
      "56       1.0                  2              2              7   \n",
      "69       1.0                  2              2              4   \n",
      "71       1.0                  2              2             11   \n",
      "...      ...                ...            ...            ...   \n",
      "6271     1.0                  2              4             10   \n",
      "6285     1.0                  2              4              7   \n",
      "6297     1.0                  1              4             19   \n",
      "6361     1.0                  2              7             10   \n",
      "6486     1.0                  1              2              4   \n",
      "\n",
      "     Citation_Weighted Influential_Citation_Count Inf_Citation_Weighted  \n",
      "28            4.082308                        NaN                   NaN  \n",
      "32            2.721539                        1.0              0.090718  \n",
      "56            0.635026                        0.0              0.000000  \n",
      "69            0.362872                        NaN                   NaN  \n",
      "71            0.997897                        NaN                   NaN  \n",
      "...                ...                        ...                   ...  \n",
      "6271          4.493290                        0.0              0.000000  \n",
      "6285          3.145303                        NaN                   NaN  \n",
      "6297          8.537250                        3.0              1.347987  \n",
      "6361          4.493290                        0.0              0.000000  \n",
      "6486          0.807586                        0.0              0.000000  \n",
      "\n",
      "[252 rows x 92 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_24988\\3177595798.py:5: DtypeWarning: Columns (1,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  citation_df = pd.read_csv('Citation_Dataset_Bio.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "paper_df = pd.read_csv('Paper_Dataset_Bio.csv')\n",
    "citation_df = pd.read_csv('Citation_Dataset_Bio.csv')\n",
    "\n",
    "# Extract the 'DOI' from Paper_Dataset_Bio.csv and 'Paper_DOI' from Citation_Dataset_Bio.csv\n",
    "paper_dois = set(paper_df['DOI'])\n",
    "citation_paper_dois = set(citation_df['Paper_Doi'])\n",
    "\n",
    "# Filter rows in paper_df where the DOI is not in citation_paper_dois\n",
    "filtered_paper_df = paper_df[~paper_df['DOI'].isin(citation_paper_dois)]\n",
    "\n",
    "# Save the filtered result to a new CSV file if you want\n",
    "filtered_paper_df.to_csv('Filtered_Paper_Dataset_Bio.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display the filtered dataframe\n",
    "print(filtered_paper_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/289 papers...\n",
      "Processed 100/289 papers...\n",
      "Processed 150/289 papers...\n",
      "Processed 200/289 papers...\n",
      "Processed 250/289 papers...\n",
      "✅ Citation data collection complete!\n"
     ]
    }
   ],
   "source": [
    "# 400개 누락만\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 논문 DOI를 이용해 인용 정보 가져오기\n",
    "def get_citation_info(doi):\n",
    "    url = f\"https://api.semanticscholar.org/v1/paper/{doi}\"\n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 5  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url)\n",
    "            \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            paper_year = data.get(\"year\", \"Unknown\")  # 원 논문 출판 연도\n",
    "            citation_data = []\n",
    "            \n",
    "            def extract_info(citation_list, citation_type):\n",
    "                for paper in citation_list:\n",
    "                    if 'doi' in paper and 'year' in paper and 'title' in paper:\n",
    "                        citation_data.append({\n",
    "                            \"Paper_Doi\": doi,\n",
    "                            \"Publish_Year\": paper_year,\n",
    "                            \"Citation_Type\": citation_type,\n",
    "                            \"Cited_Doi\": paper[\"doi\"],\n",
    "                            \"Year\": paper[\"year\"],\n",
    "                            \"Title\": paper[\"title\"]\n",
    "                        })\n",
    "            \n",
    "            extract_info(data.get(\"citations\", []), \"Forward\")\n",
    "            extract_info(data.get(\"references\", []), \"Backward\")\n",
    "            \n",
    "            return citation_data\n",
    "        \n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 지수적 백오프 적용\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# CSV 파일 로드\n",
    "input_file = \"Filtered_Paper_Dataset_Bio.csv\"\n",
    "output_file = \"Add_Citation_Dataset_Bio.csv\"\n",
    "error_file = \"error_log.txt\"\n",
    "\n",
    "df = pd.read_csv(input_file, encoding=\"utf-8-sig\")\n",
    "dois = df[\"DOI\"].dropna().unique()\n",
    "\n",
    "all_citations = []\n",
    "errors = []\n",
    "\n",
    "total_papers = len(dois)\n",
    "for idx, doi in enumerate(dois, 1):\n",
    "    citation_info = get_citation_info(doi)\n",
    "    if citation_info:\n",
    "        all_citations.extend(citation_info)\n",
    "    else:\n",
    "        errors.append(doi)\n",
    "    \n",
    "    if idx % 50 == 0:\n",
    "        print(f\"Processed {idx}/{total_papers} papers...\")\n",
    "        pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "        all_citations = []\n",
    "    \n",
    "    time.sleep(1)  # API 요청 간격 조정\n",
    "\n",
    "# 마지막 데이터 저장\n",
    "if all_citations:\n",
    "    pd.DataFrame(all_citations).to_csv(output_file, mode='a', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 오류 로그 저장\n",
    "if errors:\n",
    "    with open(error_file, \"w\") as f:\n",
    "        for doi in errors:\n",
    "            f.write(doi + \"\\n\")\n",
    "\n",
    "print(\"✅ Citation data collection complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Paper_ID Citation_Type  \\\n",
      "0   3670a4ee1bf2acf18e55646ec0d927b25b2caddd       Forward   \n",
      "1   3670a4ee1bf2acf18e55646ec0d927b25b2caddd       Forward   \n",
      "2   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "3   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "4   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "5   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "6   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "7   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "8   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "9   3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "10  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "11  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "12  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "13  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "14  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "15  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "16  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "17  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "18  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "19  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "20  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "21  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "22  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "23  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "24  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "25  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "26  3670a4ee1bf2acf18e55646ec0d927b25b2caddd      Backward   \n",
      "\n",
      "                               Cited_PaperId  Year  \\\n",
      "0   bf39c944a1862f9e953a380ab4533d973be85453  2014   \n",
      "1   e498a0d20d5da8e2f3112fd4210d45776924c13f  2016   \n",
      "2   079895d48b5204828136b3639ff805030e9a22bf  2009   \n",
      "3   054544ef91300992a76f8f0e8ab638f7516fe768  2009   \n",
      "4   c673ef35faaf4aac0011aa9609dd7ea0bd3915c5  2008   \n",
      "5   4567bfea9bfaa996683be0e6285821af51578b07  2008   \n",
      "6   2ea1f7e1a4429620073525feb769bd2380c24877  2008   \n",
      "7   c641e57633f570032cc1889d456a0a2031c82f29  2008   \n",
      "8   969c7cea592d65b584f41ec100ca1a23d5c4c5e1  2007   \n",
      "9   b12fbdc02829c340fc9e935ef632fe3105a91824  2007   \n",
      "10                                      None  2007   \n",
      "11  99212eec1df584152becf77a026f05557a06f9a9  2007   \n",
      "12  ac703c06beb71198911903294cabb4737c2ead91  2006   \n",
      "13  2d64aba39f45a3560d2772e543968764ce67f13a  2006   \n",
      "14  1b86629044283e35060d85c78033a6c003075adc  2006   \n",
      "15  c86d0b102545ea175a63bd3eddf030333ef34a89  2006   \n",
      "16  c39c3223bd7e4ac336139958e2c9c25e5e9e628b  2006   \n",
      "17  171491255108cbae10dbc36de5ec938de4768a4a  2005   \n",
      "18  240aa3fbe87b1fb085815baa87cca8bd04f52f70  2005   \n",
      "19  4ecfccd889c6b0f349e0f2d5cc8e3424a0bdebd0  2005   \n",
      "20  74f8a87a5f5e70e4aafa6ef475c97f19f9f4b540  2004   \n",
      "21  0442c89a09bfd5ba7f09340d0b23143f6c3f32b7  2004   \n",
      "22  78017150fb3f10579b6e65c35e9a824cdaa79f19  2003   \n",
      "23  f2b4098d2299d34853a6b6fc8b3380e571dc1cb8  2003   \n",
      "24  d666a299731315c81436a900cbc8b54ee8267c7f  2002   \n",
      "25  645d1b6ac152eea22af301e43c58222e9f315d8b  2008   \n",
      "26  2312289f8ea37b8a605146f985aa806531b8dd00  2005   \n",
      "\n",
      "                                                Title  \n",
      "0   An integrated analysis of the SOX2 microRNA re...  \n",
      "1   Posttranscriptional Modulation of Sox2 Activit...  \n",
      "2   Transcriptional regulation of RET by Nkx2-1, P...  \n",
      "3   Noninvasive imaging of microRNA124a‐mediated r...  \n",
      "4   In Vivo Imaging of Functional Targeting of miR...  \n",
      "5   Connecting microRNA Genes to the Core Transcri...  \n",
      "6   Bioimaging of the unbalanced expression of mic...  \n",
      "7   Interplay between microRNAs and RNA-binding pr...  \n",
      "8   The MicroRNA miR-124 promotes neuronal differe...  \n",
      "9   Improving the prediction of human microRNA tar...  \n",
      "10  Discovery of microRNA-mRNA modules via populat...  \n",
      "11  A functional study of miR-124 in the developin...  \n",
      "12  The diverse functions of microRNAs in animal d...  \n",
      "13  Detection of mammalian microRNA expression by ...  \n",
      "14  Defining a Developmental Path to Neural Fate b...  \n",
      "15  Systematic identification of microRNA function...  \n",
      "16  Anti-miRNA oligonucleotides (AMOs): ammunition...  \n",
      "17  miRNAMap: genomic maps of microRNA genes and t...  \n",
      "18          Combinatorial microRNA target predictions  \n",
      "19  Microarray analysis shows that some microRNAs ...  \n",
      "20  Large-scale reprogramming of cranial neural cr...  \n",
      "21  MicroRNAs Genomics, Biogenesis, Mechanism, and...  \n",
      "22  Genome-Wide Survey of Human Alternative Pre-mR...  \n",
      "23  Vertebrate neurogenesis is counteracted by Sox...  \n",
      "24  MicroRNA maturation: stepwise processing and s...  \n",
      "25  In Vivo Imaging of miR-221 Biogenesis in Papil...  \n",
      "26  Neural differentiation of pluripotent mouse em...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# 논문의 Semantic Scholar ID (URL에서 추출)\n",
    "paper_id = \"3670a4ee1bf2acf18e55646ec0d927b25b2caddd\"\n",
    "\n",
    "def fetch_citation_info(paper_id):\n",
    "    \"\"\"Semantic Scholar API를 이용하여 논문의 인용 및 참고문헌 정보 가져오기\"\"\"\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}\"\n",
    "    params = {\n",
    "        \"fields\": \"title,year,citations.paperId,citations.title,citations.year,references.paperId,references.title,references.year\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Citation 정보\n",
    "        citations = data.get(\"citations\", [])\n",
    "        citation_list = [\n",
    "            {\"Paper_ID\": paper_id, \"Citation_Type\": \"Forward\", \"Cited_PaperId\": c[\"paperId\"], \"Year\": c.get(\"year\"), \"Title\": c.get(\"title\")}\n",
    "            for c in citations\n",
    "        ]\n",
    "\n",
    "        # Reference 정보\n",
    "        references = data.get(\"references\", [])\n",
    "        reference_list = [\n",
    "            {\"Paper_ID\": paper_id, \"Citation_Type\": \"Backward\", \"Cited_PaperId\": r[\"paperId\"], \"Year\": r.get(\"year\"), \"Title\": r.get(\"title\")}\n",
    "            for r in references\n",
    "        ]\n",
    "        \n",
    "        return citation_list + reference_list\n",
    "    else:\n",
    "        print(\"❌ API 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# 논문의 Citation 및 Reference 정보 가져오기\n",
    "citation_data = fetch_citation_info(paper_id)\n",
    "\n",
    "# 데이터 출력\n",
    "if citation_data:\n",
    "    df = pd.DataFrame(citation_data)\n",
    "    print(df)\n",
    "    # CSV 파일 저장 (원하면 주석 해제)\n",
    "    # df.to_csv(\"Citation_Info.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "else:\n",
    "    print(\"❌ 데이터 없음\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드는 ID를 가져옴.. DOI를 가져오도록 수정하기\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_csv('Filtered_Paper_Dataset_Bio.csv')\n",
    "\n",
    "def fetch_citation_info(paper_id):\n",
    "    \"\"\"Semantic Scholar API를 이용하여 논문의 인용 및 참고문헌 정보 가져오기\"\"\"\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}\"\n",
    "    params = {\n",
    "        \"fields\": \"title,year,externalIds.doi,citations.paperId,citations.title,citations.year,citations.externalIds.doi,references.paperId,references.title,references.year,references.externalIds.doi\"\n",
    "    }\n",
    "    \n",
    "    retries = 5  # 최대 재시도 횟수\n",
    "    delay = 2  # 초기 대기 시간 (초)\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            paper_doi = data.get(\"externalIds\", {}).get(\"DOI\", paper_id)  # DOI가 없으면 기존 ID 사용\n",
    "            \n",
    "            # Citation 정보\n",
    "            citations = data.get(\"citations\", [])\n",
    "            citation_list = [\n",
    "                {\"Paper_ID\": paper_doi, \"Citation_Type\": \"Forward\", \"Cited_PaperId\": c.get(\"externalIds\", {}).get(\"DOI\", c[\"paperId\"]), \"Year\": c.get(\"year\"), \"Title\": c.get(\"title\")}\n",
    "                for c in citations\n",
    "            ]\n",
    "    \n",
    "            # Reference 정보\n",
    "            references = data.get(\"references\", [])\n",
    "            reference_list = [\n",
    "                {\"Paper_ID\": paper_doi, \"Citation_Type\": \"Backward\", \"Cited_PaperId\": r.get(\"externalIds\", {}).get(\"DOI\", r[\"paperId\"]), \"Year\": r.get(\"year\"), \"Title\": r.get(\"title\")}\n",
    "                for r in references\n",
    "            ]\n",
    "            \n",
    "            return citation_list + reference_list\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"❌ API 요청이 429 오류로 실패했습니다. {attempt + 1}/{retries}번 재시도 중... {delay}초 대기\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2  # 지수적 백오프 적용\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"❌ 404 오류: 논문 ID {paper_id}에 대한 정보가 없습니다.\")\n",
    "            return None  # 404 오류 발생 시 바로 None 반환\n",
    "        else:\n",
    "            print(f\"❌ API 요청 실패: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    print(f\"❌ API 요청이 {retries}번 실패하여 논문 ID {paper_id}를 건너뜁니다.\")\n",
    "    return None  # 모든 시도가 실패하면 None 반환\n",
    "\n",
    "# 논문의 Citation 및 Reference 정보 수집\n",
    "all_citation_data = []\n",
    "missing_papers = []  # 404 오류로 누락된 논문 리스트\n",
    "\n",
    "# 'Sem_URL'에서 paperId 추출 후 인용 정보 가져오기\n",
    "for index, row in df.iterrows():\n",
    "    sem_url = row.get('Sem_URL')\n",
    "    if pd.notna(sem_url):  # 'Sem_URL'이 비어있지 않은 경우에만 처리\n",
    "        # URL에서 paperId 추출 (예: https://www.semanticscholar.org/paper/Current-state-and-perspectives-on-erythropoietin-Lee-Ha/1aaa70f955841f3eb70d928964e388d66ba1197d)\n",
    "        match = re.search(r\"/paper/([^/]+)$\", sem_url)\n",
    "        if match:\n",
    "            paper_id = match.group(1)\n",
    "            print(f\"Processing paper ID: {paper_id}\")\n",
    "            citation_data = fetch_citation_info(paper_id)\n",
    "            if citation_data:\n",
    "                all_citation_data.extend(citation_data)\n",
    "            else:\n",
    "                missing_papers.append(paper_id)  # 404 오류로 누락된 논문 기록\n",
    "\n",
    "# 데이터 출력 및 CSV 저장\n",
    "if all_citation_data:\n",
    "    df_citations = pd.DataFrame(all_citation_data)\n",
    "    print(df_citations)\n",
    "    # 결과를 CSV로 저장 (원하면 주석 해제)\n",
    "    df_citations.to_csv('Citation_Info_Updated.csv', index=False, encoding=\"utf-8-sig\")\n",
    "else:\n",
    "    print(\"❌ No citation data found\")\n",
    "\n",
    "# 누락된 논문 ID 로그 파일로 저장\n",
    "if missing_papers:\n",
    "    with open(\"missing_papers.txt\", \"w\") as f:\n",
    "        for paper_id in missing_papers:\n",
    "            f.write(paper_id + \"\\n\")\n",
    "    print(\"❌ 일부 논문은 404 오류로 누락되었습니다. missing_papers.txt에 기록되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (1/252): Kinetic studies on the formation of various II-VI ...\n",
      "Processing (5/252): Transcriptional analysis of the rho-coumarate 3-hy...\n",
      "Processing (7/252): Diet-Induced Obesity Dramatically Reduces the Effi...\n",
      "Processing (21/252): Loose Plant Architecture1 (LPA1) determines lamina...\n",
      "Processing (29/252): Diet-Induced Obesity Dramatically Reduces the Effi...\n",
      "Processing (31/252): Development of a Novel Technique for Scaffold Fabr...\n",
      "50/252 papers processed...\n",
      "Processing (75/252): Combined Biocatalytic and Chemical Transformations...\n",
      "Processing (89/252): STUDY ON TEMPERATURE ELEVATION OF PHOTOACOUSTIC TO...\n",
      "Processing (90/252): DETECTION OF THE MORPHOLOGY OF THE CORONARY ARTERY...\n",
      "Processing (95/252): Effects of Particulate Matter on the Developments ...\n",
      "100/252 papers processed...\n",
      "Processing (106/252): Comparison of 2D and 3D cell-based models using hu...\n",
      "Processing (135/252): Optimization of adipose tissue-derived mesenchymal...\n",
      "Processing (136/252): Amelioration of autoimmune arthritis by adoptive t...\n",
      "150/252 papers processed...\n",
      "Processing (159/252): Engineering the genetic components of a whole-cell...\n",
      "Processing (175/252): Effects of Replacing Pork with Tuna Levels on the ...\n",
      "Processing (176/252): Effect of Various Packaging Methods on Small-Scale...\n",
      "200/252 papers processed...\n",
      "250/252 papers processed...\n",
      "✅ Data processing complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# 파일 로드\n",
    "df = pd.read_csv('Filtered_Paper_Dataset_Bio.csv')\n",
    "\n",
    "# Semantic Scholar 검색 URL\n",
    "SEARCH_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "# API 호출 함수\n",
    "def get_semantic_scholar_url(title):\n",
    "    params = {\"query\": title, \"limit\": 1}\n",
    "    response = requests.get(SEARCH_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        papers = data.get(\"data\", [])\n",
    "        if papers:\n",
    "            return f\"https://www.semanticscholar.org/paper/{papers[0]['paperId']}\"\n",
    "    return \"\"\n",
    "\n",
    "# 진행 상황을 출력하는 함수\n",
    "def print_progress(idx, total, title):\n",
    "    print(f\"Processing ({idx}/{total}): {title[:50]}...\")  # 제목이 너무 길면 잘라서 출력\n",
    "\n",
    "# 'Sem_URL'이 비어있는 경우 검색 수행\n",
    "total_papers = len(df)\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.isna(row['Sem_URL']):\n",
    "        title = row['Title']\n",
    "        if pd.notna(title):\n",
    "            # 진행 상황 출력\n",
    "            print_progress(idx + 1, total_papers, title)\n",
    "            \n",
    "            # Semantic Scholar URL을 얻어 업데이트\n",
    "            df.at[idx, 'Sem_URL'] = get_semantic_scholar_url(title)\n",
    "            \n",
    "            # 요청 간 딜레이 추가\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # 진행 상황을 주기적으로 출력\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"{idx + 1}/{total_papers} papers processed...\")\n",
    "\n",
    "# 결과 저장\n",
    "df.to_csv('Filtered_Paper_Dataset_Bio_Updated.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"✅ Data processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\river\\AppData\\Local\\Temp\\ipykernel_27440\\1881955953.py:5: DtypeWarning: Columns (1,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  citation_df = pd.read_csv('Citation_Dataset_Bio.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               DOI  Year_Perf                   Paper_Doi  \\\n",
      "1295         10.3892/ijo.2011.1319       2012       10.3892/ijo.2011.1319   \n",
      "2503     10.1007/s00253-011-3758-5       2012   10.1007/s00253-011-3758-5   \n",
      "3416           10.1093/nar/gkr1021       2012         10.1093/nar/gkr1021   \n",
      "3470           10.1093/nar/gkr1127       2012         10.1093/nar/gkr1127   \n",
      "4784     10.1007/s00449-011-0610-3       2012   10.1007/s00449-011-0610-3   \n",
      "...                            ...        ...                         ...   \n",
      "429092     10.1538/expanim.19-0065       2020     10.1538/expanim.19-0065   \n",
      "429546          10.1111/bcpt.13342       2020          10.1111/bcpt.13342   \n",
      "429829  10.1016/j.bbrc.2019.10.079       2020  10.1016/j.bbrc.2019.10.079   \n",
      "430340     10.1074/jbc.M116.737940       2017     10.1074/jbc.M116.737940   \n",
      "430598         10.1038/mp.2017.113       2018         10.1038/mp.2017.113   \n",
      "\n",
      "       Publish_Year  \n",
      "1295           2011  \n",
      "2503           2011  \n",
      "3416           2011  \n",
      "3470           2011  \n",
      "4784           2011  \n",
      "...             ...  \n",
      "429092       2019.0  \n",
      "429546       2019.0  \n",
      "429829       2019.0  \n",
      "430340       2016.0  \n",
      "430598       2017.0  \n",
      "\n",
      "[561 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "paper_df = pd.read_csv('Paper_Dataset_Bio.csv')\n",
    "citation_df = pd.read_csv('Citation_Dataset_Bio.csv')\n",
    "\n",
    "# Merge the dataframes on the 'DOI' and 'Paper_Doi' columns to compare the 'Year' and 'Publish_Year'\n",
    "merged_df = pd.merge(paper_df, citation_df, left_on='DOI', right_on='Paper_Doi', how='inner')\n",
    "\n",
    "# Filter the rows where the 'Year_Perf' in Paper Dataset does not match the 'Publish_Year' in Citation Dataset\n",
    "mismatched_data = merged_df[merged_df['Year_Perf'] != merged_df['Publish_Year']]\n",
    "\n",
    "# Remove duplicates based on 'DOI'\n",
    "unique_mismatched_data = mismatched_data.drop_duplicates(subset='DOI')\n",
    "\n",
    "# Select only the relevant columns\n",
    "unique_mismatched_data = unique_mismatched_data[['DOI', 'Year_Perf', 'Paper_Doi', 'Publish_Year']]\n",
    "\n",
    "# Display the unique mismatched data\n",
    "print(unique_mismatched_data)\n",
    "\n",
    "# If you want to save this data to a new CSV file\n",
    "unique_mismatched_data.to_csv('Unique_Mismatched_Paper_Citation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title has been successfully added based on matching DOI and saved to 'Updated_Unique_Mismatched_Paper_Citation.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "unique_mismatched_df = pd.read_csv('Unique_Mismatched_Paper_Citation.csv')\n",
    "paper_bio_df = pd.read_csv('Paper_Dataset_Bio.csv')\n",
    "\n",
    "# Ensure the column names are as expected\n",
    "# Assuming the DOI column is named 'DOI' in both datasets and 'Title' is the column name in Paper_Dataset_Bio.csv\n",
    "# If the column names are different, adjust the names accordingly\n",
    "\n",
    "# Merge the dataframes on 'DOI' to get the title corresponding to each DOI in Unique_Mismatched_Paper_Citation\n",
    "merged_df = pd.merge(unique_mismatched_df, paper_bio_df[['DOI', 'Title']], on='DOI', how='left')\n",
    "\n",
    "# Save the result back to a CSV file\n",
    "merged_df.to_csv('Updated_Unique_Mismatched_Paper_Citation.csv', index=False)\n",
    "\n",
    "print(\"Title has been successfully added based on matching DOI and saved to 'Updated_Unique_Mismatched_Paper_Citation.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
